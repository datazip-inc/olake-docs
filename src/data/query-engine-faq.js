/**
 * FAQ JSON-LD mainEntity for each query engine page.
 * Kept in .js (not MDX) so MDX parser does not see '@' in object literals.
 */

export const queryEngineFaq = {
  athena: [
    { '@type': 'Question', name: 'What makes Amazon Athena a strong query engine for Apache Iceberg?', acceptedAnswer: { '@type': 'Answer', text: 'Amazon Athena (Engine v3) is a serverless AWS-native query engine that integrates deeply with the AWS ecosystem and AWS Glue Data Catalog, enabling analytics and DML operations on Iceberg tables without managing infrastructure. It offers auto-scaling Presto-based execution, fine-grained Lake Formation governance, and millisecond-precision time travel for audit and historical analysis.' } },
    { '@type': 'Question', name: 'Which Iceberg features does Athena support and what are its limitations?', acceptedAnswer: { '@type': 'Answer', text: 'Athena v3 supports complete DML operations including INSERT, UPDATE, DELETE, and MERGE, with row-level changes written as delete files in merge-on-read mode. It supports time travel using FOR TIMESTAMP AS OF and FOR VERSION AS OF, schema evolution, and built-in optimization via OPTIMIZE and VACUUM. However, it supports only the AWS Glue Data Catalog (no REST or Hive catalogs), no native streaming ingestion, and writes Iceberg v2 format only.' } },
    { '@type': 'Question', name: 'How does Athena integrate with AWS services for Iceberg workloads?', acceptedAnswer: { '@type': 'Answer', text: 'Athena integrates natively with AWS Glue for metadata management, Lake Formation for access control and governance, QuickSight for BI visualization, CloudTrail for audit trails, and AWS Glue ETL for external data ingestion. This native AWS service mesh simplifies enterprise analytics on Iceberg tables in AWS environments.' } },
    { '@type': 'Question', name: 'What are common use cases for using Amazon Athena with Iceberg tables?', acceptedAnswer: { '@type': 'Answer', text: 'Athena is ideal for serverless analytics on AWS, ad-hoc SQL analysis, BI dashboards, cost-efficient pay-per-query workloads on Iceberg tables, compliance audits with fine-grained security, and historical data analysis using time travel. It shines in environments where teams want zero infrastructure management and deep AWS integration for Iceberg analytics.' } }
  ],
  bigquery: [
    { '@type': 'Question', name: 'How does Google BigQuery support Apache Iceberg tables?', acceptedAnswer: { '@type': 'Answer', text: 'Google BigQuery (Engine v3) provides a serverless, fully managed query engine for Apache Iceberg tables with deep integration into the Google Cloud ecosystem. It supports a dual model with BigQuery-managed Iceberg tables offering full DML capabilities (INSERT, UPDATE, DELETE, MERGE), and BigLake external Iceberg tables that allow SQL analytics and limited insert operations via external tools like Dataflow/Spark. Managed tables benefit from automatic file optimization and clustering without manual maintenance.' } },
    { '@type': 'Question', name: 'What Iceberg features does BigQuery support and what are its limitations?', acceptedAnswer: { '@type': 'Answer', text: 'BigQuery supports essential Iceberg features including full SQL analytics, DML operations on managed tables, time travel via FOR SYSTEM_TIME AS OF, and automatic optimization like compaction and metadata garbage collection. It currently supports Parquet only and provides limited streaming ingestion via the Storage Write API (preview). External Iceberg tables through BigLake have restricted DML and no SQL time travel today.' } },
    { '@type': 'Question', name: 'How does BigQuery integrate with GCP services for Iceberg workloads?', acceptedAnswer: { '@type': 'Answer', text: 'BigQuery integrates natively with AWS Glue or Hive via BigLake external catalogs, plus GCP services like Dataplex for governance, BigQuery ML for analytics, and Dataflow/Storage Write API for high-throughput ingestion. This ecosystem enables analytics, data science, and BI workflows without needing to manage query infrastructure.' } },
    { '@type': 'Question', name: 'What are common use cases for using BigQuery with Iceberg tables?', acceptedAnswer: { '@type': 'Answer', text: 'Common use cases include serverless analytics on cloud data lakes, ad-hoc SQL reporting, real-time data ingestion via Storage Write API, and multi-engine data lakehouse architectures where BigQuery is the primary analytics engine coexisting with Spark or Flink on the same Iceberg datasets.' } }
  ],
  clickhouse: [
    { '@type': 'Question', name: "What is ClickHouse's current support level for Apache Iceberg tables?", acceptedAnswer: { '@type': 'Answer', text: 'ClickHouse v25.4 includes experimental support for reading Apache Iceberg tables, offering SQL analytics on Parquet files with functions like ENGINE=Iceberg, icebergS3(), and icebergCluster() for querying Iceberg data. Write support and broader DML operations are planned for later releases in 2025.' } },
    { '@type': 'Question', name: 'Which Iceberg features does ClickHouse support today and what are the limitations?', acceptedAnswer: { '@type': 'Answer', text: 'ClickHouse currently supports read-only analytics on Iceberg tables, including basic time travel using snapshot IDs or timestamps, and reading position and equality deletes (merge-on-read). It does not yet support native DELETE, UPDATE, or MERGE operations, and write capabilities are scheduled for future releases.' } },
    { '@type': 'Question', name: 'How does ClickHouse connect to Iceberg catalogs?', acceptedAnswer: { '@type': 'Answer', text: 'ClickHouse supports evolving catalog options, including Hadoop-style path catalogs since v24.3 and REST catalog support (e.g., Nessie, Polaris/Unity, AWS Glue) since v24.12. Additional catalog integrations such as Hive Metastore and R2 are under testing or planned in future versions.' } },
    { '@type': 'Question', name: 'What use cases is ClickHouse best suited for with Iceberg data?', acceptedAnswer: { '@type': 'Answer', text: "ClickHouse with Iceberg works well for high-performance analytics and ad-hoc SQL queries on large Iceberg datasets where the primary need is fast read performance. It's particularly useful for BI dashboards and analytical workloads over Iceberg tables, with write and full DML support improving as the engine evolves." } }
  ],
  databricks: [
    { '@type': 'Question', name: 'How does Databricks support Apache Iceberg tables?', acceptedAnswer: { '@type': 'Answer', text: 'Databricks Runtime 14.3 LTS+ exposes Iceberg metadata via a Unity Catalog REST endpoint, allowing external Iceberg engines to perform full SELECT and time travel queries on Delta tables that are exposed in Iceberg format through UniForm technology. This makes it possible to access the same data using standard Iceberg clients without duplicating storage.' } },
    { '@type': 'Question', name: 'What features of Iceberg tables are available through Databricks?', acceptedAnswer: { '@type': 'Answer', text: "With Unity Catalog and UniForm: External engines can perform full read-only Iceberg analytics and time travel with snapshot ID or timestamp syntax. Iceberg metadata is generated automatically on every Delta commit. Unity Catalog's RBAC and credential vending provide enterprise security and governance for external query clients. However, external write and DML operations via the REST catalog are currently limited, and modifications are primarily managed within Databricks itself." } },
    { '@type': 'Question', name: 'How does Databricks make Iceberg and Delta data interoperable?', acceptedAnswer: { '@type': 'Answer', text: "Databricks uses UniForm multi-format technology to make the same table accessible as both Delta and Iceberg, generating Iceberg metadata on every commit. This lets tools outside Databricks (e.g., Trino, Spark, Flink) query lakehouse data using standard Iceberg APIs while maintaining Delta Lake's ACID guarantees internally." } },
    { '@type': 'Question', name: 'What are typical use cases for querying Iceberg data via Databricks?', acceptedAnswer: { '@type': 'Answer', text: 'Common scenarios include: external analytics where BI tools or partner systems query Iceberg views of internal Delta tables; cross-platform data sharing using Iceberg REST for federated access with governance; and compliance and audits via read-only historical access. These allow teams to share insights with external engines without giving full Databricks cluster access.' } }
  ],
  doris: [
    { '@type': 'Question', name: 'What makes Apache Doris a supported query engine for Apache Iceberg?', acceptedAnswer: { '@type': 'Answer', text: 'Apache Doris v2.1+ is a MPP analytical database that can directly read and write Apache Iceberg tables, offering high-performance vectorized execution, multi-catalog support, and SQL analytics on large open lakehouse datasets. It supports Parquet/ORC formats and integrates with various Iceberg catalogs for unified metadata access.' } },
    { '@type': 'Question', name: 'What Iceberg features does Doris support and what are the limitations?', acceptedAnswer: { '@type': 'Answer', text: 'Doris supports full SELECT, INSERT, INSERT OVERWRITE, basic UPDATE and DELETE via Iceberg delete files, and table creation (CTAS). It handles position and equality delete files and offers time travel with SQL syntax (FOR VERSION/VERSION AS OF). However, MERGE statements are not fully supported natively and continuous streaming writes require external tools.' } },
    { '@type': 'Question', name: 'How does Doris integrate with Iceberg catalogs and metadata?', acceptedAnswer: { '@type': 'Answer', text: 'Doris supports a wide range of Iceberg catalog types, including Hive Metastore, AWS Glue, REST, Hadoop, DLF, and S3Tables, allowing you to manage and query Iceberg tables across environments and storage systems without duplicating data.' } },
    { '@type': 'Question', name: 'What are common use cases for using Doris with Iceberg tables?', acceptedAnswer: { '@type': 'Answer', text: 'Doris with Iceberg is suited for high-performance analytics and OLAP on open lakehouse data, complex SQL queries with vectorized execution, federated querying across catalogs, historical analysis via time travel, and real-time insights when combined with streaming ingestion tools like Flink.' } }
  ],
  dreamio: [
    { '@type': 'Question', name: 'What makes Dremio a powerful query engine for Apache Iceberg?', acceptedAnswer: { '@type': 'Answer', text: 'Dremio v26 delivers full Iceberg authoring capabilities with a built-in Polaris catalog, complete SQL DDL & DML support including MERGE, UPDATE, DELETE, and INSERT, advanced optimization features like Data Reflections, and Arctic git-like branching for versioning — all designed to accelerate analytics directly on Iceberg tables.' } },
    { '@type': 'Question', name: 'Which Iceberg features does Dremio support and what are its limitations?', acceptedAnswer: { '@type': 'Answer', text: 'Dremio supports full Iceberg table operations including CREATE, ALTER, INSERT, MERGE, UPDATE, DELETE, and schema evolution. It also offers Arctic/Nessie branch and tag versioning and enterprise governance with RBAC and audit logs. Limitations include no native streaming ingestion and Parquet-only writes; support for some Iceberg v3 features is planned for later releases.' } },
    { '@type': 'Question', name: 'How does Dremio handle catalog integration for Iceberg tables?', acceptedAnswer: { '@type': 'Answer', text: 'Dremio integrates with multiple Iceberg catalogs such as its built-in Polaris catalog, REST catalogs, Arctic/Nessie sources, Hive Metastore (HMS), AWS Glue, and Hadoop. This flexibility lets teams use Iceberg tables with various metadata sources without duplicating data.' } },
    { '@type': 'Question', name: 'What are common use cases for using Dremio with Iceberg tables?', acceptedAnswer: { '@type': 'Answer', text: 'Dremio is suited for modern data lakehouse platforms, advanced data engineering workflows with complex DML like CDC pipelines, and performance-critical analytics requiring automated optimization, query acceleration, and rollbacks via branching and tagging.' } }
  ],
  duckdb: [
    { '@type': 'Question', name: 'What level of Apache Iceberg support does DuckDB offer?', acceptedAnswer: { '@type': 'Answer', text: 'DuckDB v1.3+ provides lightweight read-only analytics support for Apache Iceberg tables. It can perform full SELECT queries with predicate pushdown, manifest pruning, and external file-caching to speed up data lake queries. DuckDB also supports SQL time travel syntax to query historical versions of Iceberg tables.' } },
    { '@type': 'Question', name: 'What Iceberg features are currently supported and what are the limitations in DuckDB?', acceptedAnswer: { '@type': 'Answer', text: 'DuckDB supports advanced time travel with AT (VERSION => …) and AT (TIMESTAMP => …) syntax, full SELECT analytics, REST catalog access, and external file caching for faster reads. However, it remains read-only for Iceberg tables in this version: writes, DELETE/UPDATE, and advanced streaming ingestion are not supported yet, and only Parquet file formats are recognized.' } },
    { '@type': 'Question', name: 'How does DuckDB integrate with Iceberg catalogs and storage?', acceptedAnswer: { '@type': 'Answer', text: "DuckDB's Iceberg support includes integration with Hadoop-style file system catalogs and Iceberg REST catalogs using bearer/OAuth tokens, allowing it to query data stored in object stores like S3 or GCS without requiring a Hive/Glue catalog. Its SQL engine can then access Iceberg metadata and files through these catalogs." } },
    { '@type': 'Question', name: 'What are common use cases for using DuckDB with Iceberg tables?', acceptedAnswer: { '@type': 'Answer', text: 'DuckDB with Iceberg is ideal for interactive analytics, data exploration, and prototyping on Iceberg tables directly from a local environment or lightweight server. Analysts and engineers use it for fast ad-hoc querying, historical data inspection with time travel, and data pipeline testing before production deployment.' } }
  ],
  flink: [
    { '@type': 'Question', name: 'What makes Apache Flink a reference engine for Apache Iceberg?', acceptedAnswer: { '@type': 'Answer', text: 'Apache Flink (1.18+) is recognized as a reference engine for real-time and CDC (change data capture) workloads with Apache Iceberg. It offers comprehensive streaming support, exactly-once processing semantics, and advanced incremental reads, making it well-suited for continuous data ingestion and event-driven analytics directly into Iceberg tables.' } },
    { '@type': 'Question', name: 'Which Iceberg features does Flink fully support?', acceptedAnswer: { '@type': 'Answer', text: 'Flink supports full catalog integration with systems like Hive Metastore, Hadoop Catalog, AWS Glue, JDBC, and REST catalogs. It also supports batch and streaming reads, Merge-on-Read/Copy-on-Write storage strategies, time travel, advanced filtering, and enterprise security through catalog ACLs.' } },
    { '@type': 'Question', name: 'How does Flink handle DML and row-level operations with Iceberg?', acceptedAnswer: { '@type': 'Answer', text: 'Flink supports basic INSERT operations and row-level UPSERTs when enabled on Iceberg tables, but full SQL MERGE statements are not supported in Flink SQL. This enables efficient incremental updates in streaming jobs though with some limitations compared to other engines.' } },
    { '@type': 'Question', name: 'What are common use cases for using Flink with Iceberg?', acceptedAnswer: { '@type': 'Answer', text: 'Common use cases include real-time CDC pipelines that replicate changes from sources like Kafka to Iceberg with exactly-once guarantees, continuous event processing for analytics, and high-throughput data lake ingestion where both streaming and batch workloads are managed together.' } }
  ],
  hive: [
    { '@type': 'Question', name: 'What makes Apache Hive a supported query engine for Apache Iceberg?', acceptedAnswer: { '@type': 'Answer', text: 'Apache Hive 4.0+ offers first-class support for Apache Iceberg tables with native catalog integration via HiveIcebergStorageHandler, full SQL analytics (SELECT, INSERT, CTAS), DML operations (DELETE, UPDATE, MERGE) when running on Tez, and schema evolution features. Hive works well for traditional batch analytics and large-scale ETL processing.' } },
    { '@type': 'Question', name: 'How does Hive handle DML and table writes with Iceberg?', acceptedAnswer: { '@type': 'Answer', text: "Hive's Iceberg integration supports full SQL DML operations when running on the Tez execution engine. All writes leverage Copy-on-Write (CoW) semantics, meaning data files are rewritten on update or merge operations. There is no native streaming support, so real-time ingestion usually pairs Hive with engines like Spark or Flink." } },
    { '@type': 'Question', name: 'Which Iceberg features does Hive support and what are the limitations?', acceptedAnswer: { '@type': 'Answer', text: 'Hive supports full catalog integration with Hive Metastore and other catalogs, schema evolution (e.g., ADD/RENAME COLUMN), metadata queries, and RDF integration like Ranger for security. However, it has no native streaming support, relies on CoW for writes, and currently supports Iceberg spec versions up to v1/v2 bundled in Hive.' } },
    { '@type': 'Question', name: 'What are common use cases for using Hive with Iceberg tables?', acceptedAnswer: { '@type': 'Answer', text: "Common scenarios include batch analytics on large datasets, legacy Hadoop ecosystem integration, and ETL processing in traditional data warehouse workloads. Hive's strengths lie in consistent SQL semantics and compatibility with existing Hadoop jobs while enabling Iceberg table usage." } }
  ],
  impala: [
    { '@type': 'Question', name: 'How does Apache Impala support querying Apache Iceberg tables?', acceptedAnswer: { '@type': 'Answer', text: 'Apache Impala is a massively parallel SQL query engine that supports Apache Iceberg tables, enabling SQL analytics on open lakehouse data with ACID compliance, partition pruning, and hidden partitioning. Impala can read and write Iceberg tables using catalogs like Hive Metastore, HadoopCatalog, and HadoopTables.' } },
    { '@type': 'Question', name: 'What Iceberg features does Impala support and what are its limitations?', acceptedAnswer: { '@type': 'Answer', text: 'Impala supports Iceberg features such as ACID transactions, schema evolution (add/drop/rename columns), partition layout evolution, and time travel queries with FOR SYSTEM_TIME AS OF and FOR SYSTEM_VERSION AS OF. It reads and writes Parquet format tables (Iceberg v1/v2) and handles advanced partitioning transforms. Impala also supports row-level DML via position delete files and has growing support for MERGE and optimization operations in recent releases.' } },
    { '@type': 'Question', name: 'Can Impala perform DML operations like INSERT, UPDATE, and DELETE on Iceberg tables?', acceptedAnswer: { '@type': 'Answer', text: 'Yes — Impala supports typical DML operations such as INSERT, DELETE, and UPDATE on Iceberg tables. These operations generate delete files or position delete files under the hood, enabling fine-grained modifications while maintaining snapshot isolation and ACID semantics.' } },
    { '@type': 'Question', name: 'What are common use cases for using Impala with Iceberg tables?', acceptedAnswer: { '@type': 'Answer', text: 'Impala with Iceberg is ideal for interactive SQL analytics, enterprise data warehouse queries, historical time travel analysis, and batch processing over large datasets. It integrates with Hadoop ecosystems for BI tools, dashboards, and scalable analytics on lakehouse data without needing separate ETL to proprietary formats.' } }
  ],
  presto: [
    { '@type': 'Question', name: 'What makes Presto a supported query engine for Apache Iceberg?', acceptedAnswer: { '@type': 'Answer', text: 'Presto 0.288+ is a distributed SQL query engine that supports Apache Iceberg table format for interactive analytics and batch queries. It provides multi-catalog support, advanced time travel querying syntax, and row-level DELETE operations while enabling efficient SQL analytics across Iceberg datasets.' } },
    { '@type': 'Question', name: 'Which Iceberg features does Presto support and what are its current limitations?', acceptedAnswer: { '@type': 'Answer', text: 'Presto supports INSERT, CTAS (create table as select), DELETE, and advanced time travel using AS OF TIMESTAMP or snapshot ID syntax. UPDATE capabilities are experimental and MERGE is not yet supported. It currently supports Iceberg spec versions v1 and v2 and focuses on batch analytics rather than streaming ingestion.' } },
    { '@type': 'Question', name: 'How does Presto handle catalogs and metadata when querying Iceberg tables?', acceptedAnswer: { '@type': 'Answer', text: "Presto's Iceberg integration supports a wide range of catalog types including Hive Metastore, AWS Glue, REST/Nessie, Hadoop file-based catalogs, and JDBC. It also exposes rich metadata tables like $snapshots, $history, $manifests, and more, enabling advanced schema evolution and governance workflows." } },
    { '@type': 'Question', name: 'What are common use cases for using Presto with Iceberg data?', acceptedAnswer: { '@type': 'Answer', text: 'Presto with Iceberg is well suited for interactive SQL analytics, dashboard reporting, BI use cases, ad-hoc data exploration, historical snapshot queries via time travel, and federated access to data across catalogs. It excels where low-latency distributed SQL querying is required on large Iceberg data lakes.' } }
  ],
  snowflake: [
    { '@type': 'Question', name: 'How does Snowflake support Apache Iceberg tables and querying?', acceptedAnswer: { '@type': 'Answer', text: 'Snowflake supports Apache Iceberg tables by connecting to data stored in external cloud storage (e.g., S3, GCS, or Azure Blob), allowing its query engine to read and write Iceberg datasets. You can define external volumes and catalog integrations so Snowflake can access Iceberg table metadata and data files, enabling analytics across open lakehouse data without importing it into proprietary storage.' } },
    { '@type': 'Question', name: 'What Iceberg features are available in Snowflake and what are the limitations?', acceptedAnswer: { '@type': 'Answer', text: "Snowflake's Iceberg support includes ACID transactions, schema evolution, hidden partitioning, and time travel for reproducible queries. Iceberg tables in Snowflake use Apache Parquet storage and integrate with either a Snowflake-managed Iceberg catalog or external catalogs like AWS Glue or Open Catalog. Some catalog configurations restrict write access when managed externally." } },
    { '@type': 'Question', name: 'How do Iceberg catalogs work with Snowflake for querying and metadata?', acceptedAnswer: { '@type': 'Answer', text: 'Snowflake lets you use Iceberg catalogs either by using its own managed catalog or by linking to external Iceberg catalogs through catalog integrations. This enables consistent metadata management across compute engines: Snowflake can query Iceberg tables registered in these catalogs, and external engines like Spark or Trino can also access the same metadata.' } },
    { '@type': 'Question', name: 'What are common use cases for Snowflake with Iceberg tables?', acceptedAnswer: { '@type': 'Answer', text: "Common scenarios include serverless analytics on cloud data lakes with open formats, multi-engine interoperability where tools like Spark or Trino work alongside Snowflake on the same Iceberg data, and governance-enabled lakehouse deployments where Snowflake's security features and query performance complement Iceberg's open storage principles." } }
  ],
  spark: [
    { '@type': 'Question', name: 'What makes Apache Spark a strong query engine choice for Apache Iceberg?', acceptedAnswer: { '@type': 'Answer', text: 'Apache Spark 3.3+ is considered the reference implementation for Apache Iceberg, offering comprehensive read/write operations, full DML support (including MERGE, UPDATE, and DELETE), and deep catalog compatibility with Hive Metastore, AWS Glue, REST catalogs, JDBC, and Nessie. It enables scalable batch and streaming workloads on Iceberg tables.' } },
    { '@type': 'Question', name: 'Which key Iceberg capabilities does Spark support?', acceptedAnswer: { '@type': 'Answer', text: "Spark supports essential Iceberg features including complete read/write operations, advanced DML (MERGE INTO, UPDATE, DELETE), time travel queries with SQL VERSION AS OF or TIMESTAMP AS OF, schema evolution, and table maintenance procedures — making it suitable for ETL pipelines, analytics, and reproducible versioned data access." } },
    { '@type': 'Question', name: "How does Spark handle time travel and versioning with Iceberg tables?", acceptedAnswer: { '@type': 'Answer', text: "Spark's Iceberg integration allows time travel using native SQL or DataFrame APIs to query historical snapshots. This enables data auditing, debugging, historical reporting, and reproducible workflows by accessing past versions of a table without restoring backups." } },
    { '@type': 'Question', name: 'What are common use cases for using Spark with Apache Iceberg?', acceptedAnswer: { '@type': 'Answer', text: 'Apache Spark with Iceberg is frequently used for large-scale ETL and data engineering pipelines, multi-terabyte data lake analytics, machine learning feature preparation, and hybrid streaming/batch processing. Its catalog flexibility and built-in maintenance procedures support enterprise-grade data operations.' } }
  ],
  starburst: [
    { '@type': 'Question', name: 'What makes Starburst Enterprise a strong Iceberg query engine?', acceptedAnswer: { '@type': 'Answer', text: 'Starburst Enterprise (SEP 414-E+) is an enterprise analytics platform built on Trino that provides comprehensive support for Apache Iceberg. It offers advanced SQL analytics, multi-catalog integration, full DML support (INSERT, UPDATE, DELETE, MERGE), and enterprise governance features for scalable Iceberg workloads.' } },
    { '@type': 'Question', name: 'Which Iceberg features does Starburst support and how does it integrate with metadata catalogs?', acceptedAnswer: { '@type': 'Answer', text: 'Starburst supports a wide range of Iceberg features including full read/write capabilities, atomic metadata swaps, and both Copy-on-Write and Merge-on-Read semantics. It integrates with catalogs like Hive Metastore, AWS Glue, JDBC, REST, Nessie, Snowflake, and Starburst Galaxy managed metastore, giving teams flexible metadata management and unified access.' } },
    { '@type': 'Question', name: 'How do DML operations work in Starburst with Iceberg tables?', acceptedAnswer: { '@type': 'Answer', text: 'Starburst Enterprise fully supports SQL DML operations such as INSERT, UPDATE, DELETE, and MERGE. Predicates aligned with partition definitions are pushed down to partition-level deletes; other changes emit Iceberg position or equality delete files, ensuring ACID-compliant modifications without rewriting whole partitions unnecessarily.' } },
    { '@type': 'Question', name: 'What are common use cases for using Starburst with Iceberg tables?', acceptedAnswer: { '@type': 'Answer', text: 'Starburst is ideal for enterprise analytics, BI dashboards, and multi-engine data lakehouse environments, where teams need fast SQL querying, cross-catalog federation, fine-grained security, and scalable DML on open Iceberg data without vendor lock-in. Its optimization layers help deliver high performance on large Iceberg datasets.' } }
  ],
  starrocks: [
    { '@type': 'Question', name: 'What makes StarRocks a strong query engine for Apache Iceberg?', acceptedAnswer: { '@type': 'Answer', text: 'StarRocks v3.2/3.3 is a high-performance analytical database with a vectorized OLAP engine that supports reading and writing to Apache Iceberg tables, making it ideal for fast analytics on large datasets in modern lakehouse architectures. It combines cost-based optimization and efficient query execution to accelerate analytics workloads.' } },
    { '@type': 'Question', name: 'Which Iceberg features does StarRocks support and what are its limitations?', acceptedAnswer: { '@type': 'Answer', text: 'StarRocks supports modern catalog integration with Hive Metastore, AWS Glue, and REST (e.g., Nessie) catalogs, high-performance reads including position and equality delete files, and DML like INSERT and INSERT OVERWRITE. However, it currently does not support UPDATE, DELETE, or MERGE operations, and full SQL time travel requires v3.4+.' } },
    { '@type': 'Question', name: 'How does StarRocks integrate with Iceberg catalogs and metadata?', acceptedAnswer: { '@type': 'Answer', text: 'StarRocks allows external Iceberg catalogs (e.g., Hive Metastore, Glue, REST) to be configured and queried directly, enabling catalog-centric metadata access. It also respects catalog ACLs and RBAC security, providing fine-grained governance over Iceberg datasets.' } },
    { '@type': 'Question', name: 'What are common use cases for using StarRocks with Iceberg tables?', acceptedAnswer: { '@type': 'Answer', text: "StarRocks excels in low-latency analytical queries, vectorized OLAP processing, dashboarding, and BI workloads on Iceberg tables. It's also useful as a read-optimized query layer alongside data writers (like Spark/Flink), delivering fast interactive analytics on open lakehouse data." } }
  ],
  trino: [
    { '@type': 'Question', name: 'What makes Trino a powerful query engine for Apache Iceberg?', acceptedAnswer: { '@type': 'Answer', text: 'Trino (475+) is a high-performance distributed SQL query engine designed for interactive analytics on big data. With native Iceberg support, Trino provides ANSI-SQL querying, advanced DML operations like UPDATE, DELETE, and MERGE, and efficient metadata pruning — making it ideal for ad-hoc analytics and BI workloads on Iceberg tables.' } },
    { '@type': 'Question', name: 'How does Trino support multi-catalog access for Iceberg tables?', acceptedAnswer: { '@type': 'Answer', text: "Trino's Iceberg integration supports multiple catalog types, including Hive Metastore, AWS Glue, JDBC, REST, Nessie, and Snowflake catalogs. Once configured, the same Iceberg tables can be accessed across these catalogs without duplicating data, enabling unified querying across diverse metadata sources." } },
    { '@type': 'Question', name: 'What Iceberg features does Trino support and what are its limitations?', acceptedAnswer: { '@type': 'Answer', text: 'Trino supports advanced time travel using FOR VERSION AS OF and FOR TIMESTAMP AS OF, schema evolution (ALTER TABLE add/drop/rename columns), built-in maintenance procedures, and delegated ACLs for security. However, it currently supports Iceberg spec v1/v2 and does not natively run continuous streaming ingestion — it instead queries tables updated by other engines like Flink or Spark.' } },
    { '@type': 'Question', name: 'What are common use cases for using Trino with Iceberg tables?', acceptedAnswer: { '@type': 'Answer', text: 'Trino with Iceberg is frequently used for interactive BI analytics, federated queries across heterogeneous data sources, batch analytical workloads, and historical analysis using time travel. It excels where low-latency SQL access, cross-catalog querying, and advanced DML on open data lakes are required.' } }
  ]
};
