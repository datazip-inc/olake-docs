---
slug: olake-bauplan-iceberg-lakehouse
title: "OLake + Bauplan: From Data Ingestion to Git-Like Governance"
description: Learn how OLake and Bauplan work together to create a powerful, version-controlled data lakehouse on Apache Iceberg.
tags: [Apache Iceberg, OLake, Bauplan, Lakekeeper, S3]
authors: [nayan]
image: /img/blog/2025/10/olake_bauplan_cover.png
---

While working with Data Lakes, if you've been juggling multiple tools to get data from your database into analytics, you know the pain. The operational data sits in PostgreSQL, Oracle, or MongoDB. Getting it into a Data Lake and making it queryable? That requires stitching together ETL orchestration, and data governance—usually across mumltiple tools that barely talk to each other.

In this blog, I'll show you how **OLake** and **Bauplan** work together with **Apache Iceberg** to create a data platform that actually makes sense - one where your operational data flows seamlessly into your Data Lakehouse, where your data team can work like software engineers with branches and merges.

## What's a Data Lakehouse, Anyway?

Imagine combining the best of two worlds: the flexibility and low cost of a data lake with the performance and reliability of a data warehouse. That's a lakehouse. You get to store massive amounts of raw data cheaply, but query it with the speed and structure of a traditional database. 

But here's the challenge: building a modern lakehouse that's real-time, version-controlled, and truly open has been frustratingly complex. Until now.

## The Three Building Blocks

**OLake** is a real-time data replication platform that syncs data from operational databases (like Postgres, MySQL, MongoDB) directly into data lake formats. It captures changes using CDC (Change Data Capture) and writes them as Apache Iceberg tables on object storage like S3 bucket. Think of it as a bridge between your production databases and your data lakehouse.

**Bauplan** is a data platform that brings Git-style version control to your data lakehouse. It lets you create branches, test transformations in isolation, and merge changes back to production - just like you do with code. It works natively with Apache Iceberg tables and provides a modern development workflow for data teams.

**Apache Iceberg** is the table format that makes this magic possible. It's open-source, battle-tested, and supported by virtually every modern data tool. Iceberg tables track their own history, support ACID transactions, and enable time travel queries - all while sitting on cheap object storage.

## How It All Flows Together:

Here's the complete picture of how data moves through the system:

![OLake + Bauplan Architecture](/img/blog/2025/10/bauplan_olake_architecture.png)

### The Architecture

**1. OLake** performs a full refresh of data from Postgres to Iceberg tables stored in an S3 bucket.

**2. Iceberg Tables** are written directly to S3 by OLake. Each table consists of data files (Parquet), metadata files, and manifest files that track the table's structure and history.

**3. Lakekeeper** acts as the Iceberg REST Catalog. It manages table metadata, tracks table versions, and coordinates access across different tools.

**4. Bauplan** fetches the metadata.json file from S3 to access the latest screenshot. Now you can:
- Create development branches for experimentation
- Build and test transformations safely
- Query production data without affecting it
- Merge changes when you're ready

### Prerequisites
- An instance that has write access to an S3 bucket
- Docker installed
- S3 bucket in `us-east-1` region

### Step-by-Step Implementation

Here's what we'll build: a pipeline that reads from Postgres, writes to Iceberg tables (managed by Lakekeeper), and uses Bauplan for Git-style development workflows. OLake handles the real-time replication, while Bauplan gives you branches, testing environments, and safe merges—just like working with code.

**In this implementation:** We'll use Postgres as the source and perform a full refresh of tables to Iceberg (S3 bucket). This gives us a complete snapshot of the data that we can then query and transform through Bauplan.

Ready to build this yourself? Here's how to set up the complete stack.

**Step 1: Set up Lakekeeper**

Start by setting up Lakekeeper with Docker. This will be the brain of your lakehouse, managing all table metadata:

:::tip
For detailed instructions on configuring the docker-compose file, deploying Lakekeeper, and setting up warehouse in Lakekeeper refer to the OLake documentation for [REST Catalog Lakekeeper setup](/docs/writers/iceberg/catalog/rest). 
:::

<details>
  <summary>Docker Compose file for Lakekeeper</summary>

    ```yaml
        version: "3"

services:
lakekeeper:
image: &lakekeeper-image ${LAKEKEEPER__SERVER_IMAGE:-quay.io/lakekeeper/catalog:latest-main}
pull_policy: &lakekeeper-pull-policy always
environment: &lakekeeper-environment
- LAKEKEEPER__PG_ENCRYPTION_KEY=This-is-NOT-Secure!
- LAKEKEEPER__PG_DATABASE_URL_READ=postgresql://iceberg:password@postgres:5432/iceberg
- LAKEKEEPER__PG_DATABASE_URL_WRITE=postgresql://iceberg:password@postgres:5432/iceberg
- LAKEKEEPER__AUTHZ_BACKEND=allowall
command: [ "serve" ]
healthcheck:
test: [ "CMD", "/home/nonroot/iceberg-catalog", "healthcheck" ]
interval: 1s
timeout: 10s
retries: 3
start_period: 3s
depends_on:
migrate:
condition: service_completed_successfully
ports:
- "8181:8181"
networks:
iceberg_net:

migrate:
image: *lakekeeper-image
pull_policy: *lakekeeper-pull-policy
environment: *lakekeeper-environment
restart: "no"
command: [ "migrate" ]
depends_on:
postgres:
condition: service_healthy
networks:
iceberg_net:

minio:
image: minio/minio:RELEASE.2025-04-03T14-56-28Z
container_name: minio
environment:
- MINIO_ROOT_USER=admin
- MINIO_ROOT_PASSWORD=password
- MINIO_DOMAIN=minio
networks:
iceberg_net:
aliases:
- warehouse.minio
ports:
- 9001:9001
- 9000:9000
volumes:
- ./data/minio-data:/data
command: [ "server", "/data", "--console-address", ":9001" ]

mc:
depends_on:
- minio
image: minio/mc:RELEASE.2025-04-03T17-07-56Z
container_name: mc
networks:
iceberg_net:
environment:
- AWS_ACCESS_KEY_ID=admin
- AWS_SECRET_ACCESS_KEY=password
- AWS_REGION=us-east-1
entrypoint: |
/bin/sh -c "
until (/usr/bin/mc config host add minio http://minio:9000 admin password) do echo '...waiting...' && sleep 1; done;
if ! /usr/bin/mc ls minio/warehouse > /dev/null 2>&1; then
/usr/bin/mc mb minio/warehouse;
/usr/bin/mc policy set public minio/warehouse;
fi;
tail -f /dev/null
"

postgres:
image: postgres:15
container_name: iceberg-postgres
networks:
iceberg_net:
environment:
- POSTGRES_USER=iceberg
- POSTGRES_PASSWORD=password
- POSTGRES_DB=iceberg
healthcheck:
test: [ "CMD", "pg_isready", "-U", "iceberg", "-d", "iceberg" ]
interval: 2s
timeout: 10s
retries: 3
start_period: 10s
ports:
- 5432:5432
volumes:
- ./data/postgres-data:/var/lib/postgresql/data

networks:
iceberg_net:

volumes:
postgres-data:
minio-data:
    ```
</details>

The above file can be saved in the current working directory as `docker-compose.yml`. Run the following command to start the services:
```bash
docker compose up -d
```

**Setting up Port Forwarding**

To access OLake UI and Lakekeeper UI from your local machine, we need to set up port forwarding. Run the following commands in your local machine:

```bash
ssh -L 8000:localhost:8000 olake-server
ssh -L 8181:localhost:8181 olake-server
```

:::tip[SSH Configuration]
If you haven't configured an SSH alias, add this to your local `~/.ssh/config`:
```bash
Host olake-server
HostName <YOUR_INSTANCE_PUBLIC_IP>
User azureuser
IdentityFile <PATH_TO_YOUR_PEM_FILE>
```

Replace the values with your instance details. This lets you use `olake-server` instead of typing the full SSH command each time.
:::

Once you have started the services you can access the Lakekeeper UI at: http://localhost:8181/ui




**Step 2: Configure OLake for Iceberg**

Now configure OLake to write Iceberg tables using the REST catalog.

Access the OLake UI here: http://localhost:8000. 

Create a `job` and configure the `source` and `destination` in the OLake UI. Below is an example of how your source and destination configuration should look like. Replace the placeholder values (marked with `<...>`) with your actual credentials:

<Tabs>
    <TabItem value="source" label="Source" default>
        ```json
        {
            "host": "<HOST_NAME>",
            "port": 5432,
            "database": "<DATABASE_NAME>",
            "username": "<USERNAME>",
            "password": "<PASSWORD>",
            "ssl": {
                "mode": "require"
            }
        }
        ```
    </TabItem>
    <TabItem value="destination" label="Destination" default>
        ```json
        {
            "type": "ICEBERG",
            "writer": {
            "catalog_type": "rest",
            "rest_catalog_url": "http://lakekeeper:8181/catalog",
            "iceberg_s3_path": "<WAREHOUSE_NAME>",
            "iceberg_db": "<DESTINATION_DATABASE_NAME>",
            "s3_endpoint": "https://s3.us-east-1.amazonaws.com",
            "aws_region": "us-east-1",
            "aws_access_key": "<AWS_ACCESS_KEY>",
            "aws_secret_key": "<AWS_SECRET_KEY>"
            }
        }
        ```
    </TabItem>
</Tabs>

:::warning S3 Region Requirement
If you're using your own S3 bucket, ensure it's in the **us-east-1** region. We encountered issues with the Bauplan client accessing buckets in other AWS regions.
:::

**Step 3: Discover and Sync Tables**

First, discover the available tables from your Postgres database:

```bash
docker run --pull=always \
-v "[PATH_OF_CONFIG_FOLDER]:/mnt/config" \
olakego/source-postgres:latest \
discover \
--config /mnt/config/source.json
```

This returns a list of available streams (tables). Select the ones you want to replicate to Iceberg and save them to `streams.json`.

Now run the sync to transfer data:

```bash
docker run --pull=always \
-v "[PATH_OF_CONFIG_FOLDER]:/mnt/config" \
olakego/source-postgres:latest \
sync \
--config /mnt/config/source.json \
--streams /mnt/config/streams.json \
--destination /mnt/config/destination.json
```

OLake reads from your Postgres database and writes Iceberg tables to S3. All table metadata is automatically registered in Lakekeeper.

**Step 4: Set Up Bauplan**

Before we can register tables in Bauplan, you need to install the Bauplan client and configure authentication.

Install Bauplan:

```bash
pip install bauplan
```

Set up your Bauplan API key. You'll need to create an account at [Bauplan](https://www.bauplanlabs.com/) and generate an API key from the dashboard. Then configure it:

```bash
bauplan config set api_key <your_api_key>
```

For detailed installation instructions and configuration options, refer to the [Bauplan documentation](https://docs.bauplanlabs.com/).

**Step 5: Register Tables in Bauplan**

Now we need to make Bauplan aware of the Iceberg tables. This is a two-step process:

1. **Get the metadata location** - Use PyIceberg to fetch the `metadata.json` URL from S3 bucket
2. **Register in Bauplan** - Use that metadata URL to create an external table in Bauplan

:::note
Before running these scripts, update the following variables to match your setup:
- `CATALOG_URI` - Your Lakekeeper endpoint
- `LAKEKEEPER_WAREHOUSE` - Your warehouse name
- `S3_ENDPOINT`, `S3_ACCESS_KEY`, `S3_SECRET_KEY`, `S3_REGION` - Your S3 credentials
- `ICEBERG_NAMESPACE` - Your database name (e.g., `postgres_mydb_public`)
- `ICEBERG_TABLE` - The table name you want to register
- `BAUPLAN_TABLE_NAME` - How the table will appear in Bauplan
:::

<details>
  <summary>Python script: Get metadata location from Lakekeeper</summary>

```python
# get_lakekeeper_metadata.py
from pyiceberg.catalog import rest

# Configure your Lakekeeper catalog connection
CATALOG_URI = "http://localhost:8181/catalog"
LAKEKEEPER_WAREHOUSE = "lakeeper_warehouse_name"

# S3 credentials
S3_ENDPOINT = "https://s3.us-east-1.amazonaws.com"
S3_ACCESS_KEY = "YOUR_ACCESS_KEY"
S3_SECRET_KEY = "YOUR_SECRET_KEY"
S3_REGION = "us-east-1"

# Iceberg namespace and table
ICEBERG_NAMESPACE = "database_name"
ICEBERG_TABLE = "table_name"

def get_metadata_location(namespace: str, table_name: str) -> str:
    """
    Return the fully-qualified metadata.json location
    for an Iceberg table registered in Lakekeeper.
    """
    lakekeeper_catalog = rest.RestCatalog(
        name="default",
        type="rest",
        uri=CATALOG_URI,
        **{
            "warehouse": LAKEKEEPER_WAREHOUSE,
            "s3.endpoint": S3_ENDPOINT,
            "s3.access-key-id": S3_ACCESS_KEY,
            "s3.secret-access-key": S3_SECRET_KEY,
            "s3.region": S3_REGION,
        },
    )
    table_obj = lakekeeper_catalog.load_table(identifier=(namespace, table_name))
    return table_obj.metadata_location

if __name__ == "__main__":
    metadata_path = get_metadata_location(ICEBERG_NAMESPACE, ICEBERG_TABLE)
    print(f"Metadata location for {ICEBERG_NAMESPACE}.{ICEBERG_TABLE}:\n{metadata_path}")
```

</details>

<details>
  <summary>Python script: Register table in Bauplan</summary>

```python
# bauplan_register_table.py
import bauplan
from get_lakekeeper_metadata import get_metadata_location

# Keep in sync with get_lakekeeper_metadata.py
ICEBERG_NAMESPACE = "database_name"
ICEBERG_TABLE = "table_name"

# How the table will appear inside Bauplan
BAUPLAN_TABLE_NAME = f"{ICEBERG_NAMESPACE}__{ICEBERG_TABLE}"

# Resolve the current metadata.json location from Lakekeeper
metadata_location_string = get_metadata_location(
    namespace=ICEBERG_NAMESPACE,
    table_name=ICEBERG_TABLE,
)

# Instantiate Bauplan client
client = bauplan.Client()
bauplan_user = client.info().user.username

# Create a non-main branch for safe testing
branch_name = f"{bauplan_user}.olake_integration"
client.create_branch(branch=branch_name, from_ref="main", if_not_exists=True)

# Create the namespace first
try:
    client.create_namespace(namespace='olake', branch=branch_name)
    print(f"Created namespace 'olake' on branch {branch_name}")
except Exception as e:
    print(f"Namespace 'olake' might already exist or error: {e}")

client.create_external_table_from_metadata(
    table=BAUPLAN_TABLE_NAME,
    metadata_json_uri=metadata_location_string,
    branch=branch_name,
    namespace='olake',
    overwrite=True,
)

print(f"External table registered: {BAUPLAN_TABLE_NAME} on branch {branch_name}")
```

</details>

Save both scripts in the same directory. Then run:

```bash
python bauplan_register_table.py
```

This will fetch the metadata location and register the table as an external table in Bauplan.

**Step 6: Build and test on branches**

Now comes the fun part! The registration script created a development branch in Bauplan (named something like `{bauplan_user}.olake_integration`), and your Iceberg table is now registered there. This isolated branch is your sandbox—experiment freely without touching production data.

**Working with Branches in Bauplan**

First, verify that your branch was created successfully:

```bash
bauplan branch list
```

Switch to your development branch:

```bash
bauplan checkout {bauplan_user}.olake_integration
```

With the branch checked out, you can now explore and query your data:

```bash
# List all tables in your current branch
bauplan table ls

# Query your Iceberg table directly
bauplan query "SELECT * FROM olake.{table_name}"
```

**Merging Your Changes**

After testing your transformations, compare your branch against main to review the changes:

```bash
# Review differences between your branch and main
bauplan branch diff main
```

When you're confident in your changes, merge them to production:

```bash
# Switch to main branch
bauplan checkout main

# Merge your development branch
bauplan branch merge {bauplan_user}.olake_integration

# Verify the tables are now in main
bauplan table ls --namespace olake --ref main
```

:::tip Read-Only Limitation
In this tutorial, we're using an S3 bucket with public read-only access. As a result, write operations through Bauplan queries are not supported.
:::

{/*
```bash
# Query your data safely on the dev branch
bauplan query "SELECT * FROM olake.postgres_mydb_public__orders LIMIT 100" --ref {bauplan_user}.olake_integration

# Build derived tables with transformations
bauplan query "CREATE TABLE olake.orders_daily AS 
  SELECT date, COUNT(*) as count, SUM(total) as revenue 
  FROM olake.postgres_mydb_public__orders 
  GROUP BY date" --ref {bauplan_user}.olake_integration

# Test, iterate, and when you're confident—merge to main
bauplan branch merge {bauplan_user}.olake_integration --to main
```

*/}

## Conclusion

You've just built a complete data lakehouse stack that bridges operational databases and analytics—without vendor lock-in, without proprietary formats, and without complexity. OLake continuously syncs your Postgres data to Iceberg tables, Lakekeeper manages the metadata catalog, and Bauplan gives your team Git-style workflows for safe, collaborative data development.

What makes this architecture powerful isn't just the individual tools, but how they work together on open standards. Most importantly, your data team can now work like your engineering team—with branches, testing environments, code reviews, and safe production merges. No more "testing in production" or hoping your transformation doesn't break downstream dashboards.

You now have a working pipeline that demonstrates how these tools fit together. From here, you can explore CDC for real-time updates, add more tables, or experiment with transformations in Bauplan. The open architecture gives you room to grow as your needs evolve.

## Learn More

- [OLake Documentation](https://olake.io/docs) - Complete guide to setting up OLake with various sources and destinations
- [Bauplan Documentation](https://docs.bauplanlabs.com) - Learn about branch workflows and data transformations
- [Lakekeeper](https://lakekeeper.io) - Open-source Iceberg REST catalog
- [Apache Iceberg](https://iceberg.apache.org) - The open table format powering this architecture