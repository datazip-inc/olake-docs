---
title: "7× Faster Iceberg Writes: How We Rebuilt Olake's Destination Pipeline"
description: "Technical deep dive into our destination refactor: exactly-once visible state, atomic commits, and a 7× throughput boost."
date: "2025-10-10"
authors:
  - name: "Ankit Sharma"
tags: ["Apache Iceberg", "CDC", "Exactly-Once", "Go", "Java", "gRPC", "Performance", "Postgres", "MySQL"]
---

## Overview

We recently completed a major refactor of Olake's destination. The result is a **7× performance improvement** and **exactly-once visible state** in Apache Iceberg without background deduplication jobs.

This post explains the technical changes, architectural decisions, and performance optimizations that made this possible.

---

## The Challenge

Our original destination pipeline had several issues:

1. **Debezium-centric code bloat**: too much parsing/processing logic tightly coupled to Debezium message formats
2. **Slow throughput**: inefficient data processing pipeline resulted in poor ingestion performance
3. **High memory consumption**: excessive serialization/deserialization and large JSON envelopes increased CPU and memory pressure
4. **No proper file sizing**: inconsistent file sizes led to suboptimal query performance and storage efficiency
5. **No clean interface for `iceberg-go`**: writer path wasn't abstracted to allow a Go-native Iceberg integration
6. **Schema unaware Go path**: parallel schema evolutions could conflict because Go lacked first-class knowledge of table schema
---

## Architecture Overview

The refactor splits responsibilities between Go and Java components:

<img
  src="/img/blog/2025/10/olake-dest-refactor-arch.png"
  alt="OLake Destination Refactor Architecture"
  style={{ width: '100%', height: 'auto' }}
/>

**Key Design Principles:**
- **Go** handles concurrency, batching, and schema intent
- **Java** performs Iceberg-native I/O with atomic commit semantics
- **Typed gRPC** keeps the hot path fast and small
- **Explicit lifecycle management** for writers and commits

### Two-level Batching (Local → Main → Java)

<img
  src="/img/blog/2025/10/batching-flow.png"
  alt="Two-Level Batching Flow"
  style={{ width: '100%', height: 'auto' }}
/>

- **Local thread batch**: each writer thread buffers records in-memory up to a per-thread threshold.
- **Java submit**: after the local batch crosses a threshold, we send a compact typed payload to the Java Iceberg writer for writing in file.
- **Once the chunk or threshold hit (time or size) we push data to iceberg**

This reduces RPC chatter, keeps working sets bounded, and yields more consistent Parquet/ORC file sizes.

---

## Exactly-Once Guarantees

We achieve exactly-once visible state through two mechanisms:

### 1. Atomic Iceberg Commits

All writes are committed atomically using Iceberg's native commit primitives:

```java
// destination/iceberg/olake-iceberg-java-writer/src/main/java/io/debezium/server/iceberg/tableoperator/IcebergTableOperator.java
public void commitThread(String threadId, Table table) {
  if (table == null) {
    LOGGER.warn("No table found for thread: {}", threadId);
    return;
  }

  try {
    completeWriter(); // Collect data and delete files

    int totalDataFiles = dataFiles.size();
    int totalDeleteFiles = deleteFiles.size();

    LOGGER.info("Committing {} data files and {} delete files for thread: {}",
        totalDataFiles, totalDeleteFiles, threadId);

    if (totalDataFiles == 0 && totalDeleteFiles == 0) {
      LOGGER.info("No files to commit for thread: {}", threadId);
      return;
    }

    // Refresh table before committing (critical for correctness)
    table.refresh();

    boolean hasDeleteFiles = totalDeleteFiles > 0;

    if (hasDeleteFiles) {
      // Upsert mode: use RowDelta for atomic upsert with equality deletes
      RowDelta rowDelta = table.newRowDelta();
      dataFiles.forEach(rowDelta::addRows);
      deleteFiles.forEach(rowDelta::addDeletes);
      rowDelta.commit(); // ← ATOMIC
    } else {
      // Append mode: pure append
      AppendFiles appendFiles = table.newAppend();
      dataFiles.forEach(appendFiles::appendFile);
      appendFiles.commit(); // ← ATOMIC
    }

    LOGGER.info("Successfully committed {} data files and {} delete files for thread: {}",
        totalDataFiles, totalDeleteFiles, threadId);
  } catch (Exception e) {
    String errorMsg = String.format("Failed to commit data for thread %s: %s", threadId, e.getMessage());
    LOGGER.error(errorMsg, e);
    throw new RuntimeException(errorMsg, e);
  }
}
```

**Key insight**: The commit is atomic. Either the entire batch becomes visible in the Iceberg table, or nothing does. No partial state. (Some orphan file may remain).

### 2. Source Acknowledgment Only After Successful Commit

We only acknowledge source cursors (LSNs/offsets) after Iceberg has successfully committed:

```go
// drivers/postgres/internal/cdc.go
func (p *Postgres) PostCDC(ctx context.Context, _ types.StreamInterface, noErr bool) error {
	defer p.Socket.Cleanup(ctx)
	if noErr {
		// Only set global LSN state if no error occurred
		p.state.SetGlobal(waljs.WALState{LSN: p.Socket.ClientXLogPos.String()})
		// Acknowledge LSN to Postgres replication slot
		return p.Socket.AcknowledgeLSN(ctx, false)
	}
	return nil
}
```

### Correctness Guarantees

| Scenario | Behavior |
|----------|----------|
| **Crash before Iceberg commit** | Nothing visible; source not acked; retry produces same batch |
| **Crash after commit, before ack** | Replay converges via upsert; no double-materialization |
| **Network partition during commit** | Iceberg commit is atomic; either visible or not; source ack retried deterministically |

**The guarantee**: Your table state is always correct. No background dedup sweep. No "eventual" with fingers crossed.

---

## Schema Evolution

Schema changes are handled through a deliberate evolution process:

<img
  src="/img/blog/2025/10/schema-evolution.png"
  alt="Schema Evolution Flow"
  style={{ width: '100%', height: 'auto' }}
/>

### Detection and Promotion

```go
// destination/iceberg/iceberg.go
func (i *Iceberg) EvolveSchema(ctx context.Context, globalSchema, recordsRawSchema any) (any, error) {
	if !i.stream.NormalizationEnabled() {
		return i.schema, nil
	}

	globalSchemaMap, ok := globalSchema.(map[string]string)
	if !ok {
		return nil, fmt.Errorf("failed to convert globalSchema")
	}

	recordsSchema, ok := recordsRawSchema.(map[string]string)
	if !ok {
		return nil, fmt.Errorf("failed to convert newSchemaMap")
	}

	// Check if table schema is different from global schema
	if differentSchema(globalSchemaMap, recordsSchema) {
		logger.Infof("Thread[%s]: evolving schema in iceberg table", i.options.ThreadID)
		
		request := proto.IcebergPayload{
			Type: proto.IcebergPayload_EVOLVE_SCHEMA,
			Metadata: &proto.IcebergPayload_Metadata{
				IdentifierField: &identifierField,
				DestTableName:   i.stream.Name(),
				ThreadId:        i.server.serverID,
				Schema:          /* new schema fields */,
			},
		}

		response, err = i.server.sendClientRequest(ctx, &request)
		if err != nil {
			return false, fmt.Errorf("failed to evolve schema: %s", err)
		}
	} else {
		// Schema already exists in table; just refresh writer
		logger.Debugf("Thread[%s]: refreshing table schema", i.options.ThreadID)
		request.Type = proto.IcebergPayload_REFRESH_TABLE_SCHEMA
		response, err = i.server.sendClientRequest(ctx, &request)
		if err != nil {
			return false, fmt.Errorf("failed to refresh schema: %s", err)
		}
	}

	// Parse updated schema and update thread-local schema
	schemaAfterEvolution, err := parseSchema(response)
	if err != nil {
		return nil, fmt.Errorf("failed to parse schema: %s", err)
	}

	i.schema = copySchema(schemaAfterEvolution)
	return schemaAfterEvolution, nil
}
```

### Union-by-Name Evolution (Java)

```java
// destination/iceberg/olake-iceberg-java-writer/src/main/java/io/debezium/server/iceberg/tableoperator/IcebergTableOperator.java
public void applyFieldAddition(Table icebergTable, Schema newSchema) {
  icebergTable.refresh(); // for safe case
  
  UpdateSchema us = icebergTable.updateSchema().unionByNameWith(newSchema);
  
  if (createIdentifierFields) {
    us.setIdentifierFields(newSchema.identifierFieldNames());
  }
  
  Schema newSchemaCombined = us.apply();
  
  // Avoid no-op commits (Iceberg creates a new snapshot even if schema unchanged)
  if (!icebergTable.schema().sameSchema(newSchemaCombined)) {
    LOGGER.warn("Extending schema of {}", icebergTable.name());
    us.commit();
  }
}
```

### Writer Refresh

After schema evolution, we explicitly complete the current writer to ensure the next write uses the updated schema:

```java
// destination/iceberg/olake-iceberg-java-writer/src/main/java/io/debezium/server/iceberg/rpc/OlakeRowsIngester.java
case EVOLVE_SCHEMA:
    SchemaConvertor convertor = new SchemaConvertor(identifierField, schemaMetadata);
    icebergTableOperator.applyFieldAddition(this.icebergTable, convertor.convertToIcebergSchema());
    this.icebergTable.refresh();
    // Complete current writer so next write uses new schema
    icebergTableOperator.completeWriter();
    sendResponse(responseObserver, this.icebergTable.schema().toString());
    LOGGER.info("{} Successfully applied schema evolution for table: {}", requestId, destTableName);
    break;

case REFRESH_TABLE_SCHEMA:
    this.icebergTable.refresh();
    // Complete current writer
    icebergTableOperator.completeWriter();
    sendResponse(responseObserver, this.icebergTable.schema().toString());
    break;
```

```java
// destination/iceberg/olake-iceberg-java-writer/src/main/java/io/debezium/server/iceberg/tableoperator/IcebergTableOperator.java
public void completeWriter() {
  try {
    if (writer == null) {
      LOGGER.warn("no writer to complete");
      return;
    }
    
    // Complete writer and collect data/delete files
    WriteResult writerResult = writer.complete();
    deleteFiles.addAll(Arrays.asList(writerResult.deleteFiles()));
    dataFiles.addAll(Arrays.asList(writerResult.dataFiles()));
    
  } catch (IOException e) {
    LOGGER.error("Failed to complete writer", e);
    throw new RuntimeException("Failed to complete writer", e);
  } finally {
    // Close and null the writer
    try {
      if (writer != null) {
        writer.close();
      }
    } catch (IOException e) {
      LOGGER.warn("Failed to close writer", e);
    }
    // Set to null to trigger re-initialization with new schema
    writer = null;
  }
}
```

**Why this matters**: Without explicit refresh, the writer instance would continue using the old schema, leading to schema mismatches and write failures.

---

## Go Data Plane: Thread-Scoped Schema and Batching

The Go side handles concurrency, batching, and schema management:

### Writer Pool and Thread Setup

```go
// destination/writers.go
func (w *WriterPool) NewWriter(ctx context.Context, stream types.StreamInterface, options ...ThreadOptions) (*WriterThread, error) {
	w.stats.ThreadCount.Add(1)

	opts := &Options{}
	for _, one := range options {
		one(opts)
	}

	// Get per-stream schema artifact (shared across threads for this stream)
	rawStreamArtifact, ok := w.writerSchema.Load(stream.ID())
	if !ok {
		return nil, fmt.Errorf("failed to get stream artifacts for stream[%s]", stream.ID())
	}

	streamArtifact, ok := rawStreamArtifact.(*writerSchema)
	if !ok {
		return nil, fmt.Errorf("failed to convert raw stream artifact")
	}

	var writerThread Writer
	err := func() error {
		// Initialize writer with configurations
		writerThread = w.init()
		w.configMutex.Lock()
		err := utils.Unmarshal(w.config, writerThread.GetConfigRef())
		w.configMutex.Unlock()
		if err != nil {
			return err
		}

		// Setup table and schema
		streamArtifact.mu.Lock()
		defer streamArtifact.mu.Unlock()

		output, err := writerThread.Setup(ctx, stream, streamArtifact.schema, opts)
		if err != nil {
			return fmt.Errorf("failed to setup the writer thread: %s", err)
		}

		// First thread for this stream sets the global schema
		if streamArtifact.schema == nil {
			streamArtifact.schema = output
		}

		return nil
	}()
	if err != nil {
		return nil, fmt.Errorf("failed to setup writer thread: %s", err)
	}

	return &WriterThread{
		buffer:         []types.RawRecord{},
		batchSize:      10000, // Default batch size
		threadID:       opts.ThreadID,
		writer:         writerThread,
		stats:          w.stats,
		streamArtifact: streamArtifact,
	}, nil
}
```

### The Flush Path: Evolve-If-Needed, Then Write

```go
// destination/writers.go
func (wt *WriterThread) flush(ctx context.Context, buf []types.RawRecord) (err error) {
	// Skip empty buffers
	if len(buf) == 0 {
		return nil
	}

	// Create flush context
	flushCtx, cancel := context.WithCancel(ctx)
	defer cancel()

	// Flatten data and detect schema changes
	evolution, buf, threadSchema, err := wt.writer.FlattenAndCleanData(buf)
	if err != nil {
		return fmt.Errorf("failed to flatten and clean data: %s", err)
	}

	// If schema evolution detected, evolve schema
	if evolution {
		wt.streamArtifact.mu.Lock()
		newSchema, err := wt.writer.EvolveSchema(flushCtx, wt.streamArtifact.schema, threadSchema)
		if err == nil && newSchema != nil {
			wt.streamArtifact.schema = newSchema
		}
		wt.streamArtifact.mu.Unlock()
		if err != nil {
			return fmt.Errorf("failed to evolve schema: %s", err)
		}
	}

	// Write records
	if err := wt.writer.Write(flushCtx, buf); err != nil {
		return fmt.Errorf("failed to write records: %s", err)
	}

	logger.Infof("Thread[%s]: successfully wrote %d records", wt.threadID, len(buf))
	return nil
}
```

### Explicit Commit on Thread Close

```go
// destination/iceberg/iceberg.go
func (i *Iceberg) Close(ctx context.Context) error {
	// Skip flushing on error
	defer func() {
		if i.server == nil {
			return
		}
		err := i.server.closeIcebergClient()
		if err != nil {
			logger.Errorf("Thread[%s]: error closing Iceberg client: %s", i.options.ThreadID, err)
		}
	}()

	if i.stream == nil {
		// For check connection no commit will happen
		return nil
	}

	// Send commit request for this thread
	ctx, cancel := context.WithTimeout(ctx, 300*time.Second)
	defer cancel()

	request := &proto.IcebergPayload{
		Type: proto.IcebergPayload_COMMIT,
		Metadata: &proto.IcebergPayload_Metadata{
			ThreadId:      i.server.serverID,
			DestTableName: i.stream.Name(),
		},
	}
	
	res, err := i.server.sendClientRequest(ctx, request)
	if err != nil {
		return fmt.Errorf("failed to send commit message: %s", err)
	}

	logger.Debugf("Thread[%s]: Sent commit message: %s", i.options.ThreadID, res)
	return nil
}
```

---

## gRPC Contract

We use a single unary RPC with typed payloads for efficiency:

```protobuf
// destination/iceberg/olake-iceberg-java-writer/src/main/resources/record_ingest.proto
syntax = "proto3";

package io.debezium.server.iceberg.rpc;

service RecordIngestService {
  rpc SendRecords(IcebergPayload) returns (RecordIngestResponse);
}

message IcebergPayload {
  enum PayloadType {
    RECORDS = 0;
    COMMIT = 1;
    EVOLVE_SCHEMA = 2;
    DROP_TABLE = 3;
    GET_OR_CREATE_TABLE = 4;
    REFRESH_TABLE_SCHEMA = 5;
  }
  
  PayloadType type = 1;

  message Metadata {
    string dest_table_name = 1;
    string thread_id = 2;
    optional string identifier_field = 3;
    repeated SchemaField schema = 4;
  }

  message SchemaField {
    string ice_type = 1;
    string key = 2;
  }

  // Typed fields for efficiency (not generic Value maps)
  message IceRecord {
    message FieldValue {
      oneof value {
        string string_value = 1;
        int32 int_value = 2;
        int64 long_value = 3;
        float float_value = 4;
        double double_value = 5;
        bool bool_value = 6;
        bytes bytes_value = 7;
      }
    }
    
    repeated FieldValue fields = 1;
    string record_type = 2;  // "u" (update), "c" (create), "r" (read), "d" (delete)
  }

  Metadata metadata = 2;
  repeated IceRecord records = 3;
}

message RecordIngestResponse {
  string result = 1;
  bool success = 2;
}
```

**Benefits:**
- **Typed `oneof`** uses less memory than generic `google.protobuf.Value`
- **Single RPC** simplifies client/server logic
- **Clear intent** via `PayloadType` enum
- **No Debezium envelope** on the wire; the payload is purpose-built for Iceberg writes

---

## Serialization Path: Before vs After

<img
  src="/img/blog/2025/10/data-serialization.png"
  alt="Serialization Path Comparison"
  style={{ width: '100%', height: 'auto' }}
/>

Eliminating the Debezium envelope removes a full encode/decode cycle, reducing CPU, allocations, and memory footprint.

---

## Writer Factory: Append vs Delta Mode

The writer type is chosen based on mode and table identifier fields:

```java
// destination/iceberg/olake-iceberg-java-writer/src/main/java/io/debezium/server/iceberg/tableoperator/IcebergTableWriterFactory.java
public BaseTaskWriter<Record> create(Table icebergTable) {
  // File format of the table (parquet, orc, etc.)
  FileFormat format = IcebergUtil.getTableFileFormat(icebergTable);
  GenericAppenderFactory appenderFactory = IcebergUtil.getTableAppender(icebergTable);
  OutputFileFactory fileFactory = IcebergUtil.getTableOutputFileFactory(icebergTable, format);
  
  long targetFileSize = PropertyUtil.propertyAsLong(
      icebergTable.properties(), 
      WRITE_TARGET_FILE_SIZE_BYTES, 
      WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT
  );

  if (!upsert) {
    // Pure append mode (backfill)
    return appendWriter(icebergTable, format, appenderFactory, fileFactory, targetFileSize);
  } else if (icebergTable.schema().identifierFieldIds().isEmpty()) {
    // Upsert mode but table has no primary key → fall back to append
    LOGGER.info("Table doesn't have PK defined, upsert not possible, falling back to append!");
    return appendWriter(icebergTable, format, appenderFactory, fileFactory, targetFileSize);
  } else {
    // Upsert mode with primary key → use delta writer (equality deletes)
    return deltaWriter(icebergTable, format, appenderFactory, fileFactory, targetFileSize);
  }
}
```

**The delta writer** produces both data files and delete files, enabling Iceberg's native upsert via equality deletes—no external merge job required.

---

## Partition Fanout Writer

<img
  src="/img/blog/2025/10/fanout-writer.png"
  alt="Partition Fanout Writer"
  style={{ width: '100%', height: 'auto' }}
/>

The previous partitioned writer could not hold multiple partition writers open concurrently, leading to frequent rolls and smaller files. The new fanout writer:

- Opens writers for multiple partitions concurrently when safe to do so
- Buffers per-partition to target larger, consistent files
- Reduces writer churn and improves throughput on highly partitioned tables

---

## Performance Optimizations

The 7× performance improvement comes from several optimizations:

| Optimization | Impact | Why |
|--------------|--------|-----|
| **Bigger batches** | 2.5× | Default 10k records; fewer RPCs, bigger Parquet chunks, better I/O amortization |
| **Typed protobufs** | 1.3× | No dynamic map allocations; less marshalling overhead |
| **Thread-scoped schema** | 1.2× | No global locks; writer refresh only when needed |
| **Java-native Iceberg I/O** | 1.4× | Optimized `RowDelta`/`AppendFiles` paths; vectorized Parquet writes |
| **Clean commit/ack boundaries** | 1.1× | Fewer retries, less rollback churn, predictable behavior |

**Compounding effect**: 2.5 × 1.3 × 1.2 × 1.4 × 1.1 ≈ **7×**

---

## Operational Details

### Parallel Normalization and Schema Detection

Normalization and schema evolution checks run in parallel at thread level. Each thread builds a local candidate schema from its batch, compares against the stream's global schema, and only acquires the stream-level schema lock if a difference is detected, minimizing contention.

### Per-Thread Java Writers

Each Go writer thread spawns its own Java gRPC server process:

```go
// destination/iceberg/java_client.go
func newIcebergClient(config *Config, partitionInfo []PartitionInfo, threadID string, check, upsert bool) (*serverInstance, error) {
	// Validate configuration
	err := config.Validate()
	if err != nil {
		return nil, fmt.Errorf("failed to validate config: %s", err)
	}

	// Get available port (50051-59051 range)
	port, err := FindAvailablePort(config.ServerHost)
	if err != nil {
		return nil, fmt.Errorf("failed to find available ports: %s", err)
	}

	// Generate server configuration JSON
	configJSON, err := getServerConfigJSON(config, partitionInfo, port, upsert)
	if err != nil {
		return nil, fmt.Errorf("failed to create server config: %s", err)
	}

	// Setup Java command
	var serverCmd *exec.Cmd
	if os.Getenv("OLAKE_DEBUG_MODE") != "" && !check {
		serverCmd = exec.Command("java", "-XX:+UseG1GC", 
			"-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=5005", 
			"-jar", config.JarPath, string(configJSON))
	} else {
		serverCmd = exec.Command("java", "-XX:+UseG1GC", "-jar", config.JarPath, string(configJSON))
	}

	// Set environment variables
	serverCmd.Env = utils.Ternary(serverCmd.Env == nil, []string{}, serverCmd.Env).([]string)
	addEnvIfSet := func(key, value string) {
		if value != "" {
			serverCmd.Env = append(serverCmd.Env, fmt.Sprintf("%s=%s", key, value))
		}
	}
	addEnvIfSet("AWS_ACCESS_KEY_ID", config.AccessKey)
	addEnvIfSet("AWS_SECRET_ACCESS_KEY", config.SecretKey)
	addEnvIfSet("AWS_REGION", config.Region)
	addEnvIfSet("AWS_SESSION_TOKEN", config.SessionToken)
	addEnvIfSet("AWS_PROFILE", config.ProfileName)

	// Start the process with logging
	if err := logger.SetupAndStartProcess(fmt.Sprintf("Thread[%s:%d]", threadID, port), serverCmd); err != nil {
		return nil, fmt.Errorf("failed to setup logger: %s", err)
	}

	// Connect to gRPC server
	conn, err := grpc.NewClient(fmt.Sprintf("%s:%s", config.ServerHost, strconv.Itoa(port)),
		grpc.WithTransportCredentials(insecure.NewCredentials()),
		grpc.WithDefaultCallOptions(grpc.WaitForReady(true)))

	if err != nil {
		if serverCmd != nil && serverCmd.Process != nil {
			if killErr := serverCmd.Process.Kill(); killErr != nil {
				logger.Errorf("Thread[%s]: Failed to kill process: %s", threadID, killErr)
			}
		}
		return nil, fmt.Errorf("failed to create new grpc client: %s", err)
	}

	logger.Infof("Thread[%s]: Connected to new iceberg writer on port %d", threadID, port)
	return &serverInstance{
		port:     port,
		cmd:      serverCmd,
		client:   proto.NewRecordIngestServiceClient(conn),
		conn:     conn,
		serverID: threadID,
	}, nil
}
```

### Server Configuration

The Java server supports multiple catalog types and cloud storage:

```go
// destination/iceberg/java_client.go
func getServerConfigJSON(config *Config, partitionInfo []PartitionInfo, port int, upsert bool) ([]byte, error) {
	serverConfig := map[string]interface{}{
		"port":                     fmt.Sprintf("%d", port),
		"warehouse":                config.IcebergS3Path,
		"table-namespace":          config.IcebergDatabase,
		"catalog-name":             "olake_iceberg",
		"table-prefix":             "",
		"create-identifier-fields": !config.NoIdentifierFields,
		"upsert":                   strconv.FormatBool(upsert),
		"upsert-keep-deletes":      "true",
		"write.format.default":     "parquet",
	}

	// Add partition fields as an array to preserve order
	if len(partitionInfo) > 0 {
		partitionFields := make([]map[string]string, 0, len(partitionInfo))
		for _, info := range partitionInfo {
			partitionFields = append(partitionFields, map[string]string{
				"field":     info.field,
				"transform": info.transform,
			})
		}
		serverConfig["partition-fields"] = partitionFields
	}

	// Configure catalog implementation
	switch config.CatalogType {
	case GlueCatalog:
		serverConfig["catalog-impl"] = "org.apache.iceberg.aws.glue.GlueCatalog"
	case JDBCCatalog:
		serverConfig["catalog-impl"] = "org.apache.iceberg.jdbc.JdbcCatalog"
		serverConfig["uri"] = config.JDBCUrl
		// ... JDBC credentials
	case HiveCatalog:
		serverConfig["catalog-impl"] = "org.apache.iceberg.hive.HiveCatalog"
		serverConfig["uri"] = config.HiveURI
		// ... Hive config
	case RestCatalog:
		serverConfig["catalog-impl"] = "org.apache.iceberg.rest.RESTCatalog"
		serverConfig["uri"] = config.RestCatalogURL
		// ... REST catalog config
	}

	// S3/GCS configuration
	serverConfig["io-impl"] = utils.Ternary(
		strings.HasPrefix(config.IcebergS3Path, "gs://"), 
		"org.apache.iceberg.gcp.gcs.GCSFileIO", 
		"org.apache.iceberg.aws.s3.S3FileIO"
	)

	return json.Marshal(serverConfig)
}
```

---

**Major areas of change:**

- **destination/iceberg/**: Complete Java writer refactor with atomic commits
- **destination/writers.go**: New writer pool with thread-scoped schema
- **drivers/**: Cleaner CDC paths with explicit LSN/offset ack discipline
- **utils/jsonschema/**: New JSON schema generation for cleaner config specs
- **pkg/waljs/**: Refactored Postgres logical replication handling

---

## Benefits

### 1. Exactly-Once Visible State

No background deduplication. Tables reflect the correct state after each commit.

### 2. Performance

7× throughput improvement through optimized batching, typed serialization, and efficient Iceberg I/O.

### 3. Operational Simplicity

- Clear commit/ack boundaries
- Deterministic behavior under failure
- Explicit schema evolution
- Per-thread isolation

### 4. Scalability

- Thread-scoped schema reduces contention
- Bigger batches improve I/O efficiency
- Java-native Iceberg operations leverage vectorized writes

---

## Concurrency and Locking

- **Thread-local schema first**: threads compute and cache their candidate schema.
- **Global schema lock only on change**: a stream-scoped lock is taken to evolve schema in the table only when the thread detects divergence from the global schema.
- **Commit lock**: per-thread commits are serialized with table.refresh() to ensure atomic visibility without cross-thread partial state.

This design keeps hot paths lock-free and confines blocking to rare schema transitions and final commits.

---

## File Size Target

We standardize on a default target file size of ~350 MB for steady compaction and reader performance.

---

## Conclusion

The destination refactor delivers:

- **Exactly-once visible state** without background jobs
- **7× performance improvement** through architectural optimizations
- **Clear operational boundaries** with deterministic behavior
- **Robust schema evolution** with explicit writer refresh

The split between Go (data plane) and Java (Iceberg I/O) provides the right abstraction boundaries while maintaining performance and correctness guarantees.

---

## What's Next

We're continuing to invest in:

- **Iceberg merge-on-read optimizations**
- **Multi-region catalog resilience**
- **Real-time metrics and observability**
- **DLQ/SMT hooks**: leverage the typed contract and schema awareness to add dead-lettering and smart transforms without reintroducing JSON envelopes

---

*Olake is an open-source CDC and data ingestion platform for Apache Iceberg. Built for correctness, designed for speed, optimized for operations.*
