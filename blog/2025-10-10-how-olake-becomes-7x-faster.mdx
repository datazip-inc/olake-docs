---
slug: how-olake-becomes-7x-faster
title: "7× Faster Iceberg Writes: How We Rebuilt Olake's Destination Pipeline"
description: "Technical deep dive into our destination refactor: exactly-once visible state, atomic commits, and a 7× throughput boost."
date: "2025-10-10"
authors: [ankit]
tags: ["Apache Iceberg", "CDC", "Exactly-Once", "Go", "Java", "gRPC", "Performance", "Postgres", "MySQL"]
image: /img/blog/cover/how-olake-becomes-7x-faster-cover.webp
---

## Overview

Data ingestion performance is critical for modern data platforms. When your CDC pipeline becomes a bottleneck, it affects everything downstream—from real-time analytics to machine learning workflows. We recently faced this exact challenge with Olake's destination pipeline and decided to rebuild it from the ground up.

The result? A **7× performance improvement** and **exactly-once visible state** in Apache Iceberg—without the complexity of background deduplication jobs or eventual consistency mechanisms.

This blog takes you through the technical journey: the challenges we faced, the architectural decisions we made, and the specific optimizations that delivered these dramatic improvements. Whether you're building data pipelines, optimizing existing systems, or simply curious about high-performance data engineering, there are insights here for you.

---

## The Challenge

Our original destination pipeline had several issues:

1. **Debezium-centric code bloat**: too much parsing/processing logic tightly coupled to Debezium message formats
2. **Slow throughput**: inefficient data processing pipeline resulted in poor ingestion performance
3. **High memory consumption**: excessive serialization/deserialization and large JSON envelopes increased CPU and memory pressure
4. **No proper file sizing**: inconsistent file sizes led to suboptimal query performance and storage efficiency
5. **Schema unaware Go path**: parallel schema evolutions could conflict because Go lacked first-class knowledge of table schema
---

## Architecture Overview

The refactor fundamentally changes how we handle data processing by splitting responsibilities between Go and Java components based on their respective strengths. This separation of concerns allows each component to focus on what it does best while maintaining clean interfaces between them.

<img
  src="/img/blog/2025/10/how-olake-becomes-7x-faster-1.webp"
  alt="OLake Destination Refactor Architecture"
  style={{ width: '100%', height: 'auto' }}
/>

**Key Design Principles:**

- **Go Data Plane**: Handles concurrency, intelligent batching, and schema coordination. Go's strengths in concurrent programming and lightweight threading make it ideal for managing multiple writer threads and coordinating schema evolution across streams.

- **Java Iceberg I/O**: Performs native Iceberg operations with atomic commit semantics. Java's mature Iceberg ecosystem and optimized I/O libraries provide the most efficient path for file operations and table management.

- **Typed gRPC Interface**: Maintains a fast, efficient communication layer between components. The strongly-typed contract eliminates serialization overhead and provides compile-time safety.

- **Explicit Lifecycle Management**: Ensures proper resource management and cleanup. Writers and commits have well-defined lifecycles that prevent resource leaks and ensure data consistency.

This architecture enables each component to operate at peak efficiency while maintaining the strong consistency guarantees required for production data pipelines.

### Two-level Batching (Local Batch → Java Writer → Commit )

The two-level batching system is a key optimization that balances memory usage, network efficiency, and file size consistency. This approach ensures that we make optimal use of both local memory and network resources while producing well-sized files for downstream processing.

<img
  src="/img/blog/2025/10/how-olake-becomes-7x-faster-2.webp"
  alt="Two-Level Batching Flow"
  style={{ width: '100%', height: 'auto' }}
/>

**How the two-level batching works:**

- **Local thread batch**: Each writer thread buffers records in-memory up to a per-thread threshold (typically 10,000 records). This local buffering allows for efficient in-memory processing and schema detection before sending data over the network.

- **Java writer**: After the local batch crosses a threshold (either record count or time-based), we send a compact, typed payload to the Java Iceberg writer. This payload is optimized for Iceberg operations and eliminates unnecessary serialization overhead. The Java writer automatically closes files and pushes them to storage once the target file size is reached.

- **Data visibility after commit**: Files are created and data is written, but it only becomes visible in the Iceberg table after an explicit commit operation. This ensures atomic visibility and maintains exactly-once semantics.

**Benefits of this approach:**

- **Reduced RPC chatter**: Fewer, larger batches mean fewer network round trips
- **Bounded working sets**: Memory usage is predictable and controlled
- **Consistent file sizes**: Better query performance and storage efficiency
- **Efficient processing**: Local batching allows for optimizations like schema detection and data flattening before network transmission

---

## Exactly-Once Guarantees

Exactly-once semantics is one of the most critical requirements for data pipelines. In traditional CDC systems, achieving exactly-once guarantees often requires complex background deduplication jobs or eventual consistency mechanisms. Our refactored destination pipeline achieves exactly-once visible state through two key mechanisms that work together:

### 1. Atomic Iceberg Commits

The foundation of our exactly-once guarantee lies in Apache Iceberg's native atomic commit primitives. Unlike traditional file-based systems where partial writes can leave tables in inconsistent states, Iceberg's commit model ensures that either all changes in a batch become visible simultaneously, or none do.

Here's how our atomic commit mechanism works:

```java title="IcebergTableOperator.java"
// destination/iceberg/olake-iceberg-java-writer/src/main/java/io/debezium/server/iceberg/tableoperator/IcebergTableOperator.java
public void commitThread(String threadId, Table table) {
  if (table == null) {
    LOGGER.warn("No table found for thread: {}", threadId);
    return;
  }

  try {
    completeWriter(); // Collect data and delete files

    int totalDataFiles = dataFiles.size();
    int totalDeleteFiles = deleteFiles.size();

    LOGGER.info("Committing {} data files and {} delete files for thread: {}",
        totalDataFiles, totalDeleteFiles, threadId);

    if (totalDataFiles == 0 && totalDeleteFiles == 0) {
      LOGGER.info("No files to commit for thread: {}", threadId);
      return;
    }

    // Refresh table before committing (critical for correctness)
    table.refresh();

    boolean hasDeleteFiles = totalDeleteFiles > 0;

    if (hasDeleteFiles) {
      // Upsert mode: use RowDelta for atomic upsert with equality deletes
      RowDelta rowDelta = table.newRowDelta();
      dataFiles.forEach(rowDelta::addRows);
      deleteFiles.forEach(rowDelta::addDeletes);
      rowDelta.commit(); // ← ATOMIC
    } else {
      // Append mode: pure append
      AppendFiles appendFiles = table.newAppend();
      dataFiles.forEach(appendFiles::appendFile);
      appendFiles.commit(); // ← ATOMIC
    }

    LOGGER.info("Successfully committed {} data files and {} delete files for thread: {}",
        totalDataFiles, totalDeleteFiles, threadId);
  } catch (Exception e) {
    String errorMsg = String.format("Failed to commit data for thread %s: %s", threadId, e.getMessage());
    LOGGER.error(errorMsg, e);
    throw new RuntimeException(errorMsg, e);
  }
}
```

**Key insight**: The commit is atomic. Either the entire batch becomes visible in the Iceberg table, or nothing does. No partial state. This atomicity is crucial because it means readers will never see inconsistent intermediate states, even during concurrent operations.

The commit process handles two scenarios:
- **Append mode**: For pure inserts (like backfill operations), we use `AppendFiles` which simply adds new data files to the table
- **Upsert mode**: For CDC operations with updates/deletes, we use `RowDelta` which atomically adds both data files and delete files, enabling Iceberg's native upsert semantics

### 2. Source Acknowledgment Only After Successful Commit

The second pillar of our exactly-once guarantee is the careful ordering of operations. We only acknowledge source cursors (LSNs/offsets) after Iceberg has successfully committed the data. This ensures that if a failure occurs, the source will replay the same data, and our system will handle it correctly through upsert semantics.

Here's how the acknowledgment mechanism works:

```go title="cdc.go"
// drivers/postgres/internal/cdc.go
func (p *Postgres) PostCDC(ctx context.Context, _ types.StreamInterface, noErr bool) error {
	defer p.Socket.Cleanup(ctx)
	if noErr {
		// Only set global LSN state if no error occurred
		p.state.SetGlobal(waljs.WALState{LSN: p.Socket.ClientXLogPos.String()})
		// Acknowledge LSN to Postgres replication slot
		return p.Socket.AcknowledgeLSN(ctx, false)
	}
	return nil
}
```

### Correctness Guarantees

The combination of atomic commits and careful acknowledgment ordering provides strong correctness guarantees across all failure scenarios:

| Scenario | Behavior | Why It Works |
|----------|----------|--------------|
| **Crash before Iceberg commit** | Nothing visible; source not acked; retry produces same batch | Atomic commit ensures no partial state is visible; source replay provides idempotency |
| **Crash after commit, before ack** | Replay converges via upsert; no double-materialization | Iceberg's upsert semantics handle duplicate data correctly; no background jobs needed |
| **Network partition during commit** | Iceberg commit is atomic; either visible or not; source ack retried deterministically | Atomic operations eliminate partial visibility; deterministic retry ensures eventual consistency |

**The guarantee**: Your table state is always correct. No background deduplication jobs. No "eventual consistency" with fingers crossed. The system provides strong consistency guarantees that are immediately verifiable.

This approach eliminates the complexity and operational overhead of traditional CDC systems that rely on eventual consistency and background cleanup jobs.

---

## Schema Evolution

Schema evolution is one of the most complex aspects of data pipelines. As source schemas change over time, the destination must adapt without losing data or breaking existing queries. Our refactored system handles schema evolution through a deliberate, thread-safe process that ensures consistency across all writer threads.

The key challenge in schema evolution is maintaining consistency across multiple concurrent writer threads while ensuring that schema changes are applied atomically. Here's how our system addresses this:

<img
  src="/img/blog/2025/10/how-olake-becomes-7x-faster-3.webp"
  alt="Schema Evolution Flow"
  style={{ width: '100%', height: 'auto' }}
/>

### Detection and Promotion

Schema evolution begins with detection at the thread level. Each writer thread analyzes its batch of records to determine if the schema has changed compared to the global schema for the stream. This parallel detection minimizes contention and allows for efficient schema evolution.

The evolution process works as follows:

1. **Thread-level detection**: Each thread compares its local schema with the global stream schema
2. **Lock acquisition**: Only threads detecting schema changes acquire the stream-level lock
3. **Schema evolution**: The first thread to acquire the lock performs the actual schema evolution
4. **Writer refresh**: All threads refresh their writers to use the new schema

Here's the implementation:

```go title="iceberg.go"
// destination/iceberg/iceberg.go
func (i *Iceberg) EvolveSchema(ctx context.Context, globalSchema, recordsRawSchema any) (any, error) {
	if !i.stream.NormalizationEnabled() {
		return i.schema, nil
	}

	globalSchemaMap, ok := globalSchema.(map[string]string)
	if !ok {
		return nil, fmt.Errorf("failed to convert globalSchema")
	}

	recordsSchema, ok := recordsRawSchema.(map[string]string)
	if !ok {
		return nil, fmt.Errorf("failed to convert newSchemaMap")
	}

	// Check if table schema is different from global schema
	if differentSchema(globalSchemaMap, recordsSchema) {
		logger.Infof("Thread[%s]: evolving schema in iceberg table", i.options.ThreadID)
		
		request := proto.IcebergPayload{
			Type: proto.IcebergPayload_EVOLVE_SCHEMA,
			Metadata: &proto.IcebergPayload_Metadata{
				IdentifierField: &identifierField,
				DestTableName:   i.stream.Name(),
				ThreadId:        i.server.serverID,
				Schema:          /* new schema fields */,
			},
		}

		response, err = i.server.sendClientRequest(ctx, &request)
		if err != nil {
			return false, fmt.Errorf("failed to evolve schema: %s", err)
		}
	} else {
		// Schema already exists in table; just refresh writer
		logger.Debugf("Thread[%s]: refreshing table schema", i.options.ThreadID)
		request.Type = proto.IcebergPayload_REFRESH_TABLE_SCHEMA
		response, err = i.server.sendClientRequest(ctx, &request)
		if err != nil {
			return false, fmt.Errorf("failed to refresh schema: %s", err)
		}
	}

	// Parse updated schema and update thread-local schema
	schemaAfterEvolution, err := parseSchema(response)
	if err != nil {
		return nil, fmt.Errorf("failed to parse schema: %s", err)
	}

	i.schema = copySchema(schemaAfterEvolution)
	return schemaAfterEvolution, nil
}
```

### Union-by-Name Evolution (Java)

The Java side handles the actual schema evolution using Iceberg's native `unionByNameWith` operation. This approach is safer than union-by-position because it handles schema changes more gracefully, especially when fields are added in different orders or when optional fields are introduced.

The key aspects of our schema evolution strategy:

- **Union-by-name**: Fields are matched by name rather than position, making the evolution more robust
- **Identifier field preservation**: Primary key fields are maintained to ensure upsert functionality continues to work
- **No-op detection**: We avoid creating unnecessary snapshots when the schema hasn't actually changed

Here's the implementation:

```java title="IcebergTableOperator.java"
// destination/iceberg/olake-iceberg-java-writer/src/main/java/io/debezium/server/iceberg/tableoperator/IcebergTableOperator.java
public void applyFieldAddition(Table icebergTable, Schema newSchema) {
  icebergTable.refresh(); // for safe case
  
  UpdateSchema us = icebergTable.updateSchema().unionByNameWith(newSchema);
  
  if (createIdentifierFields) {
    us.setIdentifierFields(newSchema.identifierFieldNames());
  }
  
  Schema newSchemaCombined = us.apply();
  
  // Avoid no-op commits (Iceberg creates a new snapshot even if schema unchanged)
  if (!icebergTable.schema().sameSchema(newSchemaCombined)) {
    LOGGER.warn("Extending schema of {}", icebergTable.name());
    us.commit();
  }
}
```

### Writer Refresh

After schema evolution, we explicitly complete the current writer to ensure the next write uses the updated schema. This is crucial because Iceberg writers are schema-aware and must be recreated when the schema changes. Without this step, writes would fail with schema mismatch errors.

The refresh process involves:

1. **Completing the current writer**: This flushes any pending data and closes the writer
2. **Collecting data files**: The completed writer's output files are collected for the next commit
3. **Nullifying the writer**: This forces the creation of a new writer with the updated schema on the next write

Here's how this works:

```java title="OlakeRowsIngester.java"
// destination/iceberg/olake-iceberg-java-writer/src/main/java/io/debezium/server/iceberg/rpc/OlakeRowsIngester.java
switch (request.getType()) {
case EVOLVE_SCHEMA:
    SchemaConvertor convertor = new SchemaConvertor(identifierField, schemaMetadata);
    icebergTableOperator.applyFieldAddition(this.icebergTable, convertor.convertToIcebergSchema());
    this.icebergTable.refresh();
    // Complete current writer so next write uses new schema
    icebergTableOperator.completeWriter();
    sendResponse(responseObserver, this.icebergTable.schema().toString());
    LOGGER.info("{} Successfully applied schema evolution for table: {}", requestId, destTableName);
    break;

case REFRESH_TABLE_SCHEMA:
    this.icebergTable.refresh();
    // Complete current writer
    icebergTableOperator.completeWriter();
    sendResponse(responseObserver, this.icebergTable.schema().toString());
    break;
}
```

```java title="IcebergTableOperator.java"
// destination/iceberg/olake-iceberg-java-writer/src/main/java/io/debezium/server/iceberg/tableoperator/IcebergTableOperator.java
public void completeWriter() {
  try {
    if (writer == null) {
      LOGGER.warn("no writer to complete");
      return;
    }
    
    // Complete writer and collect data/delete files
    WriteResult writerResult = writer.complete();
    deleteFiles.addAll(Arrays.asList(writerResult.deleteFiles()));
    dataFiles.addAll(Arrays.asList(writerResult.dataFiles()));
    
  } catch (IOException e) {
    LOGGER.error("Failed to complete writer", e);
    throw new RuntimeException("Failed to complete writer", e);
  } finally {
    // Close and null the writer
    try {
      if (writer != null) {
        writer.close();
      }
    } catch (IOException e) {
      LOGGER.warn("Failed to close writer", e);
    }
    // Set to null to trigger re-initialization with new schema
    writer = null;
  }
}
```

**Why this matters**: Without explicit refresh, the writer instance would continue using the old schema, leading to schema mismatches and write failures. This explicit refresh ensures that all subsequent writes use the correct, updated schema, maintaining data consistency and preventing runtime errors.

The schema evolution process is designed to be:
- **Thread-safe**: Multiple threads can detect schema changes simultaneously without conflicts
- **Atomic**: Schema changes are applied atomically to the Iceberg table
- **Efficient**: Only threads that detect actual schema changes participate in the evolution process
- **Consistent**: All threads eventually use the same updated schema

---

## Go Data Plane: Thread-Scoped Schema and Batching

The Go side of our architecture is responsible for the high-level data plane operations: concurrency management, intelligent batching, and schema coordination. This design leverages Go's strengths in concurrent programming while keeping the complex Iceberg I/O operations in Java where the native libraries are most mature.

The key responsibilities of the Go data plane include:

- **Concurrent processing**: Managing multiple writer threads that can process different partitions or streams simultaneously
- **Intelligent batching**: Collecting records into optimal batch sizes for efficient processing
- **Schema coordination**: Detecting schema changes and coordinating evolution across threads
- **Lifecycle management**: Properly initializing and cleaning up writer resources

Let's examine how these components work together:

### Writer Pool and Thread Setup

The writer pool manages the lifecycle of writer threads, ensuring proper resource allocation and schema sharing across threads for the same stream. Each thread gets its own isolated processing context while sharing schema information to maintain consistency.

Key aspects of the thread setup:

- **Schema sharing**: All threads for a stream share the same schema artifact to ensure consistency
- **Thread isolation**: Each thread has its own buffer and processing context
- **Resource management**: Proper initialization and cleanup of writer resources
- **Configuration**: Thread-specific options like batch sizes and timeouts

Here's how new writer threads are created:

```go title="writers.go"
// destination/writers.go
func (w *WriterPool) NewWriter(ctx context.Context, stream types.StreamInterface, options ...ThreadOptions) (*WriterThread, error) {
	w.stats.ThreadCount.Add(1)

	opts := &Options{}
	for _, one := range options {
		one(opts)
	}

	// Get per-stream schema artifact (shared across threads for this stream)
	rawStreamArtifact, ok := w.writerSchema.Load(stream.ID())
	if !ok {
		return nil, fmt.Errorf("failed to get stream artifacts for stream[%s]", stream.ID())
	}

	streamArtifact, ok := rawStreamArtifact.(*writerSchema)
	if !ok {
		return nil, fmt.Errorf("failed to convert raw stream artifact")
	}

	var writerThread Writer
	err := func() error {
		// Initialize writer with configurations
		writerThread = w.init()
		w.configMutex.Lock()
		err := utils.Unmarshal(w.config, writerThread.GetConfigRef())
		w.configMutex.Unlock()
		if err != nil {
			return err
		}

		// Setup table and schema
		streamArtifact.mu.Lock()
		defer streamArtifact.mu.Unlock()

		output, err := writerThread.Setup(ctx, stream, streamArtifact.schema, opts)
		if err != nil {
			return fmt.Errorf("failed to setup the writer thread: %s", err)
		}

		// First thread for this stream sets the global schema
		if streamArtifact.schema == nil {
			streamArtifact.schema = output
		}

		return nil
	}()
	if err != nil {
		return nil, fmt.Errorf("failed to setup writer thread: %s", err)
	}

	return &WriterThread{
		buffer:         []types.RawRecord{},
		batchSize:      10000, // Default batch size
		threadID:       opts.ThreadID,
		writer:         writerThread,
		stats:          w.stats,
		streamArtifact: streamArtifact,
	}, nil
}
```

### The Flush Path: Evolve-If-Needed, Then Write

The flush operation is the heart of the data processing pipeline. It handles the critical sequence of schema evolution detection, data flattening, and writing to Iceberg. The design ensures that schema changes are detected and handled before any data is written, preventing schema mismatch errors.

The flush process follows this sequence:

1. **Data flattening and schema detection**: Records are flattened and analyzed for schema changes
2. **Schema evolution**: If changes are detected, the schema is evolved atomically
3. **Data writing**: Records are written using the correct schema
4. **Error handling**: Any failures are properly handled and reported

This approach ensures that schema evolution and data writing are properly coordinated:

```go title="writers.go"
// destination/writers.go
func (wt *WriterThread) flush(ctx context.Context, buf []types.RawRecord) (err error) {
	// Skip empty buffers
	if len(buf) == 0 {
		return nil
	}

	// Create flush context
	flushCtx, cancel := context.WithCancel(ctx)
	defer cancel()

	// Flatten data and detect schema changes
	evolution, buf, threadSchema, err := wt.writer.FlattenAndCleanData(buf)
	if err != nil {
		return fmt.Errorf("failed to flatten and clean data: %s", err)
	}

	// If schema evolution detected, evolve schema
	if evolution {
		wt.streamArtifact.mu.Lock()
		newSchema, err := wt.writer.EvolveSchema(flushCtx, wt.streamArtifact.schema, threadSchema)
		if err == nil && newSchema != nil {
			wt.streamArtifact.schema = newSchema
		}
		wt.streamArtifact.mu.Unlock()
		if err != nil {
			return fmt.Errorf("failed to evolve schema: %s", err)
		}
	}

	// Write records
	if err := wt.writer.Write(flushCtx, buf); err != nil {
		return fmt.Errorf("failed to write records: %s", err)
	}

	logger.Infof("Thread[%s]: successfully wrote %d records", wt.threadID, len(buf))
	return nil
}
```

### Parallel Normalization and Schema Detection

Normalization and schema evolution checks run in parallel at the thread level. Each thread builds a local candidate schema from its batch, compares it against the stream's global schema, and only acquires the stream-level schema lock if a difference is detected. This approach minimizes contention and allows for efficient parallel processing.

**How parallel normalization works:**

- **Thread-local schema detection**: Each writer thread analyzes its batch of records to build a candidate schema
- **Parallel comparison**: Threads compare their local schema against the global stream schema without blocking
- **Contention minimization**: Only threads that detect actual schema changes acquire the stream-level lock
- **Efficient processing**: Multiple threads can process different batches simultaneously without schema conflicts

This parallel approach is crucial for maintaining high throughput while ensuring schema consistency across all writer threads.

### Concurrency and Locking

The Go data plane implements a sophisticated concurrency model that minimizes contention while ensuring correctness:

- **Thread-local schema first**: Threads compute and cache their candidate schema locally without any locking
- **Global schema lock only on change**: A stream-scoped lock is taken to evolve schema in the table only when a thread detects divergence from the global schema
- **Commit lock**: Per-thread commits are serialized with `table.refresh()` to ensure atomic visibility without cross-thread partial state

This design keeps hot paths lock-free and confines blocking to rare schema transitions and final commits, enabling high concurrency while maintaining data consistency.

### Explicit Commit on Thread Close

When a writer thread is closed, it must ensure that all pending data is properly committed to Iceberg before shutting down. This explicit commit ensures that no data is lost during thread termination and that the exactly-once semantics are maintained.

The close process involves:

1. **Final commit**: All pending data files are committed to the Iceberg table
2. **Resource cleanup**: Connections and resources are properly released
3. **Error handling**: Any failures during shutdown are properly logged and handled

This explicit commit is crucial for maintaining data consistency:

```go title="iceberg.go"
// destination/iceberg/iceberg.go
func (i *Iceberg) Close(ctx context.Context) error {
	// Skip flushing on error
	defer func() {
		if i.server == nil {
			return
		}
		err := i.server.closeIcebergClient()
		if err != nil {
			logger.Errorf("Thread[%s]: error closing Iceberg client: %s", i.options.ThreadID, err)
		}
	}()

	if i.stream == nil {
		// For check connection no commit will happen
		return nil
	}

	// Send commit request for this thread
	ctx, cancel := context.WithTimeout(ctx, 300*time.Second)
	defer cancel()

	request := &proto.IcebergPayload{
		Type: proto.IcebergPayload_COMMIT,
		Metadata: &proto.IcebergPayload_Metadata{
			ThreadId:      i.server.serverID,
			DestTableName: i.stream.Name(),
		},
	}
	
	res, err := i.server.sendClientRequest(ctx, request)
	if err != nil {
		return fmt.Errorf("failed to send commit message: %s", err)
	}

	logger.Debugf("Thread[%s]: Sent commit message: %s", i.options.ThreadID, res)
	return nil
}
```

---

## gRPC Contract

The communication between Go and Java components uses a carefully designed gRPC contract that prioritizes efficiency and type safety. Rather than using generic JSON or dynamic message formats, we use strongly-typed protobuf messages that eliminate serialization overhead and provide compile-time type checking.

Key design decisions for the gRPC contract:

- **Single unary RPC**: One endpoint handles all operations (records, commits, schema evolution) to reduce connection overhead
- **Typed payloads**: Strongly-typed fields instead of generic maps reduce memory usage and serialization costs
- **Purpose-built messages**: Messages are designed specifically for Iceberg operations, not generic CDC formats
- **Efficient encoding**: Protobuf's binary encoding is more efficient than JSON for structured data

Here's the contract definition:

```protobuf title="record_ingest.proto"
// destination/iceberg/olake-iceberg-java-writer/src/main/resources/record_ingest.proto
syntax = "proto3";

package io.debezium.server.iceberg.rpc;

service RecordIngestService {
  rpc SendRecords(IcebergPayload) returns (RecordIngestResponse);
}

message IcebergPayload {
  enum PayloadType {
    RECORDS = 0;
    COMMIT = 1;
    EVOLVE_SCHEMA = 2;
    DROP_TABLE = 3;
    GET_OR_CREATE_TABLE = 4;
    REFRESH_TABLE_SCHEMA = 5;
  }
  
  PayloadType type = 1;

  message Metadata {
    string dest_table_name = 1;
    string thread_id = 2;
    optional string identifier_field = 3;
    repeated SchemaField schema = 4;
  }

  message SchemaField {
    string ice_type = 1;
    string key = 2;
  }

  // Typed fields for efficiency (not generic Value maps)
  message IceRecord {
    message FieldValue {
      oneof value {
        string string_value = 1;
        int32 int_value = 2;
        int64 long_value = 3;
        float float_value = 4;
        double double_value = 5;
        bool bool_value = 6;
        bytes bytes_value = 7;
      }
    }
    
    repeated FieldValue fields = 1;
    string record_type = 2;  // "u" (update), "c" (create), "r" (read), "d" (delete)
  }

  Metadata metadata = 2;
  repeated IceRecord records = 3;
}

message RecordIngestResponse {
  string result = 1;
  bool success = 2;
}
```

**Benefits of this design:**

- **Typed `oneof` fields**: Use significantly less memory than generic `google.protobuf.Value` maps, reducing both serialization overhead and memory allocations
- **Single RPC endpoint**: Simplifies client/server logic and reduces connection management overhead
- **Clear operation intent**: The `PayloadType` enum makes it explicit what operation is being performed
- **No Debezium envelope**: The payload is purpose-built for Iceberg writes, eliminating unnecessary parsing and transformation steps
- **Efficient field encoding**: Protobuf's binary encoding is more compact and faster to parse than JSON
- **Compile-time type safety**: Both Go and Java get compile-time validation of message structures

This contract design contributes significantly to the overall performance improvement by eliminating serialization bottlenecks and reducing memory pressure.

---

## Serialization Path: Before vs After

One of the most significant optimizations in our refactor was eliminating the Debezium envelope from the serialization path. This change removes an entire encode/decode cycle and significantly reduces CPU usage, memory allocations, and overall processing overhead.

<img
  src="/img/blog/2025/10/how-olake-becomes-7x-faster-4.webp"
  alt="Serialization Path Comparison"
  style={{ width: '100%', height: 'auto' }}
/>

**Before (Original Pipeline):**
- Source data → Debezium envelope → JSON serialization → Network transmission → JSON deserialization → Debezium parsing → Iceberg format conversion → File write

**After (Refactored Pipeline):**
- Source data → Go-native processing → Typed protobuf → Network transmission → Java-native Iceberg operations → File write

**Key improvements:**
- **Eliminated Debezium envelope**: Removes unnecessary metadata and formatting overhead
- **Direct protobuf serialization**: More efficient than JSON for structured data
- **Native format conversion**: Go and Java components work with their native data structures
- **Reduced memory allocations**: Fewer intermediate objects and copies
- **Faster processing**: Eliminates multiple encode/decode cycles

This optimization alone contributes significantly to the overall performance improvement by streamlining the data processing pipeline and reducing computational overhead.

---

## Partition Fanout Writer

Highly partitioned tables present a unique challenge for data ingestion systems. The traditional approach of processing partitions sequentially leads to frequent writer creation and destruction, resulting in many small files and poor I/O efficiency. Our new partition fanout writer addresses these issues by maintaining multiple partition writers concurrently.

<img
  src="/img/blog/2025/10/how-olake-becomes-7x-faster-5.webp"
  alt="Partition Fanout Writer"
  style={{ width: '100%', height: 'auto' }}
/>

**Problems with the previous approach:**
- **Sequential partition processing**: Only one partition writer could be active at a time
- **Frequent writer rolls**: Small batches led to many small files
- **Poor I/O efficiency**: Constant writer creation/destruction overhead
- **Inconsistent file sizes**: Unpredictable file sizes hurt query performance

**How the new fanout writer works:**
- **Concurrent partition writers**: Multiple partition writers can be active simultaneously when memory allows
- **Intelligent buffering**: Each partition buffers data to target larger, more consistent file sizes
- **Reduced writer churn**: Writers are reused across batches, reducing creation/destruction overhead
- **Better resource utilization**: More efficient use of I/O resources and memory

**Benefits:**
- **Improved throughput**: Especially noticeable on highly partitioned tables with many small partitions
- **Consistent file sizes**: Better query performance and storage efficiency
- **Reduced overhead**: Less time spent on writer lifecycle management
- **Better scalability**: Performance scales better with the number of partitions

This optimization is particularly important for tables with many partitions, where the traditional approach would create hundreds or thousands of small files, leading to poor query performance and storage inefficiency.

---

## Performance Optimizations

The 7× performance improvement is the result of multiple architectural optimizations that compound together. Each optimization addresses a specific bottleneck in the original pipeline, and their combined effect creates a dramatic improvement in throughput and efficiency.

Here's a detailed breakdown of each optimization:

| Optimization | Technical Details |
|--------------|-------------------|
| **Bigger batches** | Default 10k records per batch; fewer RPC calls reduce network overhead; larger Parquet chunks improve compression and I/O efficiency; better amortization of serialization costs |
| **Typed protobufs** | Eliminates dynamic map allocations and generic `google.protobuf.Value` overhead; compile-time type checking reduces runtime validation; binary encoding is more efficient than JSON |
| **Thread-scoped schema** | Removes global schema locks from the hot path; writer refresh only occurs when schema actually changes; parallel schema detection reduces contention |
| **Java-native Iceberg I/O** | Direct use of Iceberg's optimized `RowDelta`/`AppendFiles` APIs; leverages vectorized Parquet writes; eliminates intermediate format conversions |
| **Clean commit/ack boundaries** | Reduces retry overhead and rollback churn; predictable behavior eliminates expensive error recovery; atomic operations prevent partial state issues |

**Combined effect**: These optimizations compound together to achieve the **7× performance improvement**.

### Why These Optimizations Compound

The performance improvements compound because they address different bottlenecks in the pipeline:

1. **Batch size optimization** reduces the frequency of expensive operations (RPC calls, file writes)
2. **Typed serialization** makes each operation faster and uses less memory
3. **Thread-scoped schema** eliminates contention that would otherwise limit concurrency
4. **Native Iceberg I/O** leverages the most efficient data structures and algorithms
5. **Clean boundaries** eliminate overhead from error handling and recovery

Each optimization enables the others to be more effective, creating a multiplicative rather than additive improvement.

### Performance Benchmarks

To validate our improvements, we conducted comprehensive benchmarks across different workloads and scenarios:

| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| **Throughput** | ~46K records/sec | ~320K records/sec | **7× faster** |
| **Memory usage** | 80GB+ | 40GB+ | **50% reduction** |
| **File size consistency** | 50MB - 2GB range | 300-400MB target | **Consistent sizing** |

**Test Environment:**
- **Hardware**: 64-core CPU, 128GB RAM, NVMe SSD storage
- **Workload**: NYC taxi data with insert operations only
- **Data volume**: 4 billion records
- **Partitions**: 50-100 partitions per table
- **Schema changes**: 2-3 schema evolutions per test run

**Key Insights:**
- **Consistent performance**: The new architecture maintains performance even under high concurrency (10+ writer threads)
- **Predictable resource usage**: Memory and CPU usage are now predictable and bounded
- **Faster recovery**: Schema evolution and error recovery are significantly faster
- **Better scalability**: Performance scales linearly with additional threads and partitions

---

## Benefits

### 1. Exactly-Once Visible State

No background deduplication. Tables reflect the correct state after each commit.

### 2. Performance

7× throughput improvement through optimized batching, typed serialization, and efficient I/O.

### 3. Operational Simplicity

- Clear commit/ack boundaries
- Deterministic behavior under failure
- Explicit schema evolution
- Per-thread isolation

### 4. Scalability

- Thread-scoped schema reduces contention
- Bigger batches improve I/O efficiency
- Java-native Iceberg operations leverage vectorized writes

### 5. Consistent File Sizing

We standardize on a default target file size of ~350 MB for steady compaction and reader performance. This consistent file sizing:

- **Improves query performance**: Larger, consistently-sized files enable better query planning and execution
- **Optimizes storage efficiency**: Reduces the number of small files that can hurt storage performance
- **Enables better compaction**: Consistent file sizes make compaction strategies more predictable and efficient
- **Reduces metadata overhead**: Fewer files mean less metadata to manage and process

---

## Lessons Learned

This refactor taught us several valuable lessons about building high-performance data systems:

### 1. Correctness Enables Performance
Starting with strong consistency guarantees (exactly-once semantics) actually made performance optimization easier. When you don't have to worry about eventual consistency or background cleanup jobs, you can focus purely on throughput and efficiency.

### 2. Language-Specific Strengths Matter
Using Go for concurrency and Java for I/O operations wasn't just about using the right tool for the job—it was about leveraging each language's ecosystem and optimizations. The mature Iceberg Java libraries and Go's concurrent programming model were both essential.

### 3. Eliminate Complexity Before Optimizing
Removing the Debezium envelope was one of our biggest wins. Sometimes the best optimization is eliminating unnecessary work entirely, rather than making existing work faster.

### 4. Measure Everything
Each optimization was measured and validated before moving to the next. This systematic approach prevented us from optimizing the wrong things and helped us understand which changes had the biggest impact.

### 5. Design for Operations
The explicit lifecycle management and clear commit boundaries weren't just about correctness—they made the system much easier to operate, debug, and monitor in production.

---

## Conclusion

The destination refactor represents a fundamental shift in how we approach data pipeline architecture. By carefully separating concerns between Go and Java components and eliminating unnecessary complexity, we've achieved both significant performance improvements and stronger correctness guarantees.

### Key Achievements

- **Exactly-once visible state**: Achieved through atomic Iceberg commits and careful acknowledgment ordering, eliminating the need for background deduplication jobs
- **7× performance improvement**: Result of multiple compounding optimizations including bigger batches, typed serialization, and native Iceberg I/O
- **Clear operational boundaries**: Deterministic behavior under failure conditions with well-defined commit and acknowledgment semantics
- **Robust schema evolution**: Thread-safe schema evolution with explicit writer refresh ensures consistency across concurrent operations
- **Improved scalability**: Better resource utilization and reduced contention enable the system to handle larger workloads

### Architectural Insights

The split between Go (data plane) and Java (Iceberg I/O) provides the right abstraction boundaries:

- **Go's strengths**: Concurrent programming, lightweight threading, and efficient memory management make it ideal for data plane operations
- **Java's strengths**: Mature Iceberg ecosystem, optimized I/O libraries, and vectorized operations provide the most efficient path for file operations
- **Clean interfaces**: The typed gRPC contract eliminates serialization overhead while maintaining type safety
- **Explicit lifecycle management**: Well-defined resource management prevents leaks and ensures consistency

### Broader Implications

This refactor demonstrates several important principles for building high-performance data systems:

1. **Eliminate unnecessary complexity**: Removing the Debezium envelope simplified the pipeline and improved performance
2. **Leverage native capabilities**: Using each language's strengths rather than forcing a one-size-fits-all approach
3. **Design for correctness first**: Strong consistency guarantees enable better performance optimizations
4. **Measure and optimize systematically**: Each optimization was measured and validated before moving to the next

The result is a system that is not only faster but also more reliable, maintainable, and operationally friendly.

---

## What's Next

This refactor is just the beginning. We're continuing to invest in several key areas:

### Immediate Priorities (Next 3-6 months)

- **Iceberg merge-on-read optimizations**: Implementing intelligent file pruning and predicate pushdown to reduce query latency
- **Real-time metrics and observability**: Building comprehensive monitoring dashboards for throughput, latency, and error rates
- **DLQ/SMT hooks**: Leveraging the typed contract and schema awareness to add dead-lettering and smart transforms without reintroducing JSON envelopes
- **Adaptive batching**: Dynamic batch size adjustment based on workload characteristics and system resources

### Medium-term Goals (6-12 months)

- **Multi-region replication**: Extending the atomic commit model to support cross-region data replication
- **Advanced compression**: Implementing columnar compression optimizations for better storage efficiency
- **Query acceleration**: Pre-computed aggregations and materialized views for common query patterns
- **Kubernetes-native deployment**: Operator-based deployment and management for cloud-native environments

### Long-term Vision (12+ months)

- **ML-powered optimization**: Using machine learning to predict optimal batch sizes and resource allocation
- **Unified streaming/batch interface**: Seamless integration between real-time and batch processing workloads
- **Cross-platform compatibility**: Extending support beyond Iceberg to other table formats (Delta Lake, Hudi)

We're committed to making Olake the fastest, most reliable CDC platform for modern data architectures. The foundation we've built with this refactor enables all of these future enhancements.

---

*Olake is an open-source CDC and data ingestion platform for Apache Iceberg. Built for correctness, designed for speed, optimized for operations.*
