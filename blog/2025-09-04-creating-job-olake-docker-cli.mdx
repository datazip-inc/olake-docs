---
title: "From Postgres to Iceberg: Creating OLake Jobs with Docker CLI and UI"
description: "A friendly, step-by-step walkthrough to configure replication from Postgres to Apache Iceberg (Glue catalog) using the OLake UI or the Docker CLI."
slug: creating-job-olake-docker-cli
authors: [akshay, vishal]
tags: [iceberg,replication]
image: /img/blog/cover/pipeline-on-olake.png
jsonLd: |
  {
    "@context": "https://schema.org",
    "@type": "TechArticle",
    "headline": "From Postgres to Iceberg: Creating OLake Jobs with Docker CLI and UI",
    "alternativeHeadline": "A friendly, step-by-step walkthrough to configure replication from Postgres to Apache Iceberg using the OLake UI or Docker CLI",
    "author": [
      {
        "@type": "Person",
        "name": "Akshay",
        "jobTitle": "OLake Team",
        "image": "https://olake.io/img/authors/akshay.jpg",
        "email": "hello@olake.io",
        "sameAs": "https://linkedin.com/in/akshay"
      },
      {
        "@type": "Person",
        "name": "Vishal",
        "jobTitle": "OLake Team",
        "image": "https://olake.io/img/authors/vishal.jpg",
        "email": "hello@olake.io",
        "sameAs": "https://linkedin.com/in/vishal"
      }
    ],
    "publisher": {
      "@type": "Organization",
      "name": "OLake",
      "logo": {
        "@type": "ImageObject",
        "url": "https://olake.io/img/logo/olake-blue.svg",
        "width": 200,
        "height": 60
      },
      "url": "https://olake.io",
      "sameAs": [
        "https://github.com/datazip-inc/olake",
        "https://join.slack.com/t/getolake/shared_invite/zt-2uyphqf69-KQxih9Gwd4GCQRD_XFcuyw"
      ]
    },
    "datePublished": "2025-09-04T00:00:00.000Z",
    "dateModified": "2025-09-04T00:00:00.000Z",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "https://olake.io/blog/creating-job-olake-docker-cli"
    },
    "description": "A friendly, step-by-step walkthrough to configure replication from Postgres to Apache Iceberg (Glue catalog) using the OLake UI or the Docker CLI.",
    "keywords": "olake, postgres, apache iceberg, replication, docker cli, ui, data replication, cdc, glue catalog, tutorial, guide, data engineering, etl, elt",
    "articleSection": "Tutorial",
    "inLanguage": "en",
    "isPartOf": {
      "@type": "Blog",
      "name": "OLake Blog",
      "url": "https://olake.io/blog"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "Data Replication",
        "description": "Process of copying and synchronizing data between different data sources and destinations"
      },
      {
        "@type": "Thing",
        "name": "PostgreSQL to Iceberg",
        "description": "Replication workflow from PostgreSQL database to Apache Iceberg data lakehouse format"
      },
      {
        "@type": "Thing",
        "name": "Docker CLI",
        "description": "Command-line interface for managing Docker containers and applications"
      },
      {
        "@type": "Thing",
        "name": "AWS Glue Catalog",
        "description": "Centralized metadata repository for data lakes and data warehouses"
      }
    ],
    "mentions": [
      {
        "@type": "SoftwareApplication",
        "name": "OLake",
        "applicationCategory": "Data Replication Tool"
      },
      {
        "@type": "SoftwareApplication",
        "name": "PostgreSQL",
        "applicationCategory": "Database"
      },
      {
        "@type": "SoftwareApplication",
        "name": "Apache Iceberg",
        "applicationCategory": "Data Lakehouse Format"
      },
      {
        "@type": "SoftwareApplication",
        "name": "Docker",
        "applicationCategory": "Container Platform"
      },
      {
        "@type": "SoftwareApplication",
        "name": "AWS Glue",
        "applicationCategory": "Data Catalog Service"
      }
    ],
    "mainEntity": {
      "@type": "FAQPage",
      "mainEntity": [
        {
          "@type": "Question",
          "name": "How do you create an OLake replication job?",
          "acceptedAnswer": {
            "@type": "Answer",
            "text": "You can create OLake replication jobs using either the guided UI wizard for visual setup or the Docker CLI for automation and DevOps control. Both methods allow you to configure replication from PostgreSQL to Apache Iceberg with CDC, normalization, filters, partitioning, and scheduling."
          }
        },
        {
          "@type": "Question",
          "name": "What are the benefits of using OLake for data replication?",
          "acceptedAnswer": {
            "@type": "Answer",
            "text": "OLake offers straightforward, fast, and adaptable replication to Apache Iceberg without forcing you into one way of working. It provides flexibility in setup methods, handles CDC efficiently, and integrates well with modern data lakehouse architectures."
          }
        },
        {
          "@type": "Question",
          "name": "What features does OLake support for replication?",
          "acceptedAnswer": {
            "@type": "Answer",
            "text": "OLake supports Change Data Capture (CDC), data normalization, filtering, partitioning, scheduling, and seamless integration with Apache Iceberg and AWS Glue catalog. It provides both UI and CLI interfaces for different workflow preferences."
          }
        },
        {
          "@type": "Question",
          "name": "How does OLake compare to other replication tools?",
          "acceptedAnswer": {
            "@type": "Answer",
            "text": "Unlike platforms like Fivetran, Airbyte, or Debezium, OLake focuses on making replication to Apache Iceberg straightforward and adaptable without infrastructure complexity or high costs. It offers both simplicity and flexibility for different team preferences."
          }
        }
      ]
    },
    "tutorial": {
      "@type": "HowTo",
      "name": "How to Create OLake Jobs for Postgres to Iceberg Replication",
      "step": [
        {
          "@type": "HowToStep",
          "name": "Choose Setup Method",
          "text": "Select between UI wizard for visual setup or Docker CLI for automation"
        },
        {
          "@type": "HowToStep",
          "name": "Configure Source",
          "text": "Set up PostgreSQL connection and select tables for replication"
        },
        {
          "@type": "HowToStep",
          "name": "Configure Destination",
          "text": "Configure Apache Iceberg destination with AWS Glue catalog settings"
        },
        {
          "@type": "HowToStep",
          "name": "Set Up CDC and Features",
          "text": "Enable CDC, configure normalization, filters, and partitioning"
        },
        {
          "@type": "HowToStep",
          "name": "Schedule and Deploy",
          "text": "Set up scheduling and deploy the replication job"
        }
      ]
    }
  }
---

# From Postgres to Iceberg: Creating OLake Jobs with Docker CLI and UI

Data replication has become one of the most essential building blocks in modern data engineering. Whether it's keeping your analytics warehouse in sync with operational databases or feeding real-time pipelines for machine learning, companies rely on tools to move data quickly and reliably.

Today, there's no shortage of options—platforms like Fivetran, Airbyte, Debezium, and even custom-built Flink or Spark pipelines are widely used to handle replication. But each of these comes with trade-offs: infrastructure complexity, cost, or lack of flexibility when you want to adapt replication to your specific needs.

That's where OLake comes in. Instead of forcing you into one way of working, OLake focuses on making replication into Apache Iceberg (and other destinations) straightforward, fast, and adaptable. You can choose between a guided UI experience for simplicity or a Docker CLI flow for automation and DevOps-style control.

In this blog, we'll walk through how to set up a replication job in OLake, step by step. We'll start with the UI wizard for those who prefer a visual setup, then move on to the CLI-based workflow for teams that like to keep things in code. By the end, you'll have a job that continuously replicates from Postgres → Apache Iceberg (Glue catalog) with CDC, normalization, filters, partitioning, and scheduling—all running seamlessly.

## Two Setup Styles (pick what fits you)

### Option A — UI "Job-first" (guided, all-in-one)
Perfect if you want a clear wizard and visual guardrails.

### Option B — CLI (Docker)
Great if you prefer terminal, versioned JSON, or automation.

Both produce the **same result**. Choose the path that matches your workflow today.

## Option A — OLake UI (Guided)

We'll take the "job-first" approach. It's straightforward and keeps you in one flow.

### 1) Create a Job

From the left nav, go to **Jobs → Create Job**.  
You'll land on a wizard that starts with the **source**.

![Job page](/img/docs/getting-started/create-your-first-job/job-create.png)

### 2) Configure the Source (Postgres)

Choose **Set up a new source** → select **Postgres** → keep OLake version at the latest stable.  
Name it clearly, fill the Postgres endpoint config, and hit **Test Connection**.

![Job source connector](/img/docs/getting-started/create-your-first-job/job-source-connector.png)

![Job source config](/img/docs/getting-started/create-your-first-job/job-source-config.png)

> 📝 **Planning for CDC?**  
> Make sure a **replication slot** exists in Postgres.  
> See: [Replication Slot Guide](/docs/connectors/postgres/setup/generic).

### 3) Configure the Destination (Iceberg + Glue)

Now we set where the data will land.  
Pick **Apache Iceberg** as the destination, and **AWS Glue** as the catalog.

![Job dest connector](/img/docs/getting-started/create-your-first-job/job-dest-connector.png)

![Job dest catalog](/img/docs/getting-started/create-your-first-job/job-dest-catalog.png)

Provide the connection details and **Test Connection**.

![Job dest config](/img/docs/getting-started/create-your-first-job/job-dest-config.png)

### 4) Configure Streams

This is where we dial in *what* to replicate and *how*.  
For this walkthrough, we'll:

- Include stream `fivehundred`
- **Sync mode:** **Full Refresh + CDC**
- **Normalization:** **On**
- **Filter:** `dropoff_datetime >= "2010-01-01 00:00:00"`
- **Partitioning:** by **year** extracted from `dropoff_datetime`
- **Schedule:** every day at **12:00 AM**

![Job streams page](/img/docs/getting-started/create-your-first-job/job-streams.png)

Select the checkbox for `fivehundred`, then click the stream name to open stream settings.  
Pick the sync mode and toggle **Normalization**.

![Select stream](/img/docs/getting-started/create-your-first-job/job-stream-select.png)

Let's make the destination query-friendly. Open **Partitioning** → choose `dropoff_datetime` → **year**.  
Want more? Read the [Partitioning Guide](/docs/writers/parquet/partitioning).

![Stream partitioning](/img/docs/getting-started/create-your-first-job/job-stream-partition.png)

Add the **Data Filter** so we only move rows from 2010 onward.

![Stream filter](/img/docs/getting-started/create-your-first-job/job-data-filter.png)

Click **Next** to continue.

### 5) Schedule the Job

Give the job a clear name, set **Every Day @ 12:00 AM**, and hit **Create Job**.

![Job schedule](/img/docs/getting-started/create-your-first-job/job-schedule.png)

You're set! 🎉

![Job created](/img/docs/getting-started/create-your-first-job/job-creation-success.png)

Want results right away? Start a run immediately with **Jobs → (⋮) → Sync Now**.

![Sync now](/img/docs/getting-started/create-your-first-job/job-sync-now.png)

You'll see status badges on the right (**Running / Failed / Completed**).  
For more details, open **Job Logs & History**.

- Running  
  ![Job running](/img/docs/getting-started/create-your-first-job/job-running.png)

- Completed  
  ![Job success](/img/docs/getting-started/create-your-first-job/job-success.png)

Finally, verify that data landed in S3/Iceberg as configured:

![S3 data](/img/docs/getting-started/create-your-first-job/job-data-s3.png)

### 6) Manage Your Job (from the Jobs page)

**Sync Now** — Trigger a run without waiting.  

**Edit Streams** — Change which streams are included and tweak replication settings.  
Use the stepper to jump between **Source** and **Destination**.

![Edit streams](/img/docs/getting-started/create-your-first-job/job-edit-streams-page.png)

> By default, source/destination editing is locked. Click **Edit** to unlock.

![Edit destination](/img/docs/getting-started/create-your-first-job/job-edit-destination.png)

> 🔄 **Need to change Partitioning / Filter / Normalization for an existing stream?**  
> Unselect the stream → **Save** → reopen **Edit Streams** → re-add it with new settings.

**Pause Job** — Temporarily stop runs. You'll find paused jobs under **Inactive Jobs**, where you can **Resume** any time.

![Pause/Resume](/img/docs/getting-started/create-your-first-job/job-resume.png)

**Job Logs & History** — See all runs. Use **View Logs** for per-run details.

![Job logs list](/img/docs/getting-started/create-your-first-job/view-logs.png)

![Logs page](/img/docs/getting-started/create-your-first-job/logs-page.png)

**Job Settings** — Rename, change frequency, pause, or delete.  
Deleting a job moves its source/destination to **inactive** (if not used elsewhere).

![Job settings](/img/docs/getting-started/create-your-first-job/job-settings.png)

## Option B — OLake CLI (Docker)

Prefer terminals, PR reviews, and repeatable runs? Let's do the same pipeline via Docker.

### Prerequisites

- **Docker** installed and running  
- OLake images: **Docker Hub → `olakego/*`**

### How the CLI flow works

1. **Configure source & destination** (JSON files)  
2. **Discover streams** → writes a `streams.json`  
3. **Edit stream configuration** (normalization, filters, partitions, sync mode)  
4. **Run the sync**  
5. **Monitor with `stats.json`**

### What we'll build

- Source: **Postgres**  
- Destination: **Apache Iceberg** (Glue catalog)  
- Table: `fivehundred`  
- **CDC** mode + **Normalization**  
- Filter: `dropoff_datetime >= "2010-01-01 00:00:00"`  
- Partition by **year** from `dropoff_datetime`

### 1) Create Config Files

We'll put everything under `/path/to/config/`.

**Source — `source.json`**

```json title="source.json"
{
  "host": "dz-stag.postgres.database.azure.com",
  "port": 5432,
  "database": "postgres",
  "username": "postgres",
  "password": "XXX",
  "jdbc_url_params": {},
  "ssl": { "mode": "require" },
  "update_method": {
    "replication_slot": "replication_slot",
    "intital_wait_time": 120
  },
  "default_mode": "cdc",
  "max_threads": 6
}
```

> 📝 If you plan to run CDC, ensure a Postgres **replication slot** exists.
> See: [Replication Slot Guide](/docs/connectors/postgres/setup/generic).

**Destination — `destination.json`**

```json title="destination.json"
{
  "type": "ICEBERG",
  "writer": {
    "iceberg_s3_path": "s3://vz-testing-olake/olake_cli_demo",
    "aws_region": "XXX",
    "aws_access_key": "XXX",
    "aws_secret_key": "XXX",
    "iceberg_db": "olake_cli_demo",
    "grpc_port": 50051,
    "sink_rpc_server_host": "localhost"
  }
}
```

### 2) Discover Streams

This pulls available tables and writes `streams.json`.

```bash
docker run --pull=always \
  -v "/path/to/config:/mnt/config" \
  olakego/source-postgres:latest \
  discover \
  --config /mnt/config/source.json
```

*Start logs*
![Discover start](/img/docs/getting-started/create-your-first-job/cli-discover-logs-start.jpeg)

*Completion*
![Discover end](/img/docs/getting-started/create-your-first-job/cli-discover-logs-end.jpeg)

> ℹ️ Logs are also written to:
> `/path/to/config/logs/sync_[YYYY-MM-DD]_[HH-MM-SS]/olake.log`

### 3) Edit `streams.json`

Select exactly what to move and how.

* **Select streams** → keep only `fivehundred` under `"selected_streams"`.
* **Normalization** → `"normalization": true`
* **Filter** → `"filter": "dropoff_datetime >= \"2010-01-01 00:00:00\""`
* **Partitioning** → `"partition_regex": "/{dropoff_datetime, year}"`
* **Sync mode** → set the stream's `"sync_mode"` to `"cdc"`

**Minimal selection block**

```json title="streams.json (selection)"
{
  "selected_streams": {
    "public": [
      {
        "partition_regex": "/{dropoff_datetime, year}",
        "stream_name": "fivehundred",
        "normalization": true,
        "filter": "dropoff_datetime >= \"2010-01-01 00:00:00\""
      }
    ]
  }
}
```

**Full stream entry (showing supported modes)**

```json title="streams.json (stream detail)"
{
  "streams": [
    {
      "stream": {
        "name": "fivehundred",
        "namespace": "public",
        "type_schema": {
          "properties": {
            "dropoff_datetime": { "type": ["timestamp", "null"] }
          }
        },
        "supported_sync_modes": [
          "strict_cdc",
          "full_refresh",
          "incremental",
          "cdc"
        ],
        "source_defined_primary_key": [],
        "available_cursor_fields": ["id", "pickup_datetime", "rate_code_id"],
        "sync_mode": "cdc"
      }
    }
  ]
}
```

> 📚 Need a refresher on how modes differ? Check out our documentation on sync modes.

### 4) Run the Sync

Kick off replication:

```bash
docker run --pull=always \
  -v "/path/to/config:/mnt/config" \
  olakego/source-postgres:latest \
  sync \
  --config /mnt/config/streams.json \
  --catalog /mnt/config/catalog.json \
  --destination /mnt/config/destination.json
```

*Sync start*
![Sync start](/img/docs/getting-started/create-your-first-job/cli-sync-logs-start.jpeg)

*Sync completed*
![Sync completed](/img/docs/getting-started/create-your-first-job/cli-sync-logs-end.jpeg)

### 5) Monitor Progress with `stats.json`

A `stats.json` appears next to your configs:

```json title="stats.json"
{
  "Estimated Remaining Time": "0.00 s",
  "Memory": "367 mb",
  "Running Threads": 0,
  "Seconds Elapsed": "34.01",
  "Speed": "14.70 rps",
  "Synced Records": 500
}
```

Confirm the data in your destination (S3 / Iceberg):

![Data in Iceberg](/img/docs/getting-started/create-your-first-job/cli-s3-data.png)

### 6) About the `state.json` (Resumable & CDC-friendly)

When a sync starts, OLake writes a `state.json` that tracks progress and CDC offsets (e.g., Postgres LSN).
This lets you **resume without duplicates** and continue CDC seamlessly.

To resume / keep streaming:

```bash
docker run --pull=always \
  -v "/path/to/config:/mnt/config" \
  olakego/source-postgres:latest \
  sync \
  --config /mnt/config/streams.json \
  --catalog /mnt/config/catalog.json \
  --destination /mnt/config/destination.json \
  --state /mnt/config/state.json
```

More details: Check out our Postgres connector documentation for state file configuration.


---

## Quick Q&A

**UI or CLI—how should I choose?**
If you're new to OLake or prefer a guided setup, start with **UI**.
If you're automating, versioning configs, or scripting in CI, use **CLI**.

**Why "Full Refresh + CDC"?**
You get a baseline snapshot *and* continuous changes—ideal for keeping downstream analytics fresh.

**Can I change partitioning later?**

* **UI**: unselect the stream → save → re-add with updated partitioning/filter/normalization.
* **CLI**: edit `streams.json` and re-run.

---