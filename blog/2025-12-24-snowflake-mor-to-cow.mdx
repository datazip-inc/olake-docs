---
slug: olake-mor-cow-snowflake
title: "Bridging the Gap: Making OLake's MOR Iceberg Tables Compatible with Snowflake's Query Engine"
description: Learn how to make OLake's Merge-on-Read (MOR) Iceberg tables compatible with Snowflake using an automated compaction script that transforms MOR tables into Copy-on-Write (COW) format for accurate analytics queries.
tags: [iceberg, olake, snowflake]
authors: [nayan]
image: /img/blog/2025/12/snowflake_cow.webp
---
 
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

If you're using OLake to replicate database changes to Apache Iceberg and Snowflake for analytics, you've probably hit a frustrating roadblock: Snowflake doesn't support equality delete files. OLake writes data efficiently using Merge-on-Read (MOR) with equality deletes for CDC operations, but when you try to query those tables in Snowflake, the deletions, updates and inserts simply aren't honored. Your query results become incorrect, missing critical data changes.

This isn't just a Snowflake limitation—several major query engines including Databricks (for external tables) face the same challenge. While these platforms are incredibly powerful for analytics, their Iceberg implementations only support Copy-on-Write (COW) tables or position deletes at best.

In this blog, I'll walk you through the problem and show you how we've solved it with a simple yet powerful compaction script that transforms OLake's MOR tables into COW-compatible tables that Snowflake and other query engines can read correctly.

## The Problem: MOR vs COW in the Real World

Let's understand what's happening under the hood. When you use OLake for Change Data Capture (CDC), it writes data to Iceberg using a strategy called Merge-on-Read (MOR) with equality delete files. This approach is optimized for high-throughput writes:

### How OLake Writes Data (MOR with Equality Deletes):

**1. Initial Full Refresh:** OLake performs a complete historical load of your table to Iceberg.

**2. CDC Updates:** As changes happen in your source database, OLake captures them:
- New records → Written to new data files
- Updated records → Old version marked in an equality delete file, new version written to data file
- Deleted records → Marked in an equality delete file

**3. The Result:** After multiple CDC sync cycles, you have:
- Multiple data files with your records
- Multiple equality delete files tracking which records should be ignored

This is incredibly efficient for writes—OLake can stream thousands of changes per second without rewriting entire data files. However, here's the catch:

According to Snowflake's documentation, externally-managed Iceberg tables only support position deletes, not equality deletes. Also Snowflake does support querying Copy-on-Write (COW) tables, but any CDC pipeline using equality deletes (the standard approach for streaming CDC) will make the table unqueryable in Snowflake—no results will appear when querying such a table. This is why converting MOR tables with equality deletes to COW format is essential for making the data queryable in Snowflake.

## The Solution: Automated MOR to COW Compaction

The solution is to periodically compact your MOR tables into COW format—essentially creating a clean copy where all deletes and updates are fully applied by rewriting the data files. Think of it as "resolving" all the pending changes into a single, clean table state.

We've built a PySpark script that automates this entire process. Here's how it works:

### Workflow  Overview

![MOR to COW compaction workflow](/img/blog/2025/12/snowflake_mor_cow.webp)

The workflow begins with data from multiple source databases (PostgreSQL, MySQL, Oracle, MongoDB, and Kafka) being ingested through OLake. OLake writes this data to Iceberg tables using the Merge-on-Read (MOR) strategy with equality delete files, which is optimized for high-throughput CDC operations. The Spark Compaction process then transforms these MOR tables into Copy-on-Write (COW) format by rewriting the data files after applying the equality delete files into the data files, effectively resolving all equality deletes and applying updates. The resulting COW tables are stored in object storage (such as S3, Azure Blob Storage, or GCS) and can be queried directly by Snowflake as external Iceberg tables, ensuring accurate results with all deletes and updates properly applied.

### Prerequisites

Before running the compaction script, ensure you have the following installed:

- **Java 21**: Required for Spark runtime
- **Python 3.13.7**: Required for PySpark
- **Spark 3.5.2**: Apache Spark with Iceberg support

Additionally, make sure you have:
- Access to the object storage (S3, Azure Blob Storage, GCS, etc.) where your Iceberg tables are stored
- Appropriate cloud provider credentials or IAM roles configured

### Compaction Script

<details>
<summary>View the PySpark compaction script</summary>

```python
from pyspark.sql import SparkSession
from pyspark.sql.utils import AnalysisException

# ------------------------------------------------------------------------------
# Spark Session
# ------------------------------------------------------------------------------
spark = (
    SparkSession.builder
    .appName("OLake MOR to COW Compaction")
    # Avoid booting a local Hive metastore (Derby). We use Iceberg REST catalog via spark-submit.
    .config("spark.sql.catalogImplementation", "in-memory")
    .getOrCreate()
)

# ------------------------------------------------------------------------------
# User Inputs (must be provided)
# ------------------------------------------------------------------------------
CATALOG = "<YOUR_CATALOG_NAME>"
DB = "<YOUR_DATABASE_NAME>"

MOR_TABLE = f"{CATALOG}.{DB}.<YOUR_MOR_TABLE_NAME>"
COW_TABLE = f"{MOR_TABLE}_cow"
STATE_TABLE = f"{COW_TABLE}_state"

COW_LOCATION = "s3://<YOUR_BUCKET_NAME>/<YOUR_PATH>/<YOUR_COW_TABLE_NAME>"
STATE_LOCATION = f"{COW_LOCATION}_state"

PRIMARY_KEY = "_olake_id"

# ------------------------------------------------------------------------------
# Helpers
# ------------------------------------------------------------------------------
def table_exists(table_name: str) -> bool:
    try:
        spark.read.format("iceberg").load(table_name).limit(1).collect()
        return True
    except AnalysisException:
        return False


def get_latest_parent_snapshot_id(table):
    df = spark.sql(f"""
        SELECT parent_id
        FROM {table}.snapshots
        ORDER BY committed_at DESC
        LIMIT 1
    """)
    return df.collect()[0]["parent_id"]


def align_cow_schema(mor_df, cow_df):
    mor_schema = {f.name: f.dataType for f in mor_df.schema.fields}
    cow_schema = {f.name: f.dataType for f in cow_df.schema.fields}

    # Add missing columns from MOR to COW
    for col, dtype in mor_schema.items():
        if col not in cow_schema:
            print(f"Adding new column '{col}' with type '{dtype.simpleString()}' to COW table")
            spark.sql(f"""
                ALTER TABLE {COW_TABLE}
                ADD COLUMN {col} {dtype.simpleString()}
            """)

    # Update column types if they differ
    for col, mor_type in mor_schema.items():
        if col in cow_schema:
            cow_type = cow_schema[col]
            if mor_type != cow_type:
                print(f"Updating column '{col}' type from '{cow_type.simpleString()}' to '{mor_type.simpleString()}' in COW table")
                spark.sql(f"""
                    ALTER TABLE {COW_TABLE}
                    ALTER COLUMN {col} TYPE {mor_type.simpleString()}
                """)


def save_state(snapshot_id):
    """Save the current snapshot ID to the state table"""
    if not table_exists(STATE_TABLE):
        print(f"Creating state table: {STATE_TABLE}")
        spark.sql(f"""
            CREATE TABLE {STATE_TABLE} (
                current_state_id BIGINT
            )
            USING iceberg
            LOCATION '{STATE_LOCATION}'
        """)
    
    # Delete old state and insert new one
    spark.sql(f"DELETE FROM {STATE_TABLE}")
    spark.sql(f"INSERT INTO {STATE_TABLE} VALUES ({snapshot_id})")
    
    print(f"Saved snapshot ID {snapshot_id} to state table")


# ------------------------------------------------------------------------------
# Bootstrap Logic (FULL COPY)
# ------------------------------------------------------------------------------
def bootstrap_cow_table_if_needed():
    print("COW table does not exist. Creating and copying full data...")

    spark.sql(f"""
        CREATE TABLE {COW_TABLE}
        USING iceberg
        LOCATION '{COW_LOCATION}'
        AS
        SELECT *
        FROM {MOR_TABLE}
    """)

    print("Bootstrap completed: Full MOR → COW copy done.")

# ------------------------------------------------------------------------------
# Main Recurring Flow
# ------------------------------------------------------------------------------
def run_compaction_cycle():
    # Step 1: Truncate MOR
    spark.sql(f"TRUNCATE TABLE {MOR_TABLE}")
    
    # Step 2: Snapshot before truncate
    snapshot_id = get_latest_parent_snapshot_id(MOR_TABLE)
    print(f"Using MOR snapshot: {snapshot_id}")

    # Step 3: Save state
    save_state(snapshot_id)
    
    # Step 4: Load schemas from the parent snapshot
    mor_df = spark.read.format("iceberg") \
        .option("snapshot-id", snapshot_id) \
        .load(MOR_TABLE)
    
    cow_df = spark.read.format("iceberg").load(COW_TABLE)

    # Step 5: Schema alignment
    align_cow_schema(mor_df, cow_df)

    # Step 6: Merge snapshot into COW
    spark.sql(f"""
        MERGE INTO {COW_TABLE} AS target
        USING (
            SELECT *
            FROM {MOR_TABLE}
            VERSION AS OF {snapshot_id}
        ) AS source
        ON target.{PRIMARY_KEY} = source.{PRIMARY_KEY}

        WHEN MATCHED THEN
          UPDATE SET *

        WHEN NOT MATCHED THEN
          INSERT *
    """)


    print("Compaction cycle completed successfully.")

# ------------------------------------------------------------------------------
# Entry Point
# ------------------------------------------------------------------------------
if __name__ == "__main__":
    if table_exists(COW_TABLE):
        run_compaction_cycle()
    else:
        bootstrap_cow_table_if_needed()
```

</details>

**Before running the script, make sure to update the following variables in the "User Inputs" section:**

- **`CATALOG`**: Replace `<YOUR_CATALOG_NAME>` with your Iceberg catalog name
- **`DB`**: Replace `<YOUR_DATABASE_NAME>` with your database name
- **`MOR_TABLE`**: Replace `<YOUR_MOR_TABLE_NAME>` with your Merge-on-Read table name
- **`COW_LOCATION`**: Replace `<YOUR_BUCKET_NAME>`, `<YOUR_PATH>`, and `<YOUR_COW_TABLE_NAME>` with your object storage bucket, path, and COW table name 
- **`PRIMARY_KEY`**: Replace `<YOUR_PRIMARY_KEY_COLUMN_NAME>` with the actual primary key column name in your table (e.g., `_olake_id`, `id`, `employee_id`, etc.)

Note: The `COW_TABLE` and `STATE_TABLE` variables are automatically derived from `MOR_TABLE`, so you don't need to modify them.

### How the Compaction Script Works

The compaction process is designed to be safe, repeatable, and compatible with continuous CDC ingestion. Rather than interfering with OLake’s ongoing syncs, it works alongside them by leveraging Iceberg’s snapshot isolation guarantees.

The workflow starts by checking whether a Copy-on-Write (COW) version of the table already exists in the target object storage location. If this is the first time the script is run, the COW table does not exist yet. In that case, the script performs a one-time bootstrap: it creates the COW table and writes the entire resolved dataset from the MOR table into it. This establishes a clean baseline that Snowflake and other query engines can immediately query correctly.

On subsequent runs, the script switches to incremental mode. Instead of recreating the table, it updates the existing COW table with only the latest changes.

Once the Copy-on-Write (COW) table has been created, every subsequent run follows a predictable and safe update workflow:

**1. Prepare the MOR table for the next cycle:** The process begins by truncating the MOR table. This might seem scary at first—after all, you're deleting data—but it's completely safe. We're not actually losing anything because Iceberg saves all the states of table in sanpshots. The truncation simply clears out previously processed CDC data and ensures that only fresh changes are collected for the next compaction run. 

**2. Identify the last snapshot before truncation:** Here's where Iceberg's snapshot isolation becomes critical. When you truncate a table, Iceberg creates a brand new snapshot. But that new snapshot is empty—it contains no data. What we actually care about is the parent snapshot, which represents the exact moment in time when valid CDC data was still present in the MOR table, just before the truncate happened. The script queries Iceberg's metadata to find this parent snapshot ID, which becomes our pointer to the data we need to compact.

**3. Save this snapshot as the processing state:** The parent snapshot ID is saved to a dedicated state table. This simple act is what makes the entire process safe and repeatable. By saving this state, the system guarantees that:
- We always know exactly which snapshot we're processing, even if OLake writes new data later.
- If the compaction job fails midway, we can restart it from the same consistent point.
The state table is just a single-row table that stores the current snapshot ID. It's like leaving yourself a note saying "I processed everything up to this point."

**4. Read schema information from the saved snapshot:** Before we can merge data, we need to make sure our tables have the same structure. The script loads the schema from the saved snapshot to ensure that the compaction process uses the same table structure that existed when the data was written. This prevents issues where a schema change mid-compaction could cause failures or data corruption.

**5. Align the COW table schema if needed:** If the source schema has changed—for example, if a new column was added to the source database—the COW table is updated to match it. The script automatically detects missing columns or type changes and applies the necessary `ALTER TABLE` commands. This keeps both tables structurally consistent without manual intervention, which is crucial in a live CDC environment where schemas evolve over time.

**6. Merge the saved MOR snapshot into the COW table:** Finally, we use Iceberg's MERGE INTO statement to apply all the changes from the saved snapshot. This operation handles three scenarios in one pass:
- **New records** get inserted into the COW table.
- **Updated records** have their old versions replaced with new values
- **Deleted records** (marked in equality delete files) are included in the merge with `_op_type` as `d` indicating the record was deleted

Because we're merging from a specific snapshot, all these changes are applied at once, resulting in a clean COW table with no equality deletes remaining. Snowflake can now read this table perfectly.

## Testing the Compaction Script Locally

To understand how the compaction script works and see it in action, you can test it locally on your system before running it on production data. Follow these steps to run the script locally:

1. Use the following command to quickly spin up the source Postgres and destination (Iceberg/Parquet Writer) services using Docker Compose. This will download the required docker-compose files and start the containers in the background.
   ```bash
   sh -c 'curl -fsSL https://raw.githubusercontent.com/datazip-inc/olake-docs/master/docs/community/docker-compose.yml -o docker-compose.source.yml && \
   curl -fsSL https://raw.githubusercontent.com/datazip-inc/olake/master/destination/iceberg/local-test/docker-compose.yml -o docker-compose.destination.yml && \
   docker compose -f docker-compose.source.yml --profile postgres -f docker-compose.destination.yml up -d'
   ```

2. Clone the OLake repository and navigate to the project directory:
   ```bash
   git clone git@github.com:datazip-inc/olake.git && cd olake
   ```

3. Set up the configuration files:

    <Tabs>
    <TabItem value="source" label="source.json">

    ```json
    {
    "host": "localhost",
    "port": 5431,
    "database": "main",
    "username": "main",
    "password": "password",
    "jdbc_url_params": {},
    "ssl": {
        "mode": "disable"
    },
    "update_method": {
        "replication_slot": "postgres_slot",
        "publication": "olake_publication",
        "initial_wait_time": 120
    },
    "reader_batch_size": 1000,
    "max_threads": 10
    }
    ```

    </TabItem>
    <TabItem value="destination" label="destination.json">

    ```json
    {
    "type": "ICEBERG",
    "writer": {
        "catalog_type": "jdbc",
        "jdbc_url": "jdbc:postgresql://localhost:5432/iceberg",
        "jdbc_username": "iceberg",
        "jdbc_password": "password",
        "iceberg_s3_path": "s3a://warehouse",
        "s3_endpoint": "http://localhost:9000",
        "s3_use_ssl": false,
        "s3_path_style": true,
        "aws_access_key": "admin",
        "aws_secret_key": "password",
        "iceberg_db": "olake_iceberg",
        "aws_region": "us-east-1"
    }
    }
    ```

    </TabItem>
    </Tabs>

4. Sync the data to Iceberg:

   The below command generates the `streams.json` file which contains one sample table that we will use for this demonstration:

   ```bash
   ./build.sh driver-postgres discover --config $(pwd)/source.json
   ```

   Once the `streams.json` file is generated, you can sync the sample table to Iceberg:

   ```bash
   ./build.sh driver-postgres sync --config $(pwd)/source.json --catalog $(pwd)/streams.json --destination $(pwd)/destination.json
   ```

5. The data can be queried from Iceberg using the Spark Iceberg service available at [`localhost:8888`](http://localhost:8888).

   To view the table run the following SQL command:
   ```sql
   %%sql
   SELECT * FROM olake_iceberg.postgres_main_public.sample_data;
   ```

   We can modify the source database by adding and modifying few records and then running the sync again with state enabled to see the changes in the Iceberg table.

   Below command inserts two records into the source database:

   ```bash
   docker exec -it primary_postgres psql -U main -d main -c "INSERT INTO public.sample_data (id, num_col, str_col) VALUES (10, 100, 'First record'), (20, 200, 'Second record');"
   ```

   Let us also update a record in the source database:

   ```bash
   docker exec -it primary_postgres psql -U main -d main -c "UPDATE public.sample_data SET num_col = 150, str_col = 'First record updated' WHERE id = 1;"
   ```

   Now run the sync:
   
   ```bash
   ./build.sh driver-postgres sync --config $(pwd)/source.json --catalog $(pwd)/streams.json --destination $(pwd)/destination.json --state $(pwd)/state.json
   ```

   To view the updated table run the following SQL command:
   
   ```sql
   %%sql
   SELECT * FROM olake_iceberg.postgres_main_public.sample_data;
   ```

6. Run the compaction script to convert MOR tables to COW format:

   After completing the historical load and CDC sync, your Iceberg table now contains both data files and equality delete files in object storage, representing a Merge-on-Read (MOR) table. To convert this MOR table into a Copy-on-Write (COW) table, run the compaction script with the following configuration:

   Update the variables in the [compaction script](#compaction-script):

   ```python
   CATALOG = "olake_iceberg"
   DB = "postgres_main_public"
   COW_LOCATION = "s3a://warehouse/postgres_main_public_cow/sample_data_cow"
   ```

7. Verify the COW table creation:

   Once the compaction script runs successfully, we can verify the results in MinIO (the local object storage used in this demo). It can be noticed that:

   - The original MOR table with data files and equality delete files remains in `warehouse/postgres_main_public/sample_data`
   - A new COW table has been created in `warehouse/postgres_main_public_cow/sample_data_cow`, containing the resolved data with all equality deletes applied

   The COW table is now ready to be queried by Snowflake as an external Iceberg table, with all updates and deletes properly reflected in the data files.

## Running the Compaction Script

The compaction script is designed to run periodically, automatically keeping your COW table up-to-date with the latest changes from your MOR table. You can schedule this script as a cron job or through workflow orchestration tools, ensuring that your Snowflake queries always reflect the most recent data according to your requirements.

### Execution Platforms

The script can be run on any Spark cluster that has access to your Iceberg catalog and object storage (S3, Azure Blob Storage, GCS, etc.). Common execution platforms include:

- **AWS EMR**: Run the script as a Spark job on EMR clusters
- **Databricks**: Execute as a scheduled job in your Databricks workspace
- **Local Spark**: For testing or small-scale deployments

Simply submit the script using `spark-submit` with the appropriate Iceberg catalog configuration for your environment.

### Scheduling the Compaction Job

The job execution frequency can be set based on your data freshness requirements and business needs. The script is idempotent, so you can run it as frequently as needed without worrying about duplicate processing. Here are some common scheduling patterns:

- **Hourly**: For real-time dashboards and analytics that require near-live data
- **Every 6 hours**: A balanced approach for most use cases, providing good data freshness without excessive compute costs
- **Daily**: Perfect for overnight batch reporting and scenarios where daily updates are sufficient
- **On-demand**: For low-volume tables or manual refresh workflows where you trigger compaction only when needed

You can configure the schedule using cron syntax, Airflow DAG schedules, or your preferred orchestration tool. Each run will process any new changes since the last compaction, keeping your COW table synchronized with your MOR table.

For Snowflake users, once the COW table is created and being updated periodically, you simply create an external Iceberg table pointing to your COW table location, and you're ready to query with correct results—all deletes and updates properly applied.

## Conclusion

By implementing this automated compaction solution, you can now enjoy the best of both worlds: OLake's high-performance Merge-on-Read (MOR) writes for efficient CDC ingestion, combined with Snowflake-compatible Copy-on-Write (COW) tables for accurate analytics queries. 

