---
title: "How to Set Up MongoDB Apache Iceberg: Complete Guide to Building a Modern Data Lakehouse"
authors: [rohan]
slug: how-to-set-up-mongodb-apache-iceberg
tags: [mongodb,iceberg,olake,cdc]
image: /img/blog/cover/setup-mongodb.png
---

# How to Set Up MongoDB Apache Iceberg: Complete Guide to Building a Modern Data Lakehouse

**MongoDB** has become the go-to database for modern applications, handling everything from user profiles to IoT sensor data with its flexible document model. But when it comes to analytics at scale, MongoDB's document-oriented architecture faces significant challenges with complex queries, aggregations, and large-scale data processing.

That's where **Apache Iceberg** comes in. By replicating MongoDB data into Iceberg tables, you can unlock a modern, open-format data lakehouse that supports real-time analytics, schema evolution, partitioning, and time travel queries while maintaining MongoDB's operational performance.

Apache Iceberg is designed for large-scale, cost-effective analytics with native support for ACID transactions, seamless schema evolution, and compatibility with engines like Trino, Spark, and DuckDB. It's the perfect complement to MongoDB's operational strengths.

In this comprehensive guide, we'll walk through setting up a real-time pipeline from MongoDB to Apache Iceberg using OLake, covering both UI and CLI approaches. We'll explore why companies are successfully migrating to Iceberg architectures, achieving dramatic performance improvements and cost savings.

## The Growing Problem: Why MongoDB Analytics Hit Performance Walls

![MongoDB vs Iceberg Performance Comparison](/img/blog/2025/14/analytics-image.png)

### MongoDB's Document Design vs. Large-Scale Analytics Requirements

MongoDB was designed as a **NoSQL document database** optimized for flexible schemas and rapid development. While it excels at handling diverse data types and supporting agile application development, analytics presents completely different challenges.

**Document-Oriented Storage Limitations**: MongoDB stores data in BSON (Binary JSON) format optimized for flexible document structures. This works well for operational workloads, but once you approach terabyte-level MongoDB data volumes, complex analytical queries often slow to a crawl.

**Single-Node Architecture Constraints**: Unlike distributed analytics systems, MongoDB typically runs on a single node or replica set, meaning you can't simply add more machines and expect linear performance gains for analytical workloads.

### Real-World MongoDB Performance Bottlenecks

Research shows that MongoDB analytics queries experience dramatic performance degradation as data volumes grow:

- **Small datasets (< 10GB)**: Sub-second query responses
- **Medium datasets (100GB+)**: Tens of seconds to minutes
- **Large datasets (1TB+)**: Minutes to hours for complex analytics

**Common Performance Issues Include:**

- **Slow Aggregation Pipeline Execution**: Complex aggregation pipelines on large collections might take minutes or hours in MongoDB, whereas columnar, distributed systems could return similar results in seconds.
- **Memory Limitations**: MongoDB's in-memory processing model becomes a bottleneck when working with datasets larger than available RAM.
- **BI Dashboard Failures**: Tools like Metabase, Tableau, or Power BI connected directly to MongoDB often display endless loading spinners or timeout errors when processing large datasets.
- **Limited Join Capabilities**: MongoDB's $lookup operations don't scale well for complex analytical joins across large collections.

### The Hidden Costs of MongoDB Analytics at Scale

**Infrastructure Costs Spiral**: Running analytical workloads directly on MongoDB often requires expensive hardware upgrades and doesn't solve the fundamental architectural limitations.

**Operational Overhead**: Complex aggregation pipelines on large collections become memory and CPU intensive, significantly impacting operational performance and user experience.

## Why Replicate MongoDB to Apache Iceberg: The Modern Solution

### Transforming Analytics Performance and Cost Structure

MongoDB is excellent for powering production applications, but it wasn't designed for large-scale analytics. Running heavy queries on MongoDB often slows down applications, causes resource contention, and hits scaling limits as data grows.

Replicating MongoDB into Apache Iceberg solves these fundamental problems:

- **Offload Analytics Workloads**: Keep operational performance fast while running complex queries on Iceberg tables without impacting production systems.
- **Handle Petabyte Scale**: Iceberg supports petabytes of data on object storage like S3, with no sharding or archiving complexity.
- **Near Real-time Synchronization**: With MongoDB Change Streams, Iceberg tables stay continuously up to date with sub-second latency.
- **Advanced Lakehouse Features**: Partitioning, schema evolution, ACID transactions, and time travel make analytics flexible and reliable.
- **Lower Cost, Open Ecosystem**: Store data cheaply in S3, query with engines like Trino or Spark, and avoid vendor lock-in.



![MongoDB Change Streams Architecture](/img/blog/2025/14/change-streams.png)

### Key Challenges in MongoDB to Iceberg Replication

Moving data from MongoDB into Iceberg sounds simple, but in practice there are several technical hurdles that require careful planning:

**Change Data Capture Implementation Complexity**

- **MongoDB Change Streams Setup**: Getting MongoDB Change Streams configured correctly requires proper replica set setup, appropriate permissions, and understanding of oplog mechanics. The oplog must be properly configured for reliable CDC.
- **Real-time vs. Batch Processing**: Choosing between real-time Change Streams and batch replication affects both data freshness and infrastructure complexity. Real-time offers low latency but requires more sophisticated monitoring and error handling.

**Schema Evolution and Data Type Compatibility**

- **Dynamic Schema Changes**: MongoDB's flexible schema means documents can have varying structures. If your pipeline can't adapt to schema evolution, it breaks during routine application updates.
- **Data Type Mismatches**: MongoDB types like ObjectId, Date, or complex nested structures don't always translate cleanly into Iceberg-compatible schemas. Careful mapping strategies are essential for long-term reliability.

**Performance Optimization and Partitioning Strategy**

- **MongoDB Collection Structure vs. Analytics**: MongoDB collections aren't designed for analytics, so choosing the right Iceberg partitioning strategy makes or breaks query performance. Poor partitioning decisions can result in slow queries and high file scan costs.
- **Reliability and Monitoring**: Network hiccups, oplog rotations, or failed writes can quietly push MongoDB CDC pipelines out of sync without proper monitoring. Robust state management and recovery procedures are crucial for production deployments.

These challenges are why many DIY approaches get complicated quickly. Tools like OLake smooth over these technical edges while handling CDC configuration, schema evolution, partitioning optimization, and reliability monitoring automatically.

## Step-by-Step MongoDB to Iceberg Migration Workflow with OLake

![Step-by-Step Migration Process](/img/blog/2025/14/architecture.png)

### How MongoDB to Iceberg Replication Works

At a high level, the flow is straightforward: **MongoDB → OLake → Iceberg**. Here's what happens behind the scenes to enable real-time MongoDB analytics:

**Real-Time Change Data Capture Process**

1. **Listen to Changes**: OLake connects to MongoDB's Change Streams, which record every insert, update, and delete operation in real-time. This approach provides millisecond-latency change detection without impacting production performance.
2. **Capture and Transform**: Those changes are read continuously, normalized, and mapped into Iceberg-compatible data types while preserving data integrity and handling schema evolution automatically.
3. **Write to Iceberg**: OLake writes the data into Iceberg tables in your data lake (S3, HDFS, MinIO, etc.), respecting partition strategies and schema requirements for optimal query performance.
4. **Stay in Sync**: As new changes flow into MongoDB, they automatically propagate to Iceberg, keeping your lakehouse tables fresh and query-ready for real-time analytics.

**Automated Optimization and Reliability**

The best part? You don't need to worry about edge cases like schema evolution, or partitioning logic, OLake handles these automatically while ensuring your Iceberg tables remain efficient and consistent.

**Advanced Features Include:**
- Schema drift detection and handling for seamless evolution
- Partition optimization based on query patterns and data volume
- State management with recovery capabilities for production reliability

For deeper technical insights into what makes OLake fast and reliable for MongoDB-to-Iceberg pipelines, check out the performance optimization guide: [OLake Performance Guide](https://olake.io/blog/what-makes-olake-fast).

## Step-by-Step Guide: MongoDB to Iceberg Migration

### Prerequisites

Before we start, make sure you have:

**OLake UI deployed (or CLI setup).** - Check out [this Quickstart doc](https://olake.io/docs/getting-started/quickstart)

**MongoDB instance with:**
- Running in replica set mode (--replSet rs0).
- Enabled oplog (automatic in replica sets)
- Read access to the tables for the MongoDB user.
- Version 4.0 or higher

**Destination catalog for Iceberg, e.g.:**
- AWS Glue + S3
- Hive Metastore + HDFS/MinIO

**(Optional) Query engine like Athena/Trino/Presto or Spark SQL to validate results.**

For More details on MongoDB setup follow [this document](https://olake.io/docs/connectors/mongodb)

For this guide we will be using AWS Glue catalog for Apache Iceberg and S3 as the object store. Quick setup guide can be found [here](https://olake.io/docs/writers/iceberg/catalog/glue)

### Step 1: Setting up MongoDB

:::caution
OLake does offer a JDBC based Full Refresh and Bookmark based Incremental sync modes, so if you don't have permissions or access to enable oplog, you can directly start synching your MongoDB data with just JDBC credentials
:::

Before OLake can start replicating data from MongoDB to Apache Iceberg, you need to configure the database for logical replication.

:::info
For more details on MongoDB CDC setup for your respective environment, you can follow this document: [MongoDB and Atlas CDC Setup | OLake](https://olake.io/docs/connectors/mongodb)
:::




### Step 2: Deploy OLake UI

OLake UI provides a web-based interface for managing replication jobs, data sources, destinations, and configurations. It offers an intuitive way to create, edit, and monitor jobs without command-line complexity.

**Quick Installation with Docker**

To deploy OLake UI, you'll need Docker and Docker Compose installed on your system.

**Single Command Deployment:**
```bash
curl -sSL https://raw.githubusercontent.com/datazip-inc/olake-ui/master/docker-compose.yml | docker compose -f - up -d
```

**Access and Initial Configuration**
- **Access OLake UI**: http://localhost:8000
- **Default Login Credentials**: admin / password
- **Complete Documentation**: [OLake UI Getting Started](https://olake.io/docs/getting-started/quickstart)

**Alternative Setup**: OLake provides a configurable CLI interface for advanced users preferring command-line operations. CLI documentation: [Docker CLI Installation](https://olake.io/docs/getting-started/docker-cli)

### Step 3: Configure MongoDB Source Connection

In the OLake UI, navigate to **Sources → Add Source → MongoDB**.

**Connection Configuration:**
- **Host and Port**: Your MongoDB server endpoint
- **Username/Password**: CDC user credentials created in Step 1
- **Database Name**: Source database identifier
- **Advanced Options**: Collection selection and filtering options

OLake automatically optimizes data processing strategies for MongoDB, using efficient Change Streams processing for maximum performance during incremental sync operations.

![MongoDB Source Configuration](/img/blog/2025/14/step-3.png)

### Step 4: Configure Apache Iceberg Destination (AWS Glue)

Configure your Iceberg destination in the OLake UI for seamless lakehouse integration:

**Navigation**: Go to **Destinations → Add Destination → Glue Catalog**

**AWS Configuration:**
- **Glue Region**: AWS region for your Glue Data Catalog
- **IAM Credentials**: AWS access keys (optional if your instance has appropriate IAM roles)
- **S3 Bucket**: Storage location for Iceberg table data
- **Catalog Settings**: Additional Glue-specific configurations

**Multi-Catalog Support**: OLake supports multiple catalogs (Glue, Nessie, Polaris, Hive, Unity), providing flexibility for different architectural requirements.

**Detailed Configuration Guide**: [Glue Catalog Setup](https://olake.io/docs/connectors/glue-catalog)

**Alternative Catalogs**: For REST catalogs (Lakekeeper, Polaris) and other options: [Catalog Configuration Documentation](https://olake.io/docs/connectors)

![Iceberg Destination Configuration](/img/blog/2025/14/step-4.png)

### Step 5: Create Replication Job and Configure Collections

Once your source and destination connections are established, create and configure your replication job:

**Job Creation Process:**
1. Navigate to Jobs section and create new job
2. Configure job name and sync frequency
3. Select existing source and destination configurations
4. Choose collections/streams for Iceberg synchronization in schema section

![Job Configuration - Step 5.1](/img/blog/2025/14/step-5-1.png)

![Job Configuration - Step 5.2](/img/blog/2025/14/step-5-2.png)

**Sync Mode Options for each collection:**
- **Full Refresh**: Complete data synchronization on every job execution
- **Full Refresh + Incremental**: Initial full backfill followed by incremental updates based on Change Streams
- **Change Streams Only**: Stream only new changes from current oplog position
- **Custom Filtering**: Apply MongoDB aggregation pipeline filters for selective data replication

**Advanced Configuration Options:**
- **Normalization**: Disable for raw JSON data storage if needed
- **Partitioning**: Configure regex patterns to determine Iceberg table partitioning strategy
- **Schema Handling**: Automatic schema evolution and drift detection for flexible document structures

**Comprehensive partitioning strategies**: [Iceberg Partitioning Guide](https://olake.io/docs/understanding/iceberg-partitioning)


### Step 6: Execute Your MongoDB to Iceberg Sync

After configuring and saving your replication job:

**Execution Options:**
- **Manual Trigger**: Use "Sync Now" for immediate execution
- **Scheduled Execution**: Wait for automatic execution based on configured frequency
- **Monitoring**: Track job progress, error handling, and performance metrics
![S3 Database Structure](/img/blog/2025/14/step-6-1.png)

**Important Considerations**: Ordering during initial full loads is not guaranteed. If data ordering is critical for downstream consumption, implement sorting requirements during query execution or downstream processing stages.

#### Iceberg Database Structure in S3

Your MongoDB to Iceberg replication creates a structured hierarchy in S3 object storage:
![S3 Database Structure](/img/blog/2025/14/step6-3.png)



**Default File Formats**: OLake stores data files as Parquet format with metadata in JSON and Avro formats, following Apache Iceberg specifications for optimal query performance.


**Data Organization**: Within the ".db" folder, you'll find collections synced from MongoDB source. OLake normalizes collection and field names to ensure compatibility with Glue catalog writing restrictions.
![S3 Table Structure](/img/blog/2025/14/step-6-2.png)

**File Structure**: Each collection contains respective data and metadata files organized for efficient querying and maintenance operations.

With this setup, you now have a fully functional MongoDB-to-Iceberg pipeline running with Change Streams support, ready for analytics, lakehouse querying, and downstream consumption by various query engines.

### Step 7 (Optional): Query Iceberg Tables with AWS Athena

Validate your MongoDB to Iceberg migration by configuring AWS Athena for direct querying:

**Athena Configuration:**
- Set up Athena to connect to your Glue Data Catalog
- Execute SQL queries against replicated Iceberg tables
- Verify data consistency, query performance, and schema accuracy
- Compare results with source MongoDB data for validation

**Benefits of Athena Integration:**
- Serverless SQL querying without infrastructure management
- Pay-per-query pricing model for cost-effective analytics
- Direct Iceberg table access with full metadata support
- Integration with BI tools like QuickSight, Tableau, and Power BI

![Athena Query Interface](/img/blog/2025/14/step-7.png)

## Production Best Practices for MongoDB to Iceberg Replication

To keep your MongoDB CDC pipeline running smoothly and your Iceberg analytics performing optimally, implement these proven strategies:

### MongoDB Change Streams Configuration Best Practices

**Set Up Change Streams Properly**: Ensure your MongoDB replica set is properly configured and the oplog has sufficient size to handle your change volume. Monitor oplog usage to prevent missing changes during high-activity periods.

**Monitor Change Streams Performance**: Track Change Streams lag and processing rates to ensure real-time synchronization. Implement automated alerts for unusual processing patterns that might indicate performance issues.

### Smart Partitioning and Query Optimization

**Choose Partition Columns Strategically**: Select partition columns based on actual query patterns and data volume characteristics:

- **Temporal Partitioning**: Use created_at or updated_at for time-series analytics and event collections
- **Dimensional Partitioning**: Implement user_id or category for user analytics and dimensional data
- **Hybrid Approaches**: Combine temporal and dimensional partitioning for complex analytical workloads

Poor partitioning directly impacts performance: Improper partition choices result in slow queries and high file scan costs, making this optimization crucial for lakehouse query performance.

### Automated Maintenance and Monitoring

**Monitor Pipeline Health**: Keep continuous oversight of replication lag and sync errors. Small network hiccups or MongoDB configuration changes can disrupt Change Streams if not detected early.

**Plan for Schema Changes**: Document schemas will evolve during normal development cycles. Ensure your Iceberg pipeline and downstream consumers can evolve seamlessly with schema changes.

### State Management and Recovery Procedures

**Maintain State Backups**: The state.json file serves as the single source of truth for replication progress. Implement regular backups to prevent:
- Accidental replication resets during maintenance operations
- Data loss scenarios during disaster recovery
- Manual recovery efforts that could introduce data inconsistencies

**Automated Error Handling**: With OLake, many of these operational concerns are handled automatically, but understanding the underlying mechanisms ensures successful production MongoDB to Iceberg deployments.

## Conclusion: Transforming MongoDB Analytics with Apache Iceberg

MongoDB remains an excellent choice for operational applications, but it's not built for analytics at scale. By implementing MongoDB to Apache Iceberg replication, you achieve the best of both worlds: fast, reliable operational performance on MongoDB and powerful, cost-effective analytics on open lakehouse tables.

### Immediate Benefits of Migration

**Performance Transformation**: Companies like Memed achieved 60x improvement in data processing time (from 30-40 minutes to 40 seconds), while Netflix saw orders of magnitude faster queries compared to previous systems.

**Cost Optimization**: Natural Intelligence completed migration with zero downtime while establishing a modern, vendor-neutral platform that scales with evolving analytics needs. Organizations typically see 50-75% cost savings compared to traditional warehouse approaches.

**Operational Excellence**: With OLake's automated approach, you get:
- Full + incremental synchronization (both batch and Change Streams-based) with minimal setup complexity
- Comprehensive schema evolution support for flexible document structures
- Open file format compatibility that integrates seamlessly with your preferred query engines
- Production-ready monitoring and state management for enterprise reliability

 

### Getting Started with Your Migration

The combination of MongoDB's operational flexibility and Iceberg's analytical capabilities provides a robust foundation for modern, data-driven decision making. Whether you're building real-time dashboards, implementing advanced analytics, or developing machine learning pipelines, this replication strategy offers the scalability and flexibility that modern organizations require.

 
As the data landscape continues evolving toward open, cloud-native architectures, organizations embracing Apache Iceberg lakehouse patterns position themselves for scalable growth while maintaining operational excellence. The question isn't whether to migrate from MongoDB analytics, it's how quickly you can implement this transformation to stay competitive in today's data-driven economy.

Happy syncing!

<BlogCTA/>
