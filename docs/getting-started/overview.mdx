---
title: Overview
description: OLake getting started overview
sidebar_position: 1
---

# Overview

This getting started guide provides a comprehensive overview of OLake, for you to get started with OLake. 

## Connectors
OLake supports a variety of data sources and destinations. Below is a list of currently supported connectors:

| | Sources | Links | 
| ---| --------| ----- |
| <img src="/img/logo/mongodb.webp"  className="mt-3"  height={20} /> |  MongoDB  | [Docs](../connectors/mongodb/overview) / [Code](https://github.com/datazip-inc/olake/tree/master/drivers/mongodb) / [Getting Started](../getting-started/mongodb) |
| <img src="/img/logo/postgres.png"  className="mt-3"  height={20} /> | Postgres | [Docs](../connectors/postgres/overview) / [Code](https://github.com/datazip-inc/olake/tree/master/drivers/postgres) / [Getting Started](../getting-started/postgres) |
| <img src="/img/logo/mysql.png"  className="mt-3"  height={20} /> | MySQL |   [Docs](../connectors/mysql/overview) / [Code](https://github.com/datazip-inc/olake/tree/master/drivers/mysql) / [Getting Started](../getting-started/mysql) | 
| <img src="/img/logo/s3.png"  className="mt-3"  height={20} /> | AWS S3 |   [check issue](https://github.com/datazip-inc/olake/issues/86) | 
| <img src="/img/logo/kafka.png"  className="mt-3"  height={20} /> | Kafka |    [check issue](https://github.com/datazip-inc/olake/issues/87) | 


## Destinations

| Destination       | Supported | Docs | 
|-------------------|-----------| ---- | 
| <img src="/img/logo/iceberg.png"  className="mt-3"  height={20} /> Apache Iceberg           | Yes       | [Link](../writers/iceberg/overview) |
| <img src="/img/logo/s3.png"  className="mt-3"  height={20} /> AWS S3                | Yes       | [Link](../writers/s3/overview) |
| <img src="/img/logo/azure-adls.png"  className="mt-3"  height={20} /> Azure ADLS Gen2                | Yes       | [Link](../writers/azure-adls/overview) |
| <img src="/img/logo/gcs.png"  className="mt-3"  height={20} /> Google Cloud Storage                | Yes       | [Link](../writers/gcs/overview) |
| <img src="/img/logo/local.png"  className="mt-3"  height={20} /> Local Filesystem  | Yes       | [Link](../writers/local) |


# Doc PATH:

*Getting Started > Overview*  
***Note:** Few things can be off here.* 

## Where you can run OLake:

Below are different ways you can run OLake. 

| Platform / Orchestration | Quick-start or setup docs | Comments |
| :---- | :---- | :---- |
| Local system (bare metal / laptop) | [GitHub](https://github.com/datazip-inc/olake-docs?utm_source=chatgpt.com) |  |
| Stand-alone **Docker** container | [Docker install](https://olake.io/docs/install/docker)([OLake](https://olake.io/docs/install/docker?utm_source=chatgpt.com)) | Easiest for PoC; ships the CLI plus driver in one image. |
| **Airflow on EC2** | [EC2 DAG how-to](https://olake.io/blog/olake-airflow-on-ec2?utm_source=chatgpt.com) | Spins up a short-lived EC2 worker that pulls the OLake image, runs `sync`, then terminates. |
| **Airflow on Kubernetes** | [K8s + Airflow example](https://olake.io/blog/olake-airflow) ([OLake](https://olake.io/blog/olake-airflow?utm_source=chatgpt.com)) | Same DAG, but the `KubernetesPodOperator` schedules OLake pods inside the cluster. |

### 

## Where OLake can write the data you replicate

| Destination store | Config / reference docs | Comments |
| :---- | ----- | ----- |
| Amazon **S3 / S3 Tables?** | [S3 writer config]([OLake](https://olake.io/docs/writers/s3/config?utm_source=chatgpt.com)) | Supports both plain-Parquet and Iceberg layouts; requires `aws_access_key` / IAM role. |
| **MinIO** (self-hosted S3) |  | Drop-in S3 replacement for local tests; set `s3_endpoint`, `s3_path_style=true`. |
| Google **Cloud Storage** |  | Any S3 protocol compliant object store can work with OLake |
| Azure Blog Storage |  | Not tested |
| Local filesystem |  |  |

## File formats OLake can emit

| Output format | Docs | Comments |
| :---- | :---- | ----- |
| **Apache Iceberg** tables | [Iceberg writer overview]([OLake](https://olake.io/docs/writers/iceberg/catalog/overview?utm_source=chatgpt.com)) | Full snapshot + incremental CDC → Iceberg; works with all catalogs listed below. |
| **Parquet** files | [Parquet writer modes]([OLake](https://olake.io/docs/core/configs/writer?utm_source=chatgpt.com)) | Simple columnar dumps (no table metadata); choose local or S3 sub-mode. |

## Iceberg catalogs OLake can register to

| Catalog type | Docs / example | Comments |
| :---- | ----- | ----- |
| REST – **Gravitino** |  | Uses standard Iceberg REST; Gravitino adds multi-cloud routing. |
| REST – **Nessie** |  | Time-travel branches/tags; supply `nessie.endpoint` in `destination.json`. |
| REST – **Lakekeeper** |  | Rust-native catalog with optimistic locking; Helm chart available for K8s. |
| **Hive Metastore** | [Hive catalog config]([OLake](https://olake.io/docs/writers/iceberg/catalog/hive?utm_source=chatgpt.com)) | Classic HMS; good fit for on-prem Hadoop or EMR. |
| **JDBC Catalog** (Postgres/MySQL) | [JDBC catalog sample]([OLake](https://olake.io/docs/writers/iceberg/catalog/jdbc?utm_source=chatgpt.com)) | Stores Iceberg metadata in an RDBMS; easiest to spin up locally with Postgres. |
| **AWS Glue Catalog** | [Glue catalog IAM & config]([OLake](https://olake.io/docs/writers/iceberg/catalog/glue?utm_source=chatgpt.com)) | Best choice on AWS; lets Athena, EMR, and Redshift query the same tables instantly. |

## Data from OLake on Apache Iceberg can be queried from:

Need to verify which all query tools support which all catalogs [Sandeep Devarapalli](mailto:sandeep@datazip.io)

| QUERY TOOL | MoR / CoW Support | AWS Glue Catalog | Hive Metastore  | JDBC Catalog | Iceberg REST Catalog |
| ----- | ----- | :---: | :---: | :---: | :---: |
| Amazon Athena |  | ✅ | ❌ | ❌ | ❌ |
| Apache Spark (v3.3 +) |  | ✅ | ✅ | ✅ | ✅ |
| Apache Flink (v1.18 +) |  | ✅ | ✅ | ✅ | ✅ |
| Trino (v475 +) |  | ✅ | ✅ | ✅ | ✅ |
| Starburst Enterprise |  | ✅ | ✅ | ✅ | ✅ |
| Presto (v0.288 +) |  | ✅ | ✅ | ✅ | ✅ |
| Apache Hive v4.0 |  | ✅ | ✅ | ❌ | ✅ |
| Apache Impala v4.4 |  | ❌ | ✅ | ❌ | ❌ |
| Dremio v25/26 |  | ✅ | ✅ | ❌ | ✅ |
| DuckDB v1.2.1 |  | ❌ | ❌ | ❌ | ✅ |
| ClickHouse v24.3 + |  | ❌ | ❌ | ❌ | ✅ |
| StarRocks v3.2 + |  | ❌ | ✅ | ❌ | ✅ |
| Apache Doris v2.1 + |  | ✅ | ✅ | ❌ | ✅ |
| Google BigQuery (BigLake) |  | ❌ | ❌ | ❌ | ❌* |
| Snowflake (Iceberg GA) |  | ❌ | ❌ | ❌ | ✅ |
| Databricks (Unity Catalog API) |  | ❌ | ❌ | ❌ | ✅ |
|  |  |  |  |  | ✅ |

* BigQuery’s BigLake tables read Iceberg manifests directly without using an Iceberg catalog, so none of the four catalog types apply.

TODO:

1. Ingestion tool support for MOR / COW

## Query OLake dumped data Using Different Catalogs

1. AWS Glue + AWS Athena  
2. Glue + Spark  
3. Glue + Snowflake  
4. Glue + DuckDB  
5. REST Catalog + DuckDB  
6. REST catalog + ClickHouse  
7. Need to add more - [Rohan Khameshra](mailto:rohan@datazip.io) [Merlyn Mathew](mailto:merlyn@datazip.io) 

## Catalogue Support[​](https://olake.io/docs/roadmap#catalogue-support)

OLake supports:

1. [REST Catalog](https://olake.io/docs/writers/iceberg/catalog/rest)   
   1. Gravitino   
   2. Nessie   
   3. Lakekeeper   
   4. Unity  
2. [Hive Meta Store](https://olake.io/docs/writers/iceberg/catalog/hive)   
3. [JDBC Catalog](https://olake.io/docs/writers/iceberg/catalog/jdbc)   
4. [AWS Glue Catalog](https://olake.io/docs/writers/iceberg/catalog/glue)  
5. Azure Purview - Not Planned, [submit a request](https://github.com/datazip-inc/olake/issues/new?template=new-feature.md)  
6. BigLake Metastore - Not Planned, [submit a request](https://github.com/datazip-inc/olake/issues/new?template=new-feature.md)

Need to verify which all query tools support which all catalogs [Sandeep Devarapalli](mailto:sandeep@datazip.io)

| Sl | Query / Analytics Engine (“tool”) | Official doc / How-to link | Supported Iceberg catalogs | Comments |
| ----- | ----- | ----- | ----- | ----- |
| 1 | Amazon Athena | [Query Iceberg tables] [AWS Documentation](https://docs.aws.amazon.com/athena/latest/ug/querying-iceberg.html?utm_source=chatgpt.com) | Glue | Athena-v3 can read & write Iceberg v2 tables **only** when they’re registered in Glue. |
| 2 | Apache Spark (3.3 → 4.x) | [Spark catalog config] [Apache Iceberg](https://iceberg.apache.org/docs/latest/spark-configuration/?utm_source=chatgpt.com) | Glue, Hive, REST, JDBC | Configure with `spark.sql.catalog.<name>.type` = `glue |
| 3 | Apache Flink (1.18+) | [Flink catalog options] [Apache Iceberg](https://iceberg.apache.org/docs/1.8.0/flink-configuration/?utm_source=chatgpt.com) | Glue, Hive, REST, JDBC | Set catalog-type=glue |
| 4 | Trino ( ≥ 475 ) | [Iceberg metastores] [trino.io](https://trino.io/docs/current/object-storage/metastores.html?utm_source=chatgpt.com) | Glue, Hive, REST, JDBC | iceberg.catalog.type=glue |
| 5 | Starburst Enterprise (SEP 413 →) | [SEP Iceberg connector] [Starburst](https://docs.starburst.io/latest/object-storage/metastores.html?utm_source=chatgpt.com) | Glue, Hive, REST, JDBC | Same config keys as Trino; extra features (Warp Speed, Polaris). |
| 6 | PrestoDB (0.288+) | [Lab guide using REST] [IBM GitHub](https://ibm.github.io/presto-iceberg-lab/lab-1/?utm_source=chatgpt.com) | Glue*, Hive, REST, JDBC | Glue needs the AWS SDK fat-jar; REST landed in 0.288. |
| 7 | Apache Hive (4.0) | [Hive integration] [Apache Iceberg](https://iceberg.apache.org/docs/latest/hive/?utm_source=chatgpt.com) | Hive, Glue (via Iceberg-AWS), Hadoop | Uses StorageHandler; for Glue add AWS bundle. |
| 8 | Apache Impala (4.4) | [Impala Iceberg docs] [Impala](https://impala.apache.org/docs/build/html/topics/impala_iceberg.html?utm_source=chatgpt.com) | Hive, Hadoop | Can create & query Iceberg; Glue via HMS federation only. |
| 9 | Dremio (25/26) | [REST catalog support] [Dremio Documentation](https://docs.dremio.com/current/release-notes/version-260-release/?utm_source=chatgpt.com) | Glue, Hive, REST | Add source in **Lakehouse Catalogs** UI; JDBC not yet. |
| 10 | DuckDB (1.2.1) | [Iceberg extension] [DuckDB](https://duckdb.org/docs/stable/extensions/iceberg/overview.html?utm_source=chatgpt.com) [REST preview] [DuckDB](https://duckdb.org/2025/03/14/preview-amazon-s3-tables.html?utm_source=chatgpt.com) | REST, Hive (path-based) | `INSTALL iceberg; LOAD iceberg;` then `ATTACH 'nessie://…'` or `ATTACH 'glue+rest://…'`. |
| 11 | ClickHouse (24.3+) | [Iceberg table engine] [ClickHouse](https://clickhouse.com/docs/engines/table-engines/integrations/iceberg?utm_source=chatgpt.com) | REST, Hadoop | Read-only engine; REST integration roadmap notes catalog auto-discovery. |
| 12 | StarRocks (3.2+) | [StarRocks Iceberg quick-start][StarRocks Documentation](https://docs.starrocks.io/docs/quick_start/iceberg/) | REST, Hive | Create `CREATE CATALOG iceberg … PROPERTIES('type'='iceberg','metastore'='…')`. |
| 13 | Apache Doris (2.1+) | [Doris Iceberg catalog] [doris.apache.org](https://doris.apache.org/docs/dev/lakehouse/best-practices/doris-iceberg?utm_source=chatgpt.com) | REST, Hive, Glue, Hadoop | Multi-catalog; external queries and **writes** supported since v3.1 |
| 14 | Google BigQuery (BigLake) | [BigLake Iceberg tables] [Google Cloud](https://cloud.google.com/bigquery/docs/iceberg-external-tables?utm_source=chatgpt.com) |  | Creates **external tables**; catalog-less — points directly at manifests in GCS/S3. |
| 15 | Snowflake (2025 GA) | [Snowflake Iceberg tables] [Snowflake Documentation](https://docs.snowflake.com/en/user-guide/tables-iceberg?utm_source=chatgpt.com) | Snowflake-native, REST (read-only) | Snowflake maintains its own catalog but can **read** external REST catalogs (Unity, Glue REST). |
| 16 | Databricks Unity Catalog | [UC Iceberg REST endpoint] [Databricks Documentation](https://docs.databricks.com/gcp/en/external-access/iceberg?utm_source=chatgpt.com)[Databricks Documentation](https://docs.databricks.com/aws/en/external-access/integrations?utm_source=chatgpt.com) | REST | Endpoint `/api/2.1/unity-catalog/iceberg`; external clients Spark, Flink, Trino, DuckDB, ClickHouse, etc. |
| 17 | e6data Lakehouse Compute Engine | [e6data × S3 Tables] [e6data.com](https://www.e6data.com/blog/e6data-integrates-with-s3-tables?utm_source=chatgpt.com) | Glue REST, REST, Hive (read) | Serverless SQL engine; advertises compatibility with “all table formats & catalogs.” |

* Glue support in Presto requires the `hive.metastore=glue` shim or running Presto inside AWS EMR.

**Notes & gotchas**

* **REST flavours.** “REST” above covers standard Iceberg REST plus branded servers (Nessie, Lakekeeper, Gravitino, AWS Glue REST endpoint, Databricks Unity, Snowflake Polaris).  
* **JDBC catalog** is production-ready in Spark, Flink, Trino/Starburst, and Presto. Engines not listed in that column (e.g., ClickHouse) cannot yet use it.  
* **Hive vs Hadoop.** Some engines list “Hadoop Catalog” separately (path-based, no service). I’ve rolled those under *Hive* here if the engine simply re-uses the HMS client.  
* **Read-only vs read-write.** ClickHouse and BigQuery are read-only; Athena supports `INSERT/UPDATE/MERGE` (MoR only); most others are full read-write when using Glue/Hive/JDBC/REST.

