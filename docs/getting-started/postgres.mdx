---
title: Postgres to Iceberg
description: Getting Started with Postgres connector for OLake 
sidebar_position: 3
---


# Getting Started with OLake for Postgres

OLake helps you replicate data from Postgres into local or S3-based data lakes using Parquet or Iceberg table formats. This tutorial walks you through every step of the setup—from creating necessary configuration files to running your first data sync.

:::info
OLake UI is live (beta)! You can now use the UI to configure your Postgres source, discover streams, and sync data. Check it out at [OLake UI](./olake-ui) regarding how to setup using Docker Compose and running it locally.

![olake-source-postgres](/img/docs/sources/olake-source-postgres.png)

Now, you can use the UI to configure your Postgres source, discover streams, and sync data. 

Refer to [Postgres Connector documentation](../connectors/postgres/overview) for more details.
:::


## TLDR:
1. **Create a [`source.json`](../connectors/postgres/config)** with your Postgres connection details.
2. **Create a `destination.json`** with your Writer ([Apache Iceberg](../writers/iceberg/catalog/overview) / [AWS S3](../writers/parquet/s3) / [Azure ADLS](../writers/iceberg/azure) / [Google Cloud Storage](../writers/iceberg/gcs)) connection details.
3. **Run `discover`** to generate a `streams.json` of available streams.
4. **Run `sync`** to replicate data to your specified destination.


![discover-sync](/img/docs/discover-and-sync-low.gif)

## Introduction & Requirements

To use OLake, ensure you have:
- **Docker** installed and running on your machine.  
- **Postgres** credentials (hosts, replica set name, username/password if applicable).  
  - Install Postgres Server
  - [Docker Compose instructions to spin up Postgres replica sets](../connectors/postgres/setup/local)
  - Need sample dataset to ingest in Postgres? Refer  -> [Sample Datasets](../resources/dataset)
- **AWS S3 credentials** (if you plan to write data to AWS S3).  
- **Apache Iceberg** and **Catalog configuration** credentials (if you plan to write data to Iceberg tables).

Refer here for more details on [Writer requirements](../writers/overview).

You will also need:
- **An empty directory** to store OLake configuration files and outputs. This guide will refer to it as `OLAKE_DIRECTORY`.

:::note
For setting up the project locally on your system and debugging configs to be made, follow this guide - [Setting up debugger in VS Code](../community/contributing)
:::

## Step 1: Prepare Your Directory

1. **Create a new directory** on your local machine. Let’s call it `OLAKE_DIRECTORY`:

   ```bash
   mkdir OLAKE_DIRECTORY
   ```

2. **Inside this folder**, create two files:
   - **`destination.json`**: Specifies your output destination (local or S3).  
   - **`source.json`**: Contains connection settings for Postgres (or other databases in the future). <br/>


   ```bash
   cd OLAKE_DIRECTORY
   touch destination.json
   touch source.json
   ```

> **Folder Structure**:
> ```
> OLAKE_DIRECTORY/
>   ├─ destination.json
>   └─ source.json
> ```

![Directory](/img/docs/getting-started/dir-structure-local.webp)


### 1.1 Example `destination.json` 

Refer to Destination config section for individual writers or refer them [here.](../core/configs/writer).

1. [Apache Iceberg Writer](../writers/iceberg/catalog/overview)
2. [AWS S3 Writer](../writers/parquet/s3)
3. [Local FileSystem Writer](../writers/parquet/local)


### 1.2 Example `source.json` (Postgres)

Below is a sample `source.json` for connecting to a Postgres replica set. Customize each field to match your environment.

<PostgresSourceConfig/>

### Description of above parameters

Refer to [source configuration](../connectors/postgres/config) for more details on `source.json`.

## Step 2: Generate a Streams File

OLake needs to discover which collections (streams) exist in your Postgres. This step will create a `streams.json` listing available streams, schemas, and default sync modes.

1. **Open your terminal** in the same directory (say `OLAKE_DIRECTORY`) containing `source.json` and `destination.json`.
2. **Run the `discover` command** using Docker:

<Tabs>

<TabItem value="docker-postgres" label="OLake Docker" default>
<DockerDiscoverPostgres/>
</TabItem>

<TabItem value="docker-local" label="Locally run OLake" default>
<LocalDiscoverPostgres/>
</TabItem>

</Tabs>

:::info
`PATH_TO_OLAKE_DIRECTORY` is the absolute path where you have created the directory [as discussed above].
`-v "$HOME/PATH_TO_OLAKE_DIRECTORY:/mnt/config" \` maps to `-v /Users/JOHN_DOE_USERNAME/Desktop/projects/OLAKE_DIRECTORY:/mnt/config \` in macOS and Linux systems. Follow the same pattern in other systems.
:::


![Catalog](/img/docs/getting-started/catalog.webp)


| Flag/Parameter           | Description                                                     |
|--------------------------|-----------------------------------------------------------------|
| `discover`               | The OLake sub-command that scans Postgres schemas.              |
| `--config /mnt/config/source.json` | Tells OLake where to find your Postgres connection details. |



### 2.1 Understanding the `streams.json` File

After running `discover`, OLake generates `streams.json` in `OLAKE_DIRECTORY` with entries like:

```json title="OLAKE_DIRECTORY/streams.json" {2}
{
    "selected_streams": {
        "otter_db": [
            {
                "partition_regex": "{now(),2025,YYYY}-{now(),06,MM}-{now(),13,DD}/{string_change_language,,}",
                "stream_name": "stream_0",
                "normalization": false,
                "append_only": false,
                "chunk_column": "",         //column name to be specified
            },
            {
                "partition_regex": "{,1999,YYYY}-{,09,MM}-{,31,DD}/{latest_revision,,}",
                "stream_name": "stream_8",
                "normalization": false,
                "append_only": false,
                "chunk_column": "",         //column name to be specified
            }
        ]
    },
    "streams": [
        {
            "stream": {
                "name": "stream_8",
                "namespace": "otter_db",
                "type_schema": { ... },
                "supported_sync_modes": [
                    "full_refresh",
                    "cdc"
                ],
                "source_defined_primary_key": [
                    "_id"
                ],
                "available_cursor_fields": [],
                "sync_mode": "cdc"
            }
        },
    // ... other streams
  ]
}
```

- **`selected_streams`**: The streams / tables / collections OLake will replicate.  
- **`streams`**: Metadata for each discovered collection, including schemas and sync modes (e.g., `cdc`, `full_refresh`).
- **`partition_regex`**: Specify the regex pattern. For more details, refer to [S3 docs](../writers/parquet/partitioning)
- **`normalization`**: If set to `true`, OLake will flatten nested JSON structures (Level 1 flattening)
- **`append_only`**: The `append_only` flag determines whether records can be written to the iceberg delete file. If set to true, no records will be written to the delete file. Know more about delete file: [Iceberg MOR and COW](https://olake.io/iceberg/mor-vs-cow) 
- **`chunk_column`**: Column name to be specified, used to divide data into chunks for efficient parallel querying and extraction from the database. 


:::tip
**Exclude Streams**: You can remove unneeded collections by editing `selected_streams` directly. For instance, deleting `"customers"` if you only want to sync `orders`.
:::


**Before (including `customers`):**
```json title="OLAKE_DIRECTORY/streams.json"
"selected_streams": {
        "otter_db": [
            {
                "stream_name": "order",
                "partition_regex": "",
                "normalization": false,
                "append_only": false,
                "chunk_column": "",         //column name to be specified
            },
            {
                "stream_name": "customer",
                "partition_regex": "",
                "normalization": false,
                "append_only": false,
                "chunk_column": "",         //column name to be specified
            }
        ]
    },
```
**After (to exclude `customers`):**
```json title="OLAKE_DIRECTORY/streams.json"
"selected_streams": {
        "otter_db": [
            {
                "stream_name": "order",
                "partition_regex": "",
                "normalization": false,
                "append_only": false,
                "chunk_column": "",         //column name to be specified  
            },
        ]
    },
```

## Step 3: Run Your First Data Sync

Now that you have `streams.json`, it’s time to **sync** data from Postgres to your specified destination (local or S3).

<Tabs>

<TabItem value="docker-postgres" label="OLake Docker" default>
<DockerSyncPostgres/>
</TabItem>

<TabItem value="docker-local" label="Locally run OLake" default>
<LocalSyncPostgres/>
</TabItem>

</Tabs>


![First Data Sync](/img/docs/getting-started/full-sync.webp)


| Flag/Parameter                                | Description                                                                |
|-----------------------------------------------|----------------------------------------------------------------------------|
| `sync`                                        | The OLake sub-command that runs a data replication (snapshot + CDC).       |
| `--config /mnt/config/source.json`            | Postgres connection settings.                                               |
| `--catalog /mnt/config/streams.json`          | The file detailing which streams OLake will replicate.                     |
| `--destination /mnt/config/destination.json`       | The output configuration file (local or S3).                               |

- This command **performs both** the **initial snapshot**
- If you only want a full one-time snapshot, set the stream’s `sync_mode` to `"full_refresh"` in `streams.json`.

:::info
**Example**: If your `sync_mode` is `"cdc"`, OLake will:
1. Do a one-time full snapshot of each selected collection.
2. Automatically begin listening to Postgres’s oplog for near real-time changes.
:::

When the sync finishes, you should see new files either:
- **Locally** (in the volume-mapped directory).
- **On S3** (inside the specified `s3_path`).

### Step 3.1 Synced Data 
If you are using VS Code, install a parquet reader extension to visualize the parquet file contents that will be made post sync process.

![First Data Sync JSON](/img/docs/getting-started/json-parquet.webp)

### Step 3.2 Synced Data Normalized

If you have turned on `"normalization": true` in `streams.json` for you streams,  expect the below Level 1 Flattening of JSON data.

Read more about JSON flattening here - [Flatten Object Types and Query Arrays in Semi-Structured Nested JSON](https://olake.io/blog/flatten-array)

### Running the sync command with normalization turned on

![JSON Normalized logs](/img/docs/getting-started/normalized-logs.webp)

### Output Data Dump

![First Data Sync JSON Normalized](/img/docs/getting-started/normalized-parquet.webp)

### Step 3.3 Change output directory

If you need to output the parquet dump to some other location, you can make changes in the `destination.json` file by appending the `/mnt/config/my_directory`

```json title="OLAKE_DIRECTORY/destination.json" {5}
{
    "type": "PARQUET",
       "writer": {
         "local_path": "/mnt/config/my_directory" 
    }
  }
```

Here, `/mnt/config` represents the `OLAKE_DIRECTORY`. 

## Step 4: Resume Sync with a State File

If a sync is interrupted or you need to **resume** from a previous checkpoint, OLake automatically saves progress in a `state.json` file. Use the `--state` parameter to continue from that point:



<Tabs>

<TabItem value="docker-postgres" label="OLake Docker" default>
<DockerSyncWithStatePostgres/>
</TabItem>

<TabItem value="docker-local" label="Locally run OLake" default>
<LocalSyncWithStatePostgres/>
</TabItem>

</Tabs>


![Resume Sync with a State File](/img/docs/getting-started/state-command-run.webp)

| Flag/Parameter                      | Description                                                           |
|------------------------------------|-----------------------------------------------------------------------|
| `--state /mnt/config/state.json`   | Points OLake to an existing state file.                               |


- **`state.json`** typically includes a `resume token` (for Postgres) or an offset for other databases, ensuring OLake **does not** reprocess records it has already synced.

<PostgresStateConfig/>

In this example, `"_data": "8267B34D6..."` is a Postgres resume token that tells OLake where to pick up the CDC stream.

For more details on the  `state.json` configuration, refer the [state docs](../connectors/postgres/config#statejson-configuration)


## Debugging
Follow the debugging instructions in this guide - [Setting up debugger in VS Code](../community/contributing)

## Docker Commands & Flags
Click here for more info about [Docker Commands & Flags](../install/docker)

## Next Steps & Wrap-Up

1. **Check Your Output**: Verify your **Parquet files** (or Iceberg tables) were created either locally or in your S3 bucket.
2. **Explore Schema Evolution**: If your Postgres documents gain new fields, OLake can adapt automatically. Watch for updated schemas in subsequent runs.
3. **Try More Destinations**: OLake can also write to **Iceberg** on S3 (and more in the future). Update your destination config as needed.
4. **Analytics & Querying**: Connect your newly created Parquet/Iceberg data to engines like **Trino**, **Spark**, or **Presto** for powerful querying.

**Congratulations!** You’ve completed your first OLake data replication. If you encounter any issues or have feedback, please visit our [GitHub repository](https://github.com/datazip-inc/olake) to open an issue or contribute.

