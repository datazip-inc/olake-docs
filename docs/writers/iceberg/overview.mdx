---
title: Overview
description: OLake Apache Iceberg writer description
sidebar_position: 1
---

# Overview

## What is the Iceberg Writer?

The Iceberg Writer syncs data from databases (MySQL, MongoDB, PostgreSQL) into Apache Iceberg. It supports two primary modes:

- **Backfill Mode (Append):** For initial bulk data loads.
- **CDC Mode (Upsert):** For change data capture using equality delete writes—without the need for Spark.

## Architecture

The data flow is as follows:

1. **iceberg.go:** Starts the process.
2. **Java gRPC Server:** A Java process is spawned that receives data via gRPC in batches (256 MB in-memory size).
3. **Write to Iceberg:** The Java process writes data to an Iceberg catalog stored on S3.

The architecture diagram:

```
Golang Code  --gRPC-->  Java (Iceberg Writer)  --Write to Iceberg-->  S3 + Iceberg Catalog
```

## Supported Catalogs

| Catalog Type | Description                                            |
|--------------|--------------------------------------------------------|
| JDBC         | Uses PostgreSQL as the metadata catalog (local testing)|
| AWS Glue     | Uses AWS Glue for metadata catalog and AWS S3 for storage|

## Quick Start Guide

1. **Install Prerequisites:**  
   Follow the installation instructions for Java 17, Docker & Docker Compose, and Maven (see the respective pages).



2. **Set Environment:**  
   Use the Docker Compose setup (see the [Docker Compose](./docker-compose) page) to bring up the local test environment (Postgres JDBC, Minio, and Spark) or use AWS Glue + S3.

3. **Configure the Writer:**  
   Create your `writer.json` file using the configurations detailed on the [Configs](./config) page.



<Tabs>
<TabItem value="glue-config" label="Glue" default>

### Writer Configuration

<IcebergWriterConfig/>

Get more information refer [here](../iceberg/config#aws-s3--glue-configuration)

</TabItem>

<TabItem value="minio-local" label="MinIO" default>

### Writer Configuration

<IcebergWriterConfigLocal/>

Get more information refer [here](../iceberg/config#local-test-configuration-jdbc--minio)

</TabItem>
</Tabs>



4. **Run Sync Commands:**  
   - **Discover Command:** `<DISCOVER_COMMAND>`  
   - **Sync Command:** `<SYNC_COMMAND>`  
   - **Sync with State Command:** `<SYNC_WITH_STATE_COMMAND>`  
     
Refer to respective Database docs to use the command for discover schema and sync the data.
- [MongoDB Discover and sync command](../../connectors/mongodb/overview#sync-command)
- [Postgres Discover and sync command](../../connectors/postgres/overview#sync-command)
- [MySQL Discover and sync command](../../connectors/mysql/overview#sync-command)


5. **Verify Data:**  
   Connect to the `spark-iceberg` container and run a Spark SQL query to verify that data is populated.


## FAQ

**Q1: What databases are supported by the Iceberg Writer?**  
A: The writer supports MySQL, MongoDB, and PostgreSQL as data sources.

**Q2: What are the two primary modes of operation?**  
A:  
- **Backfill Mode:** Uses append mode for initial data loads.  
- **CDC Mode:** Uses upsert mode with equality delete writes for capturing changes.

**Q3: Why is Java used instead of Golang for the writer?**  
A: The current Golang‑iceberg project does not support equality delete writes. Java is used until the migration to iceberg‑rust is completed to improve memory usage.

**Q4: How do I verify that data is populated in Iceberg?**  
A: After running the sync, connect to the `spark-iceberg` container and run a Spark SQL query such as:

```sql
select * from olake_iceberg.olake_iceberg.table_name;
```

**Q5: Where can I update my configuration settings?**  
A: All configuration settings can be modified in the `writer.json` file, as detailed on the [Configs](./config) page.
