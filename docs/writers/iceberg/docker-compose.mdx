---
title: Local Iceberg Setup
description: Setup Docker Compose 
sidebar_position: 7
---

# Docker Compose to local setup and testing

This page explains how to set up your local test environment using Docker Compose. Here's is the [`docker-compose.yml`](https://github.com/datazip-inc/olake/blob/master/writers/iceberg/local-test/docker-compose.yml) file used for local setup and testing.

The setup is simple:

**Step 1.** Clone the OLake repository and navigate to the local test directory.
```bash
git clone git@github.com:datazip-inc/olake.git
```

**Step 2.** Start the Docker Compose services.

**Step 3.** Create the source configuration file (`source.json`) in any directory of your choosing. Just make sure to reference its file path it in the sync command.

**Step 4.** Create the destination configuration file (`destination.json`).

**Step 5.** Run the discover process for stream schema discovery. ( This command creates a `streams.json` file with complete schema of your database like all streams / tables / collections, column names. column data types, etc. You can also set [`partition_regex`](../parquet/partitioning)).  

**Step 6.** Run the sync process.  

**Step 7.** Verify the data population via `spark-sql`.

Now lets move on to detailed setup instructions.

## Prerequisites Installation

This section covers the installation steps for Java 17, Docker & Docker Compose, and Maven for macOS (M1 & Intel), Linux, and Windows.

### Java 17

  <Tabs>

  <TabItem value="java17-macos" label="macOS (M1 & Intel)" default>

  **Install via Homebrew:**
  ```bash
  brew install openjdk@17
  ```
  Or follow the [Adoptium Installation Guide](https://adoptium.net/).

  **Verification:**
  ```bash
  java -version
  ```
  Ensure it mentions Java 17.

  > **Note:** If needed, add `export PATH="/usr/local/opt/openjdk@17/bin:$PATH"` (or equivalent) to your shell config (e.g., `~/.bashrc` or `~/.zshrc`).

  </TabItem>

  <TabItem value="java17-linux" label="Linux">

  **For Ubuntu/Debian:**
  ```bash
  sudo apt update && sudo apt install openjdk-17-jdk
  ```
  **Verification:**
  ```bash
  java -version
  ```
  Ensure it mentions Java 17.

  > **Note:** Add the Java binary to your `PATH` if your system doesnâ€™t do it automatically.

  </TabItem>


  <TabItem value="java17-windows" label="Windows">

  1. Download from [Oracle](https://www.oracle.com/java/technologies/javase/jdk17-archive-downloads.html) or [Adoptium](https://adoptium.net/).
  2. Run the installer.
  3. Make sure the Java `bin` directory is in your **System PATH**.

  **Verification (Command Prompt):**
  ```bash
  java -version
  ```
  Ensure it mentions Java 17.

  </TabItem>
  </Tabs>


### Docker & Docker Compose
  <Tabs>

  <TabItem value="docker-macos" label="macOS (M1 & Intel)" default>

  **Option 1: Docker Desktop**
  1. Install Docker Desktop from [Docker Hub](https://www.docker.com/products/docker-desktop).
  2. Ensure Docker is running.

  **Option 2: Homebrew**
  ```bash
  brew install --cask docker
  ```
  **Verification:**
  ```bash
  docker --version
  ```

  </TabItem>


  <TabItem value="docker-linux" label="Linux">

  Follow [Docker Docs](https://docs.docker.com/engine/install/) for your distribution.  
  **For Ubuntu/Debian (example):**
  ```bash
  sudo apt update && sudo apt install docker.io docker-compose
  ```
  **Verification:**
  ```bash
  docker --version
  ```

  </TabItem>


  <TabItem value="docker-windows" label="Windows">

  1. Download **Docker Desktop** from [Docker Hub](https://www.docker.com/products/docker-desktop).
  2. Run the installer.
  3. Launch Docker Desktop.

  **Verification (Command Prompt):**
  ```bash
  docker --version
  ```

  </TabItem>
  </Tabs>



### Maven Installation
  <Tabs>
  <TabItem value="maven-macos" label="macOS (M1 & Intel)" default>


  **Install via Homebrew:**
  ```bash
  brew install maven
  ```
  Or download from [Apache Maven](https://maven.apache.org/download.cgi).

  **Verification:**
  ```bash
  mvn -version
  ```

  </TabItem>


  <TabItem value="maven-linux" label="Linux">

  **For Ubuntu/Debian:**
  ```bash
  sudo apt install maven
  ```
  Or follow the [Maven Installation Guide](https://maven.apache.org/install.html) for other distros.

  **Verification:**
  ```bash
  mvn -version
  ```

  </TabItem>


  <TabItem value="maven-windows" label="Windows">

  1. Download Maven from [Apache Maven](https://maven.apache.org/download.cgi).
  2. Extract or install it.
  3. Add the Maven `bin` folder to your PATH.

  **Verification (Command Prompt):**
  ```bash
  mvn -version
  ```

  </TabItem>
  </Tabs>


### Local catalog Test Setup
You can test:
1. Glue
2. Hive
3. JDBC
4. REST

catalogs using the local test setup.

**Steps to run:**

1. **Prerequisite:** Ensure Docker is installed (instructions in the Prerequisites section above).
2. **Clone OLake and Navigate to the local test directory:**
   ```shell
   git clone git@github.com:datazip-inc/olake.git
   cd writers/iceberg/local-test
   ```
3. **Start the docker-compose services:**
   ```shell
   docker compose up
   ```
   > This command starts the following services:  
   > - **Postgres:** Acts as the JDBC catalog.
   > - **Lakekeeper:** Acts as the Iceberg catalog.  
   > - **hive-metastore:** Acts as the Hive catalog.
   > - **Minio:** Provides an AWS S3-like filesystem for storing Iceberg data.  
   > - **Spark:** For querying the Iceberg data.

3. **Create the source configuration file:**  
  Get the source configs of all Databases [here](../../core/configs/source) from where you wish to sync data from.

4. **Create the destination configuration file:**  
   Create a file named `destination.json` in your working directory with the following content:
  

<Tabs>
    <TabItem value="iceberg-glue" label="AWS Glue" default>

    <GlueIcebergWriterConfig/>

    </TabItem>

    <TabItem value="iceberg-local" label="JDBC Catalog">

    <MinioJDBCIcebergWriterConfigLocal/>

    </TabItem>

    <TabItem value="rest" label="REST Catalog" default>


    <RESTIcebergWriterConfig/>

    </TabItem >

    <TabItem value="hive" label="Hive Catalog" default>

    <HiveIcebergWriterConfig/>

    </TabItem >

</Tabs>


Refer [here](../iceberg/catalog/overview) for more about catalog configs.

5. **Run the Sync Process:**  
   - **Discover Command:** _`<DISCOVER_COMMAND>`_  
   - **Sync Command:** _`<SYNC_COMMAND>`_  
   - **Sync with State Command:** _`<SYNC_WITH_STATE_COMMAND>`_  

Refer to respective Database docs to use the command for discover schema and sync the data.
- [MongoDB Discover and sync command](../../connectors/mongodb/overview#sync-command)
- [Postgres Discover and sync command](../../connectors/postgres/overview#sync-command)
- [MySQL Discover and sync command](../../connectors/mysql/overview#sync-command)


:::note
1. The first time you run the sync command, it might take a while for the process to build and download the necessary dependencies, docker images and `.jar` files. Subsequent runs will be faster.
2. The `destination.json` file should be in the same directory where you run the sync command.
:::


:::tip
If you are testing the docker compose setup with changing some configs regarding destination and some processes get stuck, consider removing / deleting the following files and directories:
- `target` directory from - `writers/iceberg/debezium-server-iceberg-sink/target` 
- `debezium-server-iceberg-sink.jar` file.
- restart the docker compose services.
:::

If you see the final screen after running the sync command such as the below one:
![iceberg-sync](/img/docs/iceberg/iceberg-sync.png)

the data is synced successfully to the Iceberg table.

6. **Verify Data Population:**

   **Connect to the `spark-iceberg` container**:
   ```sh
   docker exec -it spark-iceberg bash
   ```
   **Start Spark SQL**:
   ```sh
   spark-sql
   ```
   **Run a query to inspect your data**:
   ```sql
   select * from olake_iceberg.olake_iceberg.table_name;
   ```


:::info
The `__op` column values will have `r`, `c`, `u`, `d` that stands for - "r" for read/backfill, "c" for create, "u" for update, and "d" for delete. The `__op` column is used to track the operation performed on the data. 
:::




### Using Spark UI
Query data using Spark SQL in a Jupyter notebook or via the Spark UI. You can access the Spark UI via Jupyter Notebook or directly through the Spark UI.

Step 1: Head over to [http://localhost:8888](http://localhost:8888) to access the Spark UI. 
Step 2: Click on `File` > `New` > `Notebook`.

![select-catalog](/img/docs/iceberg/spark-1.png)

Step 3: Choose `Python 3` as the kernel.

![choose-kernel](/img/docs/iceberg/spark-2.png)

Step 4: The Jupyter Notebook will open in a new tab. 
Step 5: To run the SQL queries, you can use the following code snippet in a Jupyter Notebook cell:

```sql
%%sql

SELECT * FROM CATALOG_NAME.ICEBERG_DATABASE_NAME.TABLE_NAME;
```

![query-sql-via-spark](/img/docs/iceberg/spark-3.png)

- `CATALOG_NAME` can be: `jdbc_catalog`, `hive_catalog`, `rest_catalog`, etc.
- `ICEBERG_DATABASE_NAME` is the name of the Iceberg database you created / added as a value in `destination.json` file.


Now you can run queries on your Iceberg data using Spark SQL. Some useful commands are:
- `show databases;`
- `use <database_name>`
- `show tables from olake_iceberg.olake_iceberg;`
- `describe formatted olake_iceberg.olake_iceberg.table_name;`

:::info
Refer to [spark-defaults.conf](https://github.com/datazip-inc/olake/blob/master/destination/iceberg/local-test/spark-defaults.conf) for more information about the values set for catalog configurations.
