---
title: 4. Hive
description: Hive Metastore Catalog configuration for OLake Apache Iceberg
sidebar_position: 4
---

# Hive Catalog Write Guide

OLake integrates with **Hive Metastore** to provide full support for **Apache Iceberg tables**.

With this setup:
- **Data** is stored in object storage (S3, MinIO, GCS, or any S3-compatible system).
- **Metadata** is managed in Hive Metastore (via Thrift protocol).
- **OLake** seamlessly writes into Iceberg tables using Hive Metastore + object storage.

---

## Prerequisites

Before configuring OLake with Hive Metastore Catalog, ensure the following:

#### 1. Hive Metastore Service

- A running Hive Metastore service accessible via Thrift protocol (default port: 9083).
- This can be a standalone Hive Metastore, part of a Hadoop cluster, or a managed service like GCP Dataproc Metastore.

#### Required Metastore Permissions

The Hive Metastore must have sufficient privileges to manage Iceberg metadata. OLake requires the following permissions:

> **CREATE DATABASE** - Creates Iceberg databases/namespaces for organizing tables
>
> **CREATE TABLE** - Creates new Iceberg table metadata entries in the metastore
>
> **ALTER TABLE** - Modifies existing table metadata during schema evolution and partition updates
>
> **DROP TABLE** - Removes table metadata when tables are dropped
>
> **SELECT** - Reads metadata for table discovery, schema validation, and query planning operations

:::info Metastore Setup
For detailed Hive Metastore setup instructions and advanced configurations, refer to the [**Apache Hive Metastore documentation**](https://cwiki.apache.org/confluence/display/Hive/AdminManual+Metastore+Administration) and [**Apache Iceberg Hive Catalog documentation**](https://iceberg.apache.org/docs/latest/hive/).
:::

#### 2. Object Storage

- A bucket for storing **Iceberg data files (Parquet + metadata)**.
- Supported storage: AWS S3, MinIO, Google Cloud Storage, or any S3-compatible storage.

---

## Configuration

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import HiveIcebergWriterConfig from '@site/docs/shared/config/HiveIcebergWriterConfig.mdx';
import HiveIcebergWriterUIConfigDetails from '@site/docs/shared/config/HiveIcebergWriterUIConfigDetails.mdx';
import HiveIcebergWriterConfigDetails from '@site/docs/shared/config/HiveIcebergWriterConfigDetails.mdx';

<Tabs>
<TabItem value="hive-ui" label="OLake UI" default>

#### Prerequisite
Ensure OLake UI is running (see [OLake UI setup](/docs/install/olake-ui)).

After setting up the source, configure your **destination with Hive Metastore Catalog**.

<div style={{textAlign: 'center'}}>

</div>

<HiveIcebergWriterUIConfigDetails/>

**Click `Next →`** to test the connection. OLake will verify Hive Metastore + object storage connectivity.

</TabItem>

<TabItem value="hive-cli" label="OLake CLI">

Create a `destination.json` with the following configuration:

<HiveIcebergWriterConfig/>

### Hive Configuration Parameters

<HiveIcebergWriterConfigDetails/>

Run the sync command using this `destination.json`.

</TabItem>
</Tabs>

:::tip Connection Testing
OLake automatically validates:

* Hive Metastore connectivity via Thrift protocol
* Object storage access (S3/MinIO/GCS)
* Database creation & access permissions
:::

:::info Path Configuration
Use `iceberg_s3_path` with `s3a://` prefix if your Hive is configured for S3A filesystem. This works for most use cases. For GCS, use `gs://` prefix. For standard S3, use `s3://` prefix.
:::

---

#### Local Development Setup

Here's an example `docker-compose.yml` for OLake with **Hive Metastore + MinIO**:

```yml
version: "3.9"

services:
  postgres:
    image: postgres:15
    container_name: hive-postgres
    environment:
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: password
      POSTGRES_DB: metastore
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "hive", "-d", "metastore"]
      interval: 2s
      timeout: 10s
      retries: 5
      start_period: 10s
    volumes:
      - ./data/postgres-data:/var/lib/postgresql/data
    networks:
      - iceberg_net

  hive-metastore:
    image: apache/hive:4.0.0
    container_name: hive-metastore
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      - SERVICE_NAME=metastore
      - DB_DRIVER=postgres
      - SERVICE_OPTS=-Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://postgres:5432/metastore -Djavax.jdo.option.ConnectionUserName=hive -Djavax.jdo.option.ConnectionPassword=password
    ports:
      - "9083:9083"
    networks:
      - iceberg_net
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "9083"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  minio:
    image: minio/minio:RELEASE.2025-04-03T14-56-28Z
    container_name: minio
    environment:
      - MINIO_ROOT_USER=admin
      - MINIO_ROOT_PASSWORD=password
      - MINIO_DOMAIN=minio
    networks:
      iceberg_net:
        aliases:
          - warehouse.minio
    ports:
      - 9001:9001
      - 9000:9000
    volumes:
      - ./data/minio-data:/data
    command: [ "server", "/data", "--console-address", ":9001" ]

  mc:
    depends_on:
      - minio
    image: minio/mc:RELEASE.2025-04-03T17-07-56Z
    container_name: mc
    networks:
      iceberg_net:
    environment:
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=password
      - AWS_REGION=us-east-1
    entrypoint: |
      /bin/sh -c "
      until (/usr/bin/mc config host add minio http://minio:9000 admin password) do echo '...waiting...' && sleep 1; done;
      if ! /usr/bin/mc ls minio/warehouse > /dev/null 2>&1; then
        /usr/bin/mc mb minio/warehouse;
        /usr/bin/mc policy set public minio/warehouse;
      fi;
      tail -f /dev/null
      "

networks:
  iceberg_net:
    name: iceberg_net

volumes:
  postgres-data:
  minio-data:
```

---

## Using GCP Dataproc Metastore (Hive) with Google Cloud Storage (GCS)

OLake supports using Google Cloud Dataproc Metastore (Hive) as the Iceberg catalog and Google Cloud Storage (GCS) as the data lake destination. This allows you to leverage GCP-native services for scalable, managed metadata and storage.

**Dataproc Metastore**
![dataproc-metastore](/img/docs/iceberg/hive-gcp-dataproc.png)

#### Step-by-Step Setup

1. **Create a GCP Project** (if you don't have one).
2. **Provision a Dataproc Metastore (Hive):**
   - Go to the GCP Console → Dataproc → Metastore services.
   - Click "Create Metastore Service".
   - Fill in service name, location, version, release channel, port (default: 9083), and service tier.
   - Set the endpoint protocol to **Thrift**.
   - Expose the service to your running network (VPC/subnet).
   - Enable the Data Catalog sync option if desired.
   - Choose database type and other options as needed.
   - Click Submit. Creation may take 20–30 minutes.
3. **Expose the Metastore endpoint** to the network where OLake will run (ensure network connectivity and firewall rules allow access to the Thrift port).
4. **Create or choose a GCS bucket** for Iceberg data.
5. **Deploy OLake** in the same network (or with access to the Metastore endpoint).

#### GCP-Hive OLake Destination Config

```json title="destination.json (GCP Hive + GCS)"
{
  "type": "ICEBERG",
  "writer": {
    "catalog_type": "hive",
    "hive_uri": "thrift://<METASTORE_IP>:9083",
    "hive_clients": 10,
    "hive_sasl_enabled": false,
    "iceberg_db": "olake_iceberg",
    "iceberg_s3_path": "gs://<hive-dataproc-generated-bucket>/hive-warehouse",
    "aws_region": "us-central1"
  }
}
```

- **Replace `<METASTORE_IP>`** with your Dataproc Metastore's internal IP or hostname.
- Replace `<hive-dataproc-generated-bucket>` with the Dataproc Metastore generated `hive.metastore.warehouse.dir` bucket.
- **Set `aws_region`** to your GCP region (e.g., `us-central1`).

#### Notes

- The `hive_uri` must use the Thrift protocol and point to your Dataproc Metastore endpoint.
- The `iceberg_s3_path` can use the `gs://` prefix for GCS buckets.
- Ensure OLake has network access to the Metastore and permissions to write to the GCS bucket.
- Data written will be in Iceberg format, queryable via compatible engines (e.g., Spark, Trino) configured with the same Hive Metastore and GCS bucket.

---

## Troubleshooting

The OLake Hive Catalog connector stops immediately upon encountering errors to ensure data accuracy. Below are common issues and their fixes:

- <span style={{ color: '#1a73e8' }}>Connection Refused to Hive Metastore</span>
  - Cause: Hive Metastore not accessible or network connectivity issues.
  - Fix:
    - Verify Hive Metastore is running and accessible:
      ```bash
      telnet <hive_host> 9083
      ```
    - Check `hive_uri` format and port configuration.
    - Use `host.docker.internal` instead of `localhost` when running in Docker.
    - Ensure firewall rules allow access to Thrift port (9083).

- <span style={{ color: '#1a73e8' }}>Thrift Protocol Error</span>
  - Cause: Version mismatch or protocol configuration issues.
  - Fix:
    - Ensure Hive Metastore supports the Thrift protocol version used by OLake.
    - Verify `hive_uri` uses correct `thrift://` protocol prefix.
    - Check if SASL authentication is required and set `hive_sasl_enabled: true`.

- <span style={{ color: '#1a73e8' }}>MetaException: Database does not exist</span>
  - Cause: Specified database in `iceberg_db` doesn't exist in Hive Metastore.
  - Fix:
    - Create the database in Hive Metastore:
      ```sql
      CREATE DATABASE IF NOT EXISTS <iceberg_db>;
      ```
    - Ensure OLake has permissions to create databases.
    - Verify database name matches exactly (case-sensitive).

- <span style={{ color: '#1a73e8' }}>SSL Connection Error</span>
  - Cause: SSL/TLS configuration mismatch between client and storage.
  - Fix:
    - For S3: Ensure `s3_use_ssl` matches endpoint protocol (http/https).
    - For GCS: Use `gs://` prefix and ensure proper authentication.
    - For MinIO: Set `s3_use_ssl: false` for HTTP endpoints.

- <span style={{ color: '#1a73e8' }}>Access Denied or Forbidden Storage Error</span>
  - Cause: Invalid storage credentials or insufficient bucket permissions.
  - Fix:
    - Verify `aws_access_key` and `aws_secret_key` are correct.
    - Ensure storage bucket exists and is accessible.
    - Check bucket permissions allow read/write operations:
      ```bash
      mc policy set readwrite <alias>/<bucket_name>
      ```

- <span style={{ color: '#1a73e8' }}>NoSuchBucket Error</span>
  - Cause: Storage bucket specified in configuration doesn't exist.
  - Fix:
    - Create the bucket specified in `iceberg_s3_path`:
      ```bash
      mc mb <alias>/<bucket_name>
      ```
    - Verify bucket name matches exactly (case-sensitive).
    - Ensure bucket is in the correct region.

- <span style={{ color: '#1a73e8' }}>Table Already Exists Error</span>
  - Cause: Iceberg table conflicts in the Hive Metastore.
  - Fix:
    - Drop existing tables if safe to do so.
    - Use different `iceberg_db` name in configuration.
    - Clear existing table metadata manually if needed.

- <span style={{ color: '#1a73e8' }}>Path Style Access Error</span>
  - Cause: Storage addressing configuration issue with MinIO or non-AWS S3.
  - Fix:
    - Set `s3_path_style: true` for MinIO and non-AWS S3 services.
    - Use correct endpoint format without bucket name in the URL.
    - For GCS, ensure proper `gs://` prefix usage.

- <span style={{ color: '#1a73e8' }}>SASL Authentication Failed</span>
  - Cause: SASL configuration mismatch or authentication issues.
  - Fix:
    - Verify if Hive Metastore requires SASL authentication.
    - Set `hive_sasl_enabled: true` if SASL is required.
    - Ensure proper Kerberos configuration if using Kerberos SASL.

:::info Local Testing
If you wish to test out the Hive Catalog locally, you can use the [docker-compose](../docker-compose) setup. The local test setup uses MinIO as an S3-compatible storage and supports all [catalog types](../docker-compose#local-catalog--test-setup).

You can then setup local Spark to run queries on the Iceberg tables created in the local test setup.
:::
