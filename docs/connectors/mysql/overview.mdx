---
title: MySQL
description: OLake MySQL Connector docs
sidebar_position: 1
---
# MySQL


The OLake MySQL Source connector supports multiple synchronization modes: **Full Load**, **Incremental**, and **Change Data Capture (CDC)** using MySQL binlogs. It offers features like parallel chunking, checkpointing, and automatic resume for failed full loads. This connector can be used within the OLake UI or run locally via Docker for open-source workflows.

## Prerequisites

To use this connector, you'll need a MySQL database setup with the following.

- **MySQL Version**
  - MySQL 5.5+ (5.7+ recommended) for stable CDC and full binary logging support.
- **Binary Logging Setup**
  - Enable binary logging: `log-bin=mysql-bin`
  - Set row-based format: `binlog-format=ROW` 
  - Capture full row data: `binlog-row-image=FULL` 
  - Ensure enough disk space for binary logs.
- **Access & Permissions**
  - Superuser/root privileges to modify `my.cnf`.
  - Set a unique `server-id` for replication.
  - Create a CDC user with `REPLICATION SLAVE`, `REPLICATION CLIENT`, and `SELECT` privileges.

For detailed setup for different MySQL environments, see the guides in the section below.

- [Aurora MySQL – CDC Setup Guide](#)
- [Amazon RDS MySQL – CDC Setup Guide](#)
- [MySQL via Docker Compose – Local Setup](#)
- [Azure Database for MySQL – CDC Setup Guide](#)
- [Google Cloud SQL for MySQL – CDC Setup Guide](#)
- [Generic MySQL – CDC Setup Guide](#)

## Source Configuration

<Tabs>
  <TabItem value="ui" label="Olake UI">

    To create a MySQL source in OLake UI, first set up the OLake UI (see [OLake UI setup](../../getting-started/olake-ui)), then go to `Sources` → `+ Create Source` → select `MySQL`, enter your connection details, and click `Create`.

    :::info
    You can use it to configure your MySQL source, discover streams, and sync data. See [OLake UI setup](../../getting-started/olake-ui) for running via Docker Compose or locally.

    ![olake-source-mysql](/img/docs/sources/olake-source-mysql.png)
    :::

    #### Create a MySQL Source in OLake UI
    Follow these steps (assuming OLake UI is running locally on [localhost:8000](localhost:8000)):
    1. Navigate to the Sources tab.
    2. Click `+ Create Source`.
    3. Select `MySQL` as the source type (Connector type).
    4. Fill in the required connection details. Refer to the field reference below.
    5. Click `Create ->`.
    6. OLake tests the connection and shows success or actionable error messages.

    - How to create a destination in OLake: see [OLake Destinations](../../destinations)
    - How to create a job in OLake UI: see [OLake UI guide](../../getting-started/olake-ui#jobs)


    Configure the following properties in the UI:

    | Field                | Description                                                                 | Data Type | Required/Default |
    |----------------------|-----------------------------------------------------------------------------|-----------|------------------|
    | **hosts**            | List of database host addresses to connect to.                              | string    | Required         |
    | **username**         | Username for authenticating with the database.                              | string    | Required         |
    | **password**         | Password for the database user.                                             | string    | Required         |
    | **database**         | Name of the target database to use.                                        | string    | mysql            |
    | **port**             | Port number on which the database server is listening.                      | integer   | Required         |
    | **initial_wait_time** | Maximum duration in seconds to wait before considering the bin syncer idle. | integer   | 0                |
    | **tls_skip_verify**  | Indicates whether to skip TLS certificate verification.                     | bool      | false            |
    | **max_threads**      | Maximum number of parallel threads for processing or syncing data.          | integer   | 3                |
    | **backoff_retry_count** | Number of retry attempts for establishing sync with exponential backoff. | integer   | 3                |

  :::info MySQL to Iceberg Data Type Mapping
  When syncing data from MySQL to Iceberg, OLake converts source types to compatible Iceberg types. The table below summarizes the mapping used during writes:

  | MySQL Data Types | Iceberg Data Type |
  |---|---|
  | int, int unsigned, mediumint, mediumint unsigned, smallint, smallint unsigned, tinyint, tinyint unsigned | int |
  | bigint, bigint unsigned | bigint |
  | float, decimal(10,2) | float |
  | double, double precision, real | double |
  | datetime, timestamp | timestamp |
  | char, varchar, text, tinytext, mediumtext, longtext, enum, json, bit(1), time | string |
  :::
  </TabItem>

  <TabItem value="cli" label="CLI">

    ### Configuring via CLI

    To configure the MySQL source locally, edit the `source.json` file. Below is an example configuration for connecting to a MySQL replica set:

    ```json
    {
        "hosts": "localhost",
        "username": "root",
        "password": "password",
        "database": "main",
        "port": 3306,
        "tls_skip_verify": true,
        "update_method": {
          "initial_wait_time": 10
        },
        "max_threads": 5,
        "backoff_retry_count": 4
    }
    ```

    ### Configuration Properties

    | Field                | Description                                                                 | Data Type | Required/Default |
    |----------------------|-----------------------------------------------------------------------------|-----------|------------------|
    | **hosts**            | List of database host addresses to connect to.                              | string    | Required         |
    | **username**         | Username for authenticating with the database.                              | string    | Required         |
    | **password**         | Password for the database user.                                             | string    | Required         |
    | **database**         | Name of the target database to use.                                        | string    | mysql            |
    | **port**             | Port number on which the database server is listening.                      | integer   | Required         |
    | **initial_wait_time** | Maximum duration in seconds to wait before considering the bin syncer idle. | integer   | 0                |
    | **tls_skip_verify**  | Indicates whether to skip TLS certificate verification.                     | bool      | false            |
    | **max_threads**      | Maximum number of parallel threads for processing or syncing data.          | integer   | 3                |
    | **backoff_retry_count** | Number of retry attempts for establishing sync with exponential backoff. | integer   | 3                |

      :::info MySQL to Iceberg Data Type Mapping
  When syncing data from MySQL to Iceberg, OLake converts source types to compatible Iceberg types. The table below summarizes the mapping used during writes:

  | MySQL Data Types | Iceberg Data Type |
  |---|---|
  | int, int unsigned, mediumint, mediumint unsigned, smallint, smallint unsigned, tinyint, tinyint unsigned | int |
  | bigint, bigint unsigned | bigint |
  | float, decimal(10,2) | float |
  | double, double precision, real | double |
  | datetime, timestamp | timestamp |
  | char, varchar, text, tinytext, mediumtext, longtext, enum, json, bit(1), time | string |
  :::

### Run the OLake discover command

:::tip Discover streams and create streams.json
Use this to scan your MySQL source and generate a <code>streams.json</code> catalog next to your <code>source.json</code>. You can then review and modify <code>streams.json</code> to select streams and set sync options before running a sync.

```bash
docker run --pull=always \
  -v /Users/{PATH_TO_OLAKE_DIRECTORY}/OLAKE_DIRECTORY:/mnt/config \
  source-mysql:latest \
  discover \
  --config /mnt/config/source.json
```

:::

## Streams Configuration

The `streams.json` file is a catalog that describes all available data streams from a source, such as tables in a database. It defines the schema (columns and data types), primary keys, and all supported synchronization methods for each stream. This file allows you to select which specific streams to sync and how to configure their data transfer.

The streams.json file is organized into two main sections:

selected_streams: Lists the streams that have been chosen for processing. These are grouped by namespace.

streams: Contains an array of stream definitions. Each stream holds details about its data schema, supported synchronization modes, primary keys, and other metadata

Below is an example:

    ```json
    {
        "selected_streams": {
            "production_db": [
                {
                    "partition_regex": "",
                    "stream_name": "order_items",
                    "normalization": true
                }
            ]
        },
        "streams": [
            {
                "stream": {
                    "name": "order_items",
                    "namespace": "production_db",
                    "type_schema": {
                        "properties": {
                            "_cdc_timestamp": { "type": ["timestamp_micro", "null"] },
                            "_olake_id": { "type": ["string", "null"] },
                            "_olake_timestamp": { "type": ["timestamp_micro", "null"] },
                            "_op_type": { "type": ["string", "null"] },
                            "order_item_id": { "type": ["integer_small"] },
                            "order_id": { "type": ["integer_small"] },
                            "product_id": { "type": ["integer_small"] },
                            "quantity": { "type": ["number_small", "null"] },
                            "order_date": { "type": ["null", "timestamp"] },
                            "item_status": { "type": ["string", "null"] }
                        }
                    },
                    "supported_sync_modes": ["full_refresh", "incremental", "cdc", "strict_cdc"],
                    "source_defined_primary_key": ["order_item_id"],
                    "available_cursor_fields": ["quantity", "order_date", "order_item_id", "order_id", "product_id", "item_status"],
                    "sync_mode": "cdc"
                }
            }
        ]
    }
    ```

    ### Streams Properties

    | Property                              | Title                  | Description                                                                 | Type              | Required/Default          |
    |---------------------------------------|------------------------|-----------------------------------------------------------------------------|-------------------|---------------------------|
    | **selected_streams**                  | Selected Streams       | Lists streams chosen for syncing, grouped by namespace/database.            | Object            | Required for sync          |
    | **selected_streams.&lt;namespace&gt;[].stream_name** | Stream Name           | Name of the stream to sync (e.g., order_items).                            | String            | Required                  |
    | **selected_streams.&lt;namespace&gt;[].normalization** | Normalization       | If true, applies basic normalization to raw data.                          | Boolean           | Required. Default: true    |
    | **selected_streams.&lt;namespace&gt;[].append_mode** | Append Mode         | If true, appends new data; if false, uses merge/upsert.                    | Boolean           | Optional. Default: false   |
    | **selected_streams.&lt;namespace&gt;[].partition_regex** | Partition Regex   | Regex to filter table partitions.                                         | String            | Required. Default: ""      |
    | **selected_streams.&lt;namespace&gt;[].filter** | Filter               | SQL-like WHERE clause to sync a subset of data.                           | String            | Optional                  |
    | **selected_streams.&lt;namespace&gt;[].chunk_column** | Chunk Column       | Column for splitting large tables into chunks for parallel processing.     | String            | Optional                  |
    | **streams**                           | Available Streams      | Array of all available streams from the source.                            | Array of Objects  | Required                  |
    | **streams[].stream.name**             | Stream Name            | Name of the stream (e.g., table name).                                     | String            | Required                  |
    | **streams[].stream.namespace**        | Namespace              | Database or schema name for the stream.                                    | String            | Required                  |
    | **streams[].stream.type_schema**      | Type Schema            | Defines the structure and data types of the stream's fields.               | Object            | Required                  |
    | **streams[].stream.supported_sync_modes** | Supported Sync Modes | Lists supported sync modes (e.g., full_refresh, incremental, cdc).         | Array of Strings  | Required                  |
    | **streams[].stream.source_defined_primary_key** | Source-Defined Primary Key | Fields forming the primary key.                                 | Array of Strings  | Required                  |
    | **streams[].stream.available_cursor_fields** | Available Cursor Fields |Fields available as cursors for incremental syncs (supports one or two fields, with fallback to a secondary cursor).                     | Array of Strings  | Required                  |
    | **streams[].stream.sync_mode**        | Selected Sync Mode     | Chosen sync mode for the stream.                                          | String            | Required for sync          |
    | **streams[].stream.cursor_field**      | Selected Cursor Field  | Field used as cursor for incremental sync.                                | String            | Required for incremental sync |

### Run OLake sync command

:::tip Run OLake sync
After configuring your `source.json` and selecting streams in `streams.json`, configure a destination writer (`writer.json`). See <a href="../../destinations" style={{ color: '#1a73e8' }}>OLake Destinations</a> for supported destinations and examples.

By default, this command will perform a full load for streams set to `full_refresh`. If a stream is configured for `incremental` or `cdc`, it will use that mode from the next sync. This command will also create a `state.json` file in the same directory as your provided config (e.g., alongside `source.json`).

```bash
docker run --pull=always \
  -v /Users/{PATH_TO_OLAKE_DIRECTORY}/OLAKE_DIRECTORY:/mnt/config \
  source-mysql:latest \
  sync \
  --config /mnt/config/source.json \
  --streams /mnt/config/streams.json \
  --destination /mnt/config/writer.json \
```

:::


## State Configuration
the state.json file is essential for olake's sync process. it tracks the last processed point, ensuring seamless resume without duplicates. it also supports resumable full loads, allowing sync to restart from any failure point.

Below is an example:



    ```json
    {
        "type": "STREAM",
        "global": {
            "state": {
                "server_id": 6000,
                "state": {
                    "position": {
                        "Name": "mysql-bin.000065",
                        "Pos": 157
                    }
                }
            },
            "streams": []
        },
        "streams": [
            {
                "stream": "products",
                "namespace": "production_db",
                "sync_mode": "",
                "state": {
                    "chunks": []
                }
            },
            {
                "stream": "events_cdc",
                "namespace": "production_db",
                "sync_mode": "",
                "state": {
                    "chunks": [
                        {
                            "min": "1",
                            "max": null
                        }
                    ]
                }
            }
        ]
    }
    ```

    ### State Properties

    | Property                        | Title                        | Description                                                                 | Type              | Sample Value           |
    |---------------------------------|------------------------------|-----------------------------------------------------------------------------|-------------------|------------------------|
    | **type**                        | State Type                   | Defines the state management strategy (e.g., STREAM).                       | String            | "STREAM"               |
    | **streams**                     | Stream States                | Array of objects representing state for each stream.                        | Array of Objects  | `[{ ... }]`              |
    | **streams[].stream**            | Stream Name                  | Name of the stream (e.g., table name).                                     | String            | "sample_table"         |
    | **streams[].namespace**         | Namespace                    | Database or schema the stream belongs to.                                  | String            | "main"                 |
    | **streams[].sync_mode**         | Sync Mode                    | Synchronization mode used (e.g., incremental, cdc).                         | String            | "cdc"                  |
    | **streams[].state**             | Stream-Specific State        | Driver-specific object with cursor information for resuming sync.           | Object            | `{...}`                  |
    | **streams[].state.binlog_file** | (MySQL CDC) Binary Log File  | Name of the MySQL binary log file to resume replication.                   | String            | "mysql-bin.000003"     |
    | **streams[].state.binlog_position** | (MySQL CDC) Binary Log Position | Byte position in the binlog file to resume reading.                   | Integer           | 1027                   |
    | **streams[].state.server_id**   | (MySQL CDC) Server ID        | Unique ID of the MySQL server for replication integrity.                   | Integer           | 1000                   |
    | **streams[].state.chunks**      | (MySQL CDC) Chunking State   | Array managing state of large backfills or snapshots.                      | Array             | []                     |

### Run Olake Sync with State command

:::tip Run OLake sync with state
When a <code>state.json</code> is provided, OLake resumes from the stored cursor for each stream and continues according to the selected sync mode: <code>cdc</code> (binlog position) or <code>incremental</code> (cursor field). This ensures seamless, duplicate-free continuation from the last successful point.

```bash
docker run --pull=always \
  -v /Users/{PATH_TO_OLAKE_DIRECTORY}/OLAKE_DIRECTORY:/mnt/config \
  source-mysql:latest \
  sync \
  --config /mnt/config/source.json \
  --streams /mnt/config/streams.json \
  --destination /mnt/config/writer.json \
  --state /mnt/config/state.json
```

:::
  </TabItem>
</Tabs>

## Troubleshooting

The OLake MySQL Source connector stops immediately upon encountering errors to ensure data accuracy. Below are common issues and their fixes:
 
- <span style={{ color: '#1a73e8' }}>Failed to Get Current Binlog Position</span>
  - Cause: Binary logging not enabled, wrong format, or insufficient privileges.
  - Fix:
    - Ensure binary logging is enabled:
      ```sql
      SHOW VARIABLES LIKE 'log_bin';
      ```
      Should return `ON`.
    - Ensure row-based logging:
      ```sql
      SHOW VARIABLES LIKE 'binlog_format';
      ```
      Should return `ROW`.
    - Grant required privileges:
      ```sql
      GRANT REPLICATION CLIENT, REPLICATION SLAVE ON *.* TO 'your_user'@'%';
      ```

- <span style={{ color: '#1a73e8' }}>Idle Timeout Reached, Exiting Bin Syncer</span>
  - Cause: No new changes within the configured `initial_wait_time`.
  - Fix: Increase `initial_wait_time` in the connector configuration or verify data changes in the source database.

- <span style={{ color: '#1a73e8' }}>Column Count Mismatch: Expected X, Got Y</span>
  - Cause: Table schema changed after CDC started or incomplete binlog metadata.
  - Fix:
    - Ensure full binlog metadata:
      ```sql
      SHOW VARIABLES LIKE 'binlog_metadata';
      ```
      Should return `FULL`. To set it:
      ```sql
      SET GLOBAL binlog_metadata = 'FULL';
      ```
      Update `my.cnf` or `my.ini` for persistence.
    - Restart the connector after schema changes.

- <span style={{ color: '#1a73e8' }}>Failed to Get or Split Chunks</span>
  - Cause: Table stats not populated or table contains 0 records.
  - Fix: Run the following query to populate table statistics:
    ```sql
    ANALYZE TABLE benchmark.fhv_trips_new_no_pk;
    ```