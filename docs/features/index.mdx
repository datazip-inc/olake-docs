---
title: Features
description: OLake features overview
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

<Tabs groupId="features" queryString="tab">

<TabItem value="overview" label="Overview" default>

# OLake Features Overview

OLake is an open-source, high-performance data pipeline designed to extract data from Databses (like MongoDB, MySQL, Postgres) and load it into modern data lakehouse formats like Apache Iceberg and Parquet. 

Its modular architecture is built around fast, parallelized data ingestion, real-time change data capture (CDC), and native support for schema evolution and S3 data partitioning. 

OLake minimizes vendor lock-in while offering extensive configurability—from full snapshot loads to incremental syncs and stateful resumption—making it an ideal solution for modern data engineering challenges.

<OLakeFeaturesTLDR/>

## 1. Connector Features

OLake’s connector includes several  features that ensure efficient and reliable data replication:

### Native BSON Extraction [for mongodb]
- **What it does:** Extracts data in its raw `BSON` form directly from MongoDB.
- **Benefit:** Maintains data fidelity while boosting speed by decoding on the ETL side.

### Parallel Chunking
- **What it does:** Splits large collections into smaller virtual chunks.
- **Benefit:** Enables parallel reads, significantly reducing the time required for full collection snapshots.

### CDC Cursor Preservation
- **What it does:** Keeps track of CDC (Change Data Capture) offsets throughout the data ingestion process.
- **Benefit:** Supports the addition of new large collections without interrupting ongoing incremental syncs.

### Open Format for Freedom
- **What it does:** Uses open formats like Parquet and table formats like Apache Iceberg.
- **Benefit:** Sidesteps vendor lock-in and allows multi-engine querying.

### Resumable Full Load & Incremental Sync
- Both full snapshots and incremental (CDC) syncs are designed to be resumable. If a sync is interrupted, OLake uses state files to resume progress seamlessly.

### S3 Data Partition
- **What it does:** Supports partitioning data when writing to S3.
- **Benefit:** Improves query performance by organizing data into directories based on user-defined partitioning logic.

### Schema Evolution
- **What it does:** Detects changes in source document structures and handles non-breaking changes.
- **Benefit:** Minimizes disruptions when the underlying schema evolves; separate Parquet files are created if schema evolution is detected (currently in S3).

### Normalization vs. Denormalization
- **What it does:** Offers an option to choose between normalization and level-1 denormalization (flattening).
- **Benefit:** Gives flexibility in data transformation depending on downstream requirements.

### Supported Data Types
OLake supports a variety of data types. The table below summarizes the supported types along with descriptions and sample values:

| Data Type       | Description                                               | Sample Value                                   |
|-----------------|-----------------------------------------------------------|------------------------------------------------|
| **Null**        | Represents a null or missing value                        | `null`                                         |
| **Int64**       | 64-bit integer value                                      | `42`                                           |
| **Float64**     | 64-bit floating point number                              | `3.14159`                                      |
| **String**      | Text string                                               | `"Hello, world!"`                              |
| **Bool**        | Boolean value                                             | `true`                                         |
| **Object**      | JSON object (key-value pairs)                             | `{ "name": "Alice", "age": 30 }`               |
| **Array**       | Array of values                                           | `[1, 2, 3]`                                    |
| **Unknown**     | Unspecified or unrecognized type                          | `"unknown"`                                    |
| **Timestamp**   | Timestamp with default precision                          | `"2025-02-18T10:00:00Z"`                        |
| **TimestampMilli** | Timestamp with millisecond precision (3 decimal places) | `"2025-02-18T10:00:00.123Z"`                     |
| **TimestampMicro** | Timestamp with microsecond precision (6 decimal places) | `"2025-02-18T10:00:00.123456Z"`                  |
| **TimestampNano**  | Timestamp with nanosecond precision (9 decimal places)  | `"2025-02-18T10:00:00.123456789Z"`               |

### Synchronization Modes
The streams file defines how streams are synchronized:
- **`supported_sync_modes`:** Lists the supported modes (e.g., `full_refresh`, `cdc`).
- **`sync_mode`:** Indicates the active sync mode for a stream.

### Backoff Retry Count
- **What it does:** Implements an exponential backoff (1, 2, 4, 8, 16 minutes…) for retrying sync operations.
- **Default:** Set to 3 (or takes the default value if set to -1).

### Sync with State of Last Synced Cursor
- **What it does:** Saves progress in a state file (e.g., `state.json`) so that an interrupted sync can resume from the last known checkpoint.


## 2. Data Flow in OLake

OLake’s data flow is designed for high throughput and resiliency:

### Initial Snapshot
- **Process:** Executes a full collection read by firing queries.
- **Parallelism:** Divides the collection into chunks that are processed in parallel.
- **Insight:** Provides an estimated remaining time along with real-time stats (e.g., memory usage, running threads, records per second) in `stats.json` file created automatically at the place when your configs (`source.json` and `destination.json` are being added).

```json
{
    "Estimated Remaining Time": "340.85 s",
    "Memory": "92329 mb",
    "Running Threads": 10,
    "Seconds Elapsed": "148.00",
    "Speed": "5773.03 rps",
    "Synced Records": 854411
}
```

### Change Data Capture (CDC)
- **Process:** Sets up MongoDB change streams based on oplogs to capture near real-time updates.
- **Resilience:** Ensures that any changes during the snapshot are also captured, and subsequent updates are continuously synced.

### Parallel Processing
- **Configuration:** Users can set the number of parallel threads to balance speed with the load on the MongoDB cluster.

### Transformation & Normalization
- **What it does:** Flattens complex, semi-structured fields into relational streams.
- **Current Support:** Level-0 flattening is available, with more advanced nested JSON support coming soon.

### Integrated Writes
- **What it does:** Pushes transformed data directly to target destinations (e.g., local Parquet files, S3 Iceberg Parquet) without intermediary buffering.
- **Benefit:** Minimizes latency and avoids blocking reads during writes.

### Logs & Testing
- **Logging:** Detailed logs using Zerolog and Lumberjack help monitor the process, we create a logs.txt file that saves all your logs during the sync process. 

## 3. Architecture and Components

OLake is built with a modular design to separate responsibilities across different components:

### Drivers (Connectors)
- **Function:** Embed the logic necessary to interact with the source system (starting with MongoDB, with plans for Kafka, Postgres, MySQL, and DynamoDB).
- **Responsibilities:** 
  - **Full Load:** Efficiently fetch complete collections by processing them in parallel.
  - **CDC Management:** Configure change streams to capture and sync incremental changes.

### Destinations
- **Function:** Tightly integrated with drivers to directly push records to destinations.
- **Supported Destinations:** 
  - Local Parquet files
  - AWS S3 
  - Apache Iceberg 

### Core Functionality
- **CLI & Commands:** 
  - `spec` returns a JSON Schema for frontend configuration.
  - `check` validates configuration, streams, state files, and credentials.
  - `discover` lists streams and their schemas.
  - `sync` orchestrates the ETL process.
- **State Management:** Ensures that CDC streams can resume after interruptions.
- **Configuration & Logging:** Manages concurrency levels, connection credentials, and provides real-time monitoring of progress and throughput.

## 4. S3 Data Partitioning

OLake provides robust support for S3 data partitioning, enabling efficient data organization and faster query performance.

### Overview
- **How It Works:** After schema discovery, you can define a `partition_regex` in the `streams.json` file for each stream. This regex dynamically partitions data into folders in your S3 bucket.
- **Resulting Structure:** The generated S3 path typically includes the database name, table/collection name, and partition values (e.g., date components or key-value pairs).

### Sample Configuration
```json
{
  "selected_streams": {
    "namespace": [
      {
        "stream_name": "table1",
        "partition_regex": "/{column_name, default_value, granularity}",
        "normalization": false,
        "append_mode": false
      },
      {
        "stream_name": "table2",
        "partition_regex": "",
        "normalization": false,
        "append_mode": false  
      }
    ]
  },
  "streams": [
    {
      "stream": {
        "name": "table1",
        "namespace": "namespace",
        "sync_mode": "cdc"
      }
    },
    {
      "stream": {
        "name": "table2",
        "namespace": "namespace",
        "sync_mode": "cdc"
      }
    }
  ]
}
```

### Partition Regex Attributes
- **Structure:**  
  `"partition_regex": "/{column_name, default_value, granularity}"`
- **Components:**
  - **column_name:** (Required) Specifies which column’s value determines the partition.
  - **default_value:** (Optional) A fallback if the column is null or unparseable.
  - **granularity:** (Optional) For time-based columns, define granularity (e.g., HH, DD, MM, YYYY).

### Supported Patterns
- **now():** Uses the current local timestamp.
- **default:** Uses a constant fallback value.
- **`{col_name}`:** Inserts the specified column value.
- **Combined patterns:**  
  - `{now(),2025,YYYY}` yields the year (e.g., `2025`).
  - `{now(),2025,YYYY}-{now(),06,MM}-{now(),13,DD}` combines year, month, and day.

### Examples
- **Date-based Partition:**  
  ```json
  "partition_regex": "/{title,,}/{now(),2025,DD}/{latest_version,,}"
  ```
- **Key-Value Style Partitioning:**  
  ```json
  "partition_regex": "/age={age_column, default_age,}/city={city_column, default_city,}/type={type_column, default_type,}"
  ```
  For a record:
  ```json
  { "age": "30", "city": "mumbai", "type": "existing" }
  ```
  The resulting S3 path would be:  
  `s3://your_bucket/age=30/location=city-mumbai/type=existing/<filename>.parquet`

- **Composite Partitions Including Date:**  
  ```json
  "partition_regex": "/dt={now(),2025,YYYY}-{now(),06,MM}-{now(),13,DD}/country={country,default_country,}/city={city,default_city,}"
  ```
  This produces a path such as:  
  `s3://your_bucket/dt=2025-02-18/country=USA/city=NewYork/<filename>.parquet`

### Supported Timestamp Formats
OLake supports a wide range of timestamp formats, including:
- `"2006-01-02"`
- `"2006-01-02 15:04:05"`
- `"2006-01-02T15:04:05"`
- `"2020-08-17T05:50:22.895Z"`
- _and several others_

For custom timestamp columns, replace `now()` with the column name and specify the desired format extraction.

### Test Image Releaser
- **What it does:** Adds support for building test images via a “test_mode” option in the GitHub workflow.
- **Benefits and changes:**
  - **Test Mode Active:** Tags are prefixed with `testing-` (e.g., `testing-dev-123`).
  - **Validation:** Bypasses semantic version validation for test images, while production images must follow strict versioning.
  - **Output Examples:**  
    ```
    Test mode: true
    ✅ Test mode active - skipping semantic version validation for version: dev-123
    **** Releasing olakego/source-mongodb for platforms [linux/amd64,linux/arm64] with version [testing-dev-123] ****
    Release successful for olakego/source-mongodb version testing-dev-123
    ```

</TabItem>

<TabItem value="schema" label="Schema evolution">

# Schema Evolution and Data Type Changes

This document explains how OLake handles schema changes and data type changes in your data pipelines. It covers two distinct features that help maintain pipeline resilience when your source data structures evolve.

## Schema Evolution

Schema evolution refers to changes in your database structure like adding, removing, or renaming columns and tables. OLake handles these changes to prevent pipeline failures and data loss.

### What OLake Handles

### 1.  Schema Evolution — Column-Level Changes

| Change Type | How OLake Detects & Handles It | Typical Pipeline Impact | Extra Details & Tips |
| ----------- | ------------------------------ | ----------------------- | -------------------- |
| **Adding a column**                                  | OLake runs a *schema discovery* at the start of every sync. When a new source column appears, it is **automatically** added to the Iceberg schema (new field-ID) and starts receiving values immediately. If the source back-fills historical rows, CDC registers them as updates. **No user action is required.** | **No breakage.** Historical rows show `NULL` until back-filled. | • Monitor write throughput if a back-fill is large. |
| **Deleting (dropping) a column**                     | OLake notices the column is removed in the source streams.<br/>• The deleted column still exists in the destination (so old snapshots stay queryable).<br/>  | **No breakage**. ETL continues with a “virtual” column (null-filled).                                                                               | • Down-stream BI tools won’t break, but they might show the column full of nulls—communicate schema changes to analysts.<br/>• You can later run a “rewrite manifests” job to strip the dead column if storage footprint matters. |
| **Renaming a column**                                |  Source column renamed → old column stays in destination (no new values) → new column with updated name is created and receives all incoming data.    <br/> <br/> WIP: Iceberg keeps immutable field IDs, so on rename (customer_id → client_id) OLake just updates the column’s name on the same field ID—no data migration required.             | **No breakage**. | • Renames are instant—no file rewrites.<br/>• If you have SQL downstream, update column names in your SQL queries to use the new column name.                                                                                                           |
| **JSON / Semi-structured key add / remove / rename** | OLake flattens keys to a canonical path inside a single JSON column (or keeps raw JSON).<br/>• Added keys appear automatically.<br/>• Removed keys simply vanish from new rows.<br/>• Renamed keys are treated as “remove + add” because JSON has no intrinsic field ID.                                                                        | **No breakage**.                       |          |

:::info
-  A sparse new column (will not be synced to destination unless there is atleast 1 non `NULL` value). Because Iceberg stores data column-wise (Parquet).
:::


### 2  Schema Evolution — Table-Level Changes

| Change Type | How OLake Detects & Handles It | Typical Pipeline Impact | Extra Details & Tips|
| ----------- | ------------------------------ | ----------------------- | ------------------- |
| **Adding a table / stream**                                                  | Newly detected source tables appear in the OLake UI list. **You choose which ones to sync.** Once added, OLake performs an initial full load and then switches to CDC. Tables not selected to sync are ignored. | **No breakage.** Pipelines for existing tables run as usual; disabled tables simply do not sync. | • Initial full loads run in parallel.<br/>• Default naming is `source_db.table_name`. |
| **Removing (dropping) a table / stream**       |  No new data will get added to the deleted table. Existing table data and metadata remain queryable.                                                                                                                                                                                                                                              | **No breakage.** Downstream queries on historic data still work; new inserts stop.                                    | • If the table is recreated later with the same name but different structure, treat it as a brand-new stream to avoid field-ID collisions.                                                               |
| **Renaming a table**                                                         | Renaming of a table is treated as a new table itself and will be discovered as a new stream and on enbaling sync for this table it will be synced as full load + CDC.<br/> <br/>• The old Iceberg table keeps historical data.                                         | **No breakage**, but post-rename data lands in a separate table unless you merge histories.      | For continuous history, enable the new table quickly and (optionally) set an alias so both names map to the same Iceberg table.     |


## Schema Data Type Changes

Schema data type changes refer to modifications to the data type of existing columns (e.g., changing `INT` to `BIGINT`). OLake leverages `Apache Iceberg v2` tables' type promotion capabilities to handle compatible changes automatically.

### Supported Data Type Promotions

OLake fully supports all Iceberg v2 data type promotions:

| From          | To                          | Notes                                                 |
| ------------- | --------------------------- | ----------------------------------------------------- |
| INT           | LONG (BIGINT)               | Widening integers is safe                             |
| FLOAT         | DOUBLE                      | Promoting to higher precision works without data loss |
| DATE          | TIMESTAMP, TIMESTAMP_NS     | Dates can be safely converted to timestamps           |
| DECIMAL(P, S) | DECIMAL(P', S) where P' > P | Only widening precision is supported                  |

:::caution
- Iceberg v2 supports widening type changes only. Narrowing changes (e.g., `BIGINT` to `INT`) along with any other data type changes will result in an errror as are not supported.
- All the incompatible type changes will be handled by OLake with DLQ (Dead Letter Queue) tables (coming soon)
:::

### Handling Incompatible Type Changes

For type changes not supported by Iceberg v2 (like STRING to INT), OLake offers two options:

1. **Schema Data Type Changes Enabled with DLQ** ([coming soon](https://github.com/datazip-inc/olake/issues/265)):

   - Records with incompatible types will be routed to a Dead Letter Queue Table (DLQ)
   - Main pipeline continues processing compatible records
   - Full record information preserved for troubleshooting

2. **Schema Data Type Changes Enabled without DLQ**:

   - Sync fails with clear error message about incompatible type change
   - Message identifies the specific column and type conversion that failed

3. **Schema Data Type Changes Disabled**:

   - Any data type change results in sync failure
   - Provides explicit error about the type change detected

### Production Best Practices

- Enable Schema Data Type Changes for production environments
- Implement robust monitoring for type change errors
- Test schema changes in non-production environments first
- Document your schema and track changes over time

## Example Scenarios

### Scenario 1: Adding a Column in Source

When a new column appears in your source data:

- OLake automatically detects the new column
- The column is added to your destination schema
- New data includes values for this column
- Historical data has null values for this column

### Scenario 2: Adding a Table / Collection in Source

When a new table appears in your source database:

- OLake automatically detects the new table in the next scheduled run
- The table is added to your destination schema
- A New table gets created.

### Scenario 3: Table Name Change

When a table name changes in your source database:
- OLake automatically detects the new table name
- The table is added to your destination schema
- A New table gets created.
- The old table name is retained in the destination schema but will not be populated with new data.

### Scenario 4: INT to BIGINT Conversion

When a column changes from INT to BIGINT type:

- OLake detects the widening type change
- Column type is updated in the destination
- All values are properly converted
- Pipeline continues without interruption

### Scenario 5: Incompatible Type Change

When a column changes from STRING to INT type, incompatible with Iceberg v2, sync fails.

For more detailed information on Iceberg's schema evolution capabilities, refer to the [Apache Iceberg documentation](https://iceberg.apache.org/spec/#schema-evolution).

</TabItem>

</Tabs>


