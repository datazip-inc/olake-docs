---
title: AWS DMS vs OLake
description: Objective benchmark comparing AWS DMS and OLake for PostgreSQL to S3 (Parquet/Iceberg) migrations
sidebar_position: 3
slug: /dmsvsolake
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

## OLake vs AWS DMS Benchmark

This document summarizes measured performance, environment, and cost for migrating PostgreSQL data to Parquet Writer (AWS S3) using AWS Database Migration Service (DMS) and OLake. 

## Workload

- **Dataset:** This benchmark uses two large tables from **NYC Taxi Data** `trips2` and `fhv_trips`. For details on how the data is generated, check here: [How the data is generated!](/docs/dmsvsolake#dataset-and-table-schemas)
- **Total rows:** **4,008,587,913 rows** including both tables
- **Modes evaluated:** We are performing **Full Refresh** and **CDC** workload (50 million rows)

## Environment

- **Source database:** Azure PostgreSQL (32 vCores, 128 GB RAM)
- **Destination:** Amazon S3 (Parquet)
- **Migration compute:** c6i.16xlarge (64 vCPU, 128 GB RAM)
- **Reference compute price:** ~$3.07/hour

:::note
The **same dataset and compute** were used for both the tools.
:::

## Full Refresh Results

The full‑load test measures time and throughput to copy the complete dataset from source PostgreSQL to Parquet files in S3, serving as a baseline for bulk data movement under sustained load.

| Tool   | Rows Processed | Total Time | Avg Throughput (rows/sec) |
| ------ | --------------: | ---------: | -------------------------: |
| OLake  | 4,008,587,913  | 1 h 59 m    | 558,765                  |
| AWS DMS| 4,008,587,913 | 9 h 8 m    | 122,000                   |

**Key observations:**

- **OLake** completed the workload **~4.6 x faster** as compared to **DMS** for Full Refresh.
- OLake does not require any manual partition-boundary scripting, whereas DMS required manual boundary generation to parallelize the load for PostgreSQL sources.

**Parallelism used:** 
- OLake ran with **32 threads**. 
- DMS ran with: **40 parallel tasks**.

## CDC Results (Insert-only; 50M rows)

The CDC test measures sustained ingest of incremental changes from PostgreSQL into Parquet, validating plugin configuration, replication slots, and end‑to‑end apply throughput. We will be ingesting 25 million records for each of the tables `trips2` and `fhv_trips`. 

| Tool   | Rows Processed | Total Time | Avg Throughput (rows/sec) |
| ------ | --------------: | ---------: | -------------------------: |
| AWS DMS| 50,000,000      | 22 m 41 s  | 36,738                    |
| OLake  | 50,000,000      | 16 m 24 s  |  50,812                  |

**Key Observations:**

- **OLake** completed the workload **~1.38 × faster** as compared to **DMS** for CDC.
- OLake does not require any manual setup we simply had to select CDC option and that is it! Whereas DMS required enabling pglogical via Azure extensions, performing a complex pglogical setup, and specifying the starting LSN for replication tasks.

**Parallelism used:** 
- OLake ran with **32 threads**. 
- DMS ran with: **40 parallel tasks**.

## Resource Utilization 

We’re focusing on resource utilization for the Full Refresh process, as it’s significantly more resource-intensive than CDC.

The memory utilization shown corresponds to a transfer of **~4 billion records**. As the data volumes increase, being memory efficiency becomes highly crucial — an area where **OLake** excels.

| Memory Stats | Olake | DMS |
|--------------|-------|-----|
| Min          | 2.01 GB | 20 GB |
| Max          | 46.72 GB | 40 GB |
| Mean         | 25.70 GB | 30 GB |

**Olake's** memory usage ranged from 2.12 GB to 46.72 GB, **averaging 25.70 GB** throughout the transfer. For **DMS**, the memory usage ranged from 20 GB to 40 GB, **averaging 30 GB** throughout the transfer.

## Cost Comparison (Compute Only)

Compute costs scale linearly with runtime at a given instance class for both OLake and DMS. 

Here is the cost comparison for **Full Refresh** process.

| Scenario              | Instance         | Runtime  | Approx. Cost |
| --------------------- | ---------------- | -------: | -----------: |
| DMS Full Refresh      | c6i.16xlarge     | 9h 08m   | ~$28.03      |
| OLake Full Refresh    | c6i.16xlarge     | 1h 59m   | ~$6.08       |

**Important-** 

**OLake** is delivering **4.61x cost savings** as compared to **DMS** on compute alone. When you're moving terabytes monthly, those savings add up fast!

**Scaling Impact-**

Let us take a closer look at how the costs evolve over time for both the tools. 

Let us assume that we are moving the same dataset everyday **"just once"** for the below mentioned durations.


| Tool | 1 Month Cost | 2 Months Cost | 6 Months Cost |
|------|--------------|---------------|-------------|
| DMS  | ~$840      | ~$1,681       | ~$5045     |
| OLake| ~$182      | ~$364      | ~$1094      |

It quickly becomes evident how the costs unfold over time for both tools, painting a rather unmistakable picture of which one is more cost-efficient.<br/>**And, just a gentle reminder:** You’re rarely content with a single sync per day—so brace yourself, the costs will only climb from here.

:::info[Point to Remember]

OLake is open source; there are no licensing fees. Actual spend depends on user infra and storage.

:::

## Dataset and Table Schemas

The OLake benchmarks page publishes NYC Taxi table shapes that are appropriate for both bulk transfer and CDC scenarios at scale, which can be reused to maintain comparability across tools.

See the dataset and reproducible setup in the GitHub repository: [NYC Taxi Data Benchmark](https://github.com/datazip-inc/nyc-taxi-data-benchmark/tree/remote-postgres). 

The repository includes scripts and guidance to generate the NYC trips tables used in this benchmark.

### `trips2` table

Schema for the `trips2` table used in this benchmark:

```sql
CREATE TABLE public.trips2 (
    id BIGSERIAL NOT NULL,
    cab_type_id INT NULL,
    vendor_id INT NULL,
    pickup_datetime TIMESTAMP NULL,
    dropoff_datetime TIMESTAMP NULL,
    store_and_fwd_flag BOOLEAN NULL,
    rate_code_id INT NULL,
    pickup_longitude NUMERIC NULL,
    pickup_latitude NUMERIC NULL,
    dropoff_longitude NUMERIC NULL,
    dropoff_latitude NUMERIC NULL,
    passenger_count INT NULL,
    trip_distance NUMERIC NULL,
    fare_amount NUMERIC NULL,
    extra NUMERIC NULL,
    mta_tax NUMERIC NULL,
    tip_amount NUMERIC NULL,
    tolls_amount NUMERIC NULL,
    ehail_fee NUMERIC NULL,
    improvement_surcharge NUMERIC NULL,
    congestion_surcharge NUMERIC NULL,
    airport_fee NUMERIC NULL,
    total_amount NUMERIC NULL,
    payment_type INT NULL,
    trip_type INT NULL,
    pickup_nyct2010_gid INT NULL,
    dropoff_nyct2010_gid INT NULL,
    pickup_location_id INT NULL,
    dropoff_location_id INT NULL,
    CONSTRAINT trips2_pkey PRIMARY KEY (id)
);
```

### `fhv_trips` table

Schema for the `fhv_trips` table used in this benchmark:

```sql
CREATE TABLE fhv_trips (
id bigserial NOT NULL,
hvfhs_license_num text NULL,
dispatching_base_num text NULL,
originating_base_num text NULL,
request_datetime timestamp NULL,
on_scene_datetime timestamp NULL,
pickup_datetime timestamp NULL,
dropoff_datetime timestamp NULL,
pickup_location_id int4 NULL,
dropoff_location_id int4 NULL,
trip_miles numeric NULL,
trip_time numeric NULL,
base_passenger_fare numeric NULL,
tolls numeric NULL,
black_car_fund numeric NULL,
sales_tax numeric NULL,
congestion_surcharge numeric NULL,
airport_fee numeric NULL,
tips numeric NULL,
driver_pay numeric NULL,
shared_request bool NULL,
shared_match bool NULL,
access_a_ride bool NULL,
wav_request bool NULL,
wav_match bool NULL,
legacy_shared_ride int4 NULL,
affiliated_base_num text NULL,
CONSTRAINT fhv_trips_pkey PRIMARY KEY (id)
);
```

## Implementation Considerations

- DMS PostgreSQL full loads may require manual task mapping with partition boundaries to achieve higher parallelism.
- DMS CDC for Azure PostgreSQL required enabling `pglogical` via `azure.extensions` and configuring replication sets with aligned start LSN.
- OLake parallelization and schema handling are automatic; no manual boundary generation was required in this benchmark.


















