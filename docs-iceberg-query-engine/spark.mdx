---
title: Apache Spark
description: The reference implementation for Apache Iceberg with comprehensive read/write support
---

import { QueryEngineLayout } from '@site/src/components/Iceberg/QueryEngineLayout';
import { 
  SparkIcon,
//   DatabaseIcon,
  BoltIcon,
  ShieldCheckIcon,
  ClockIcon,
  CodeBracketIcon
} from '@heroicons/react/24/outline';

export const sparkFeatures = [
  {
    title: "Comprehensive Catalog Support",
    chip: "Full Support",
    description: "Native integration with all major catalog types including Hive, Glue, REST, and Nessie",
    icon: <BoltIcon className="w-6 h-6" />,
    color: "blue",
    score: 100,
    details: {
      title: "Catalog Integration in Spark",
      description: "Spark provides the most comprehensive catalog support among all query engines, making it the ideal choice for multi-catalog environments.",
      overviewContent: {
        strengths: [
          "Supports all major catalog implementations",
          "Seamless switching between catalogs",
          "Custom catalog plugin support",
          "Multi-catalog queries in single session"
        ],
        limitations: [
          "Catalog-specific configurations required",
          "Some features vary by catalog type",
          "Cross-catalog transactions not supported"
        ],
        bestFor: [
          "Multi-cloud deployments",
          "Hybrid on-premise/cloud architectures",
          "Organizations with existing Hive infrastructure",
          "Teams requiring catalog flexibility"
        ]
      },
      technicalSpecs: [
        {
          category: "Supported Catalogs",
          items: [
            { label: "Hive Metastore", value: "Full Support", status: "available" },
            { label: "AWS Glue", value: "Full Support", status: "available" },
            { label: "REST/Tabular", value: "Full Support", status: "available" },
            { label: "Nessie", value: "Full Support", status: "available" },
            { label: "JDBC", value: "Full Support", status: "available" },
            { label: "Hadoop", value: "Full Support", status: "available" },
            { label: "Custom Catalogs", value: "Via Plugin API", status: "available" }
          ]
        },
        {
          category: "Catalog Features",
          items: [
            { label: "Multi-catalog Support", value: "Yes", status: "available" },
            { label: "Dynamic Catalog Loading", value: "Yes", status: "available" },
            { label: "Namespace Operations", value: "Full CRUD", status: "available" },
            { label: "Table Locking", value: "Catalog-dependent", status: "available" }
          ]
        }
      ],
      codeExamples: [
        {
          title: "Hive Metastore Configuration",
          language: "properties",
          code: `spark.sql.catalog.my_catalog=org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.my_catalog.type=hive
spark.sql.catalog.my_catalog.uri=thrift://metastore-host:9083
spark.sql.catalog.my_catalog.warehouse=s3://my-bucket/warehouse`,
          description: "Standard Hive Metastore configuration for Spark"
        },
        {
          title: "AWS Glue Catalog",
          language: "properties",
          code: `spark.sql.catalog.glue=org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.glue.catalog-impl=org.apache.iceberg.aws.glue.GlueCatalog
spark.sql.catalog.glue.io-impl=org.apache.iceberg.aws.s3.S3FileIO
spark.sql.catalog.glue.warehouse=s3://my-bucket/warehouse`,
          description: "Configure Spark to use AWS Glue as the catalog"
        }
      ],
      architectureNotes: [
        {
          title: "Catalog Architecture",
          content: "Spark's catalog abstraction allows seamless integration with various metadata stores. The SparkCatalog implementation wraps underlying catalog implementations, providing a unified interface for all Iceberg operations."
        }
      ],
      externalLinks: [
        { label: "Spark Configuration Guide", url: "https://iceberg.apache.org/docs/latest/spark-configuration/", type: "docs" },
        { label: "Catalog Properties Reference", url: "https://iceberg.apache.org/docs/latest/spark-configuration/#catalogs", type: "docs" },
        { label: "Multi-Catalog Tutorial", url: "https://iceberg.apache.org/docs/latest/spark-procedures/", type: "tutorial" }
      ]
    }
  },
  {
    title: "Advanced DML Operations",
    chip: "Full Support",
    description: "Complete support for MERGE INTO, UPDATE, DELETE with efficient file handling",
    icon: <CodeBracketIcon className="w-6 h-6" />,
    color: "green",
    details: {
      title: "Data Manipulation in Spark",
      description: "Spark offers comprehensive DML support through Spark Session Extensions, enabling complex data modifications.",
      sections: [
        {
          heading: "MERGE INTO Operations",
          content: "Spark's MERGE INTO implementation is highly optimized for both small and large-scale updates.",
          subsections: [
            {
              title: "Basic Merge Syntax",
              content: "Combine insert, update, and delete operations in a single atomic transaction.",
              codeExample: `MERGE INTO target t
USING source s
ON t.id = s.id
WHEN MATCHED AND s.op = 'D' THEN DELETE
WHEN MATCHED THEN UPDATE SET t.value = s.value
WHEN NOT MATCHED THEN INSERT *`
            },
            {
              title: "Performance Optimization",
              content: "Spark automatically optimizes merge operations based on data characteristics and available statistics."
            }
          ]
        },
        {
          heading: "Delete File Management",
          content: "Since Iceberg 0.14+, Spark writes position and equality delete files instead of rewriting entire data files, significantly improving performance for row-level operations."
        }
      ],
      externalLinks: [
        { label: "Spark DML Documentation", url: "https://iceberg.apache.org/docs/latest/spark-writes/#sql-merge-into" },
        { label: "Delete Files Spec", url: "https://iceberg.apache.org/spec/#delete-files" }
      ]
    }
  },
  {
    title: "Format V3 Support",
    chip: "GA",
    description: "Full support for Iceberg V3 features including deletion vectors and new data types",
    icon: <BoltIcon className="w-6 h-6" />,
    color: "purple",
    details: {
      title: "Iceberg Format V3 in Spark",
      description: "Spark 3.5+ with Iceberg 1.8+ provides complete support for the V3 specification, unlocking advanced features.",
      sections: [
        {
          heading: "Binary Deletion Vectors",
          content: "Efficiently track deleted rows using bitmap structures instead of traditional delete files.",
          subsections: [
            {
              title: "Performance Benefits",
              content: "Deletion vectors reduce metadata overhead and improve read performance by up to 10x for tables with frequent deletes."
            },
            {
              title: "Automatic Optimization",
              content: "Spark automatically uses deletion vectors when appropriate based on delete patterns."
            }
          ]
        },
        {
          heading: "New Data Types",
          content: "Support for variant types, nanosecond timestamps, and geospatial data types.",
          subsections: [
            {
              title: "Variant Type",
              content: "Semi-structured data type for storing JSON-like data efficiently.",
              codeExample: `CREATE TABLE events (
  id BIGINT,
  payload VARIANT
) USING iceberg;`
            }
          ]
        }
      ],
      externalLinks: [
        { label: "V3 Specification", url: "https://iceberg.apache.org/spec/#version-3-extended-types-and-capabilities" },
        { label: "Deletion Vectors", url: "https://www.dremio.com/blog/apache-iceberg-v3/" }
      ]
    }
  },
  {
    title: "Time Travel & Versioning",
    chip: "Native",
    description: "Built-in support for querying historical data and managing table versions",
    icon: <ClockIcon className="w-6 h-6" />,
    color: "orange",
    details: {
      title: "Time Travel in Spark",
      description: "Spark provides native SQL syntax for accessing historical versions of Iceberg tables.",
      sections: [
        {
          heading: "Query Syntax",
          content: "Access historical data using VERSION AS OF or TIMESTAMP AS OF clauses.",
          subsections: [
            {
              title: "Snapshot-based Query",
              content: "Query specific snapshot versions of your table.",
              codeExample: `SELECT * FROM prod.db.table 
VERSION AS OF 1234567890`
            },
            {
              title: "Timestamp-based Query",
              content: "Query table state at a specific point in time.",
              codeExample: `SELECT * FROM prod.db.table 
TIMESTAMP AS OF '2024-01-15 10:00:00'`
            }
          ]
        },
        {
          heading: "Version Management",
          content: "Manage table versions through branches and tags for advanced workflows."
        }
      ],
      externalLinks: [
        { label: "Time Travel Guide", url: "https://iceberg.apache.org/docs/latest/spark-queries/#time-travel" }
      ]
    }
  },
  {
    title: "Streaming Support",
    chip: "Preview",
    description: "Incremental processing with Structured Streaming integration",
    icon: <BoltIcon className="w-6 h-6" />,
    color: "blue",
    details: {
      title: "Streaming with Spark",
      description: "Spark Structured Streaming can read and write Iceberg tables incrementally.",
      sections: [
        {
          heading: "Incremental Reads",
          content: "Consume only new data since last processed snapshot.",
          subsections: [
            {
              title: "Stream Configuration",
              content: "Configure streaming reads with start snapshot or timestamp.",
              codeExample: `spark.readStream
  .format("iceberg")
  .option("stream-from-timestamp", "2024-01-15 00:00:00")
  .load("prod.db.table")`
            }
          ]
        },
        {
          heading: "Streaming Writes",
          content: "Append-only and complete mode writes with exactly-once semantics."
        }
      ],
      externalLinks: [
        { label: "Streaming Documentation", url: "https://iceberg.apache.org/docs/latest/spark-structured-streaming/" }
      ]
    }
  },
  {
    title: "Enterprise Security",
    chip: "Full",
    description: "Comprehensive security integration with catalog-level access controls",
    icon: <ShieldCheckIcon className="w-6 h-6" />,
    color: "green",
    details: {
      title: "Security in Spark",
      description: "Spark delegates security to underlying catalogs, supporting enterprise-grade access controls.",
      sections: [
        {
          heading: "Access Control Integration",
          content: "Seamless integration with existing security frameworks.",
          subsections: [
            {
              title: "Apache Ranger",
              content: "Fine-grained access control through Hive Metastore integration."
            },
            {
              title: "AWS IAM",
              content: "Role-based access for S3 data and Glue catalog operations."
            },
            {
              title: "Nessie RBAC",
              content: "Git-like access control for multi-tenant environments."
            }
          ]
        }
      ],
      externalLinks: [
        { label: "Security Best Practices", url: "https://iceberg.apache.org/docs/latest/security/" }
      ]
    }
  }
];

export const sparkTableData = {
  title: "Apache Spark Iceberg Feature Matrix",
  description: "Detailed breakdown of Iceberg capabilities in Apache Spark 3.3+",
  variant: "default",
  columns: [
    {
      key: "feature",
      header: "Feature",
      tooltip: "Iceberg feature or capability",
      width: "w-64"
    },
    {
      key: "support",
      header: "Support Level",
      tooltip: "Level of support in Spark",
      align: "center",
      width: "w-32"
    },
    {
      key: "details",
      header: "Details",
      tooltip: "Implementation details and notes"
    },
    {
      key: "version",
      header: "Min Version",
      tooltip: "Minimum Spark version required",
      align: "center",
      width: "w-24"
    }
  ],
  rows: [
    {
      feature: { 
        value: <span className="font-medium">Hidden Partitioning</span> 
      },
      support: { 
        value: <span className="text-green-600 dark:text-green-400 font-semibold">Full</span>,
        badge: { text: "GA", variant: "success" },
        tooltip: "Complete support for all partition transforms"
      },
      details: { 
        value: "Identity, year, month, day, hour, bucket, truncate transforms",
        tooltip: "Spark automatically handles partition pruning for all transform types"
      },
      version: { value: "3.0+" }
    },
    {
      feature: { 
        value: <span className="font-medium">Schema Evolution</span> 
      },
      support: { 
        value: <span className="text-green-600 dark:text-green-400 font-semibold">Full</span>,
        badge: { text: "GA", variant: "success" }
      },
      details: { 
        value: "Add, drop, rename, reorder columns; type promotion",
        tooltip: "All schema changes are metadata-only operations"
      },
      version: { value: "3.0+" }
    },
    {
      feature: { 
        value: <span className="font-medium">Merge-on-Read</span> 
      },
      support: { 
        value: <span className="text-green-600 dark:text-green-400 font-semibold">Full</span>,
        badge: { text: "GA", variant: "success" }
      },
      details: { 
        value: "Automatic when delete files exist; configurable write mode",
        tooltip: "Spark automatically switches between CoW and MoR based on table state"
      },
      version: { value: "3.2+" }
    },
    {
      feature: { 
        value: <span className="font-medium">Metadata Tables</span> 
      },
      support: { 
        value: <span className="text-green-600 dark:text-green-400 font-semibold">Full</span>,
        badge: { text: "GA", variant: "success" }
      },
      details: { 
        value: "history, snapshots, files, manifests, partitions, all_data_files",
        tooltip: "Query table metadata using table.metadata$ syntax"
      },
      version: { value: "3.0+" }
    },
    {
      feature: { 
        value: <span className="font-medium">Table Maintenance</span> 
      },
      support: { 
        value: <span className="text-green-600 dark:text-green-400 font-semibold">Full</span>,
        badge: { text: "GA", variant: "success" }
      },
      details: { 
        value: "Stored procedures for expire_snapshots, rewrite_data_files, rewrite_manifests",
        tooltip: "Call procedures using CALL catalog.system.procedure_name"
      },
      version: { value: "3.0+" }
    },
    {
      feature: { 
        value: <span className="font-medium">Branching & Tagging</span> 
      },
      support: { 
        value: <span className="text-yellow-600 dark:text-yellow-400 font-semibold">Partial</span>,
        badge: { text: "Preview", variant: "warning" }
      },
      details: { 
        value: "Read from branches/tags; write requires procedures",
        tooltip: "Full SQL support for branch operations coming in future releases"
      },
      version: { value: "3.3+" }
    },
    {
      feature: { 
        value: <span className="font-medium">Multi-Table Transactions</span> 
      },
      support: { 
        value: <span className="text-red-600 dark:text-red-400 font-semibold">None</span>,
        badge: { text: "Not Supported", variant: "error" }
      },
      details: { 
        value: "Not supported; each table operation is independent",
        tooltip: "Requires distributed transaction coordinator"
      },
      version: { value: "N/A" }
    },
    {
      feature: { 
        value: <span className="font-medium">Row-Level Security</span> 
      },
      support: { 
        value: <span className="text-yellow-600 dark:text-yellow-400 font-semibold">Catalog-dependent</span>,
        badge: { text: "Varies", variant: "info" }
      },
      details: { 
        value: "Delegates to catalog implementation (e.g., Ranger for Hive)",
        tooltip: "Security policies are enforced by the underlying catalog"
      },
      version: { value: "3.0+" }
    }
  ]
};

export const sparkCodeExamples = [
  {
    title: "Basic Table Operations",
    description: "Creating and querying Iceberg tables in Spark",
    language: "scala",
    code: `// Configure Spark with Iceberg
spark.conf.set("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions")
spark.conf.set("spark.sql.catalog.prod", "org.apache.iceberg.spark.SparkCatalog")
spark.conf.set("spark.sql.catalog.prod.type", "hive")

// Create an Iceberg table
spark.sql("""
  CREATE TABLE IF NOT EXISTS prod.db.events (
    id BIGINT,
    event_time TIMESTAMP,
    user_id STRING,
    event_type STRING,
    properties MAP<STRING, STRING>
  ) USING iceberg
  PARTITIONED BY (days(event_time), bucket(16, user_id))
""")

// Insert data
spark.sql("""
  INSERT INTO prod.db.events 
  VALUES 
    (1, timestamp'2024-01-15 10:00:00', 'user123', 'click', map('page', 'home')),
    (2, timestamp'2024-01-15 10:05:00', 'user456', 'view', map('page', 'product'))
""")

// Query with partition pruning
spark.sql("""
  SELECT * FROM prod.db.events
  WHERE event_time >= '2024-01-15'
    AND user_id = 'user123'
""").show()`
  },
  {
    title: "Advanced DML Operations",
    description: "Performing MERGE INTO and DELETE operations",
    language: "sql",
    code: `-- Merge new events with deduplication
MERGE INTO prod.db.events AS target
USING (
  SELECT * FROM staging.events
  QUALIFY ROW_NUMBER() OVER (PARTITION BY id ORDER BY event_time DESC) = 1
) AS source
ON target.id = source.id
WHEN MATCHED AND source.event_type = 'delete' THEN
  DELETE
WHEN MATCHED THEN
  UPDATE SET
    event_time = source.event_time,
    event_type = source.event_type,
    properties = source.properties
WHEN NOT MATCHED THEN
  INSERT *

-- Delete old events
DELETE FROM prod.db.events
WHERE event_time < CURRENT_TIMESTAMP - INTERVAL 90 DAYS

-- Update specific records
UPDATE prod.db.events
SET properties = map_concat(properties, map('updated', 'true'))
WHERE event_type = 'click' AND date(event_time) = CURRENT_DATE`
  },
  {
    title: "Table Maintenance",
    description: "Running maintenance procedures for optimal performance",
    language: "sql",
    code: `-- Expire old snapshots
CALL prod.system.expire_snapshots(
  table => 'db.events',
  older_than => TIMESTAMP '2024-01-01 00:00:00',
  retain_last => 10
);

-- Rewrite small files
CALL prod.system.rewrite_data_files(
  table => 'db.events',
  strategy => 'binpack',
  options => map(
    'target-file-size-bytes', '134217728',
    'min-file-size-bytes', '33554432'
  )
);

-- Rewrite manifests for better performance
CALL prod.system.rewrite_manifests(
  table => 'db.events',
  use_caching => true
);

-- Remove orphan files
CALL prod.system.remove_orphan_files(
  table => 'db.events',
  older_than => TIMESTAMP '2024-01-10 00:00:00',
  dry_run => false
);`
  },
  {
    title: "Streaming Integration",
    description: "Using Spark Structured Streaming with Iceberg",
    language: "scala",
    code: `// Read stream from Kafka
val kafkaStream = spark
  .readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "localhost:9092")
  .option("subscribe", "events")
  .load()

// Parse and transform
val events = kafkaStream
  .select(from_json($"value".cast("string"), eventSchema).as("data"))
  .select("data.*")
  .withColumn("processing_time", current_timestamp())

// Write to Iceberg with exactly-once semantics
val query = events
  .writeStream
  .format("iceberg")
  .outputMode("append")
  .trigger(Trigger.ProcessingTime("1 minute"))
  .option("checkpointLocation", "/tmp/checkpoints/events")
  .option("fanout-enabled", "true")
  .toTable("prod.db.events")

// Read incremental changes from Iceberg
val icebergStream = spark
  .readStream
  .format("iceberg")
  .option("stream-from-timestamp", "2024-01-15 00:00:00")
  .load("prod.db.events")
  .filter($"event_type" === "purchase")
  
// Process and write to another system
icebergStream
  .writeStream
  .format("console")
  .outputMode("append")
  .start()`
  }
];

export const sparkUseCases = [
  {
    title: "Data Lake Analytics",
    description: "Spark excels at large-scale analytical workloads on data lakes",
    scenarios: [
      "ETL pipelines processing petabytes of data",
      "Complex analytical queries with multiple joins",
      "Machine learning feature engineering",
      "Data quality validation and cleansing"
    ]
  },
  {
    title: "Batch Processing",
    description: "Ideal for scheduled batch jobs and data transformations",
    scenarios: [
      "Daily/hourly data aggregations",
      "Historical data reprocessing",
      "Large-scale data migrations",
      "Report generation and materialization"
    ]
  },
  {
    title: "Hybrid Workloads",
    description: "Combine batch and streaming processing in a single framework",
    scenarios: [
      "Lambda architecture implementations",
      "Real-time + historical analytics",
      "CDC pipeline processing",
      "Event-driven data processing"
    ]
  },
  {
    title: "Data Engineering",
    description: "Build robust data pipelines with schema evolution",
    scenarios: [
      "Multi-stage transformation pipelines",
      "Data warehouse loading (ELT/ETL)",
      "Cross-system data synchronization",
      "Data archival and compliance"
    ]
  }
];

<QueryEngineLayout
  title="Apache Spark"
  description="The reference implementation for Apache Iceberg with comprehensive read/write support"
  features={sparkFeatures}
  tableData={sparkTableData}
  codeExamples={sparkCodeExamples}
  useCases={sparkUseCases}
  officialDocs="https://spark.apache.org/docs/latest/"
  gettingStarted="https://iceberg.apache.org/docs/latest/spark-getting-started/"
  additionalResources={[
    { label: "Spark Configuration", url: "https://iceberg.apache.org/docs/latest/spark-configuration/" },
    { label: "Spark Procedures", url: "https://iceberg.apache.org/docs/latest/spark-procedures/" },
    { label: "Performance Tuning", url: "https://spark.apache.org/docs/latest/sql-performance-tuning.html" }
  ]}
/>