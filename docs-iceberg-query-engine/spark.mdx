---
title: Apache Spark 3.3+
description: The reference implementation for Apache Iceberg with comprehensive read/write support, full DML capabilities, and GA Format V3 support
hide_table_of_contents: true
---

{/* ---
title: Apache Spark
description: The reference implementation for Apache Iceberg with comprehensive read/write support
---

import { QueryEngineLayout } from '@site/src/components/Iceberg/QueryEngineLayout';
import { 
  SparkIcon,
//   DatabaseIcon,
  BoltIcon,
  ShieldCheckIcon,
  ClockIcon,
  CodeBracketIcon
} from '@heroicons/react/24/outline';

export const sparkFeatures = [
  {
    title: "Comprehensive Catalog Support",
    chip: "Full Support",
    description: "Native integration with all major catalog types including Hive, Glue, REST, and Nessie",
    icon: <BoltIcon className="w-6 h-6" />,
    color: "blue",
    score: 100,
    details: {
      title: "Catalog Integration in Spark",
      description: "Spark provides the most comprehensive catalog support among all query engines, making it the ideal choice for multi-catalog environments.",
      overviewContent: {
        strengths: [
          "Supports all major catalog implementations",
          "Seamless switching between catalogs",
          "Custom catalog plugin support",
          "Multi-catalog queries in single session"
        ],
        limitations: [
          "Catalog-specific configurations required",
          "Some features vary by catalog type",
          "Cross-catalog transactions not supported"
        ],
        bestFor: [
          "Multi-cloud deployments",
          "Hybrid on-premise/cloud architectures",
          "Organizations with existing Hive infrastructure",
          "Teams requiring catalog flexibility"
        ]
      },
      technicalSpecs: [
        {
          category: "Supported Catalogs",
          items: [
            { label: "Hive Metastore", value: "Full Support", status: "available" },
            { label: "AWS Glue", value: "Full Support", status: "available" },
            { label: "REST/Tabular", value: "Full Support", status: "available" },
            { label: "Nessie", value: "Full Support", status: "available" },
            { label: "JDBC", value: "Full Support", status: "available" },
            { label: "Hadoop", value: "Full Support", status: "available" },
            { label: "Custom Catalogs", value: "Via Plugin API", status: "available" }
          ]
        },
        {
          category: "Catalog Features",
          items: [
            { label: "Multi-catalog Support", value: "Yes", status: "available" },
            { label: "Dynamic Catalog Loading", value: "Yes", status: "available" },
            { label: "Namespace Operations", value: "Full CRUD", status: "available" },
            { label: "Table Locking", value: "Catalog-dependent", status: "available" }
          ]
        }
      ],
      codeExamples: [
        {
          title: "Hive Metastore Configuration",
          language: "properties",
          code: `spark.sql.catalog.my_catalog=org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.my_catalog.type=hive
spark.sql.catalog.my_catalog.uri=thrift://metastore-host:9083
spark.sql.catalog.my_catalog.warehouse=s3://my-bucket/warehouse`,
          description: "Standard Hive Metastore configuration for Spark"
        },
        {
          title: "AWS Glue Catalog",
          language: "properties",
          code: `spark.sql.catalog.glue=org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.glue.catalog-impl=org.apache.iceberg.aws.glue.GlueCatalog
spark.sql.catalog.glue.io-impl=org.apache.iceberg.aws.s3.S3FileIO
spark.sql.catalog.glue.warehouse=s3://my-bucket/warehouse`,
          description: "Configure Spark to use AWS Glue as the catalog"
        }
      ],
      architectureNotes: [
        {
          title: "Catalog Architecture",
          content: "Spark's catalog abstraction allows seamless integration with various metadata stores. The SparkCatalog implementation wraps underlying catalog implementations, providing a unified interface for all Iceberg operations."
        }
      ],
      externalLinks: [
        { label: "Spark Configuration Guide", url: "https://iceberg.apache.org/docs/latest/spark-configuration/", type: "docs" },
        { label: "Catalog Properties Reference", url: "https://iceberg.apache.org/docs/latest/spark-configuration/#catalogs", type: "docs" },
        { label: "Multi-Catalog Tutorial", url: "https://iceberg.apache.org/docs/latest/spark-procedures/", type: "tutorial" }
      ]
    }
  },
  {
    title: "Advanced DML Operations",
    chip: "Full Support",
    description: "Complete support for MERGE INTO, UPDATE, DELETE with efficient file handling",
    icon: <CodeBracketIcon className="w-6 h-6" />,
    color: "green",
    details: {
      title: "Data Manipulation in Spark",
      description: "Spark offers comprehensive DML support through Spark Session Extensions, enabling complex data modifications.",
      sections: [
        {
          heading: "MERGE INTO Operations",
          content: "Spark's MERGE INTO implementation is highly optimized for both small and large-scale updates.",
          subsections: [
            {
              title: "Basic Merge Syntax",
              content: "Combine insert, update, and delete operations in a single atomic transaction.",
              codeExample: `MERGE INTO target t
USING source s
ON t.id = s.id
WHEN MATCHED AND s.op = 'D' THEN DELETE
WHEN MATCHED THEN UPDATE SET t.value = s.value
WHEN NOT MATCHED THEN INSERT *`
            },
            {
              title: "Performance Optimization",
              content: "Spark automatically optimizes merge operations based on data characteristics and available statistics."
            }
          ]
        },
        {
          heading: "Delete File Management",
          content: "Since Iceberg 0.14+, Spark writes position and equality delete files instead of rewriting entire data files, significantly improving performance for row-level operations."
        }
      ],
      externalLinks: [
        { label: "Spark DML Documentation", url: "https://iceberg.apache.org/docs/latest/spark-writes/#sql-merge-into" },
        { label: "Delete Files Spec", url: "https://iceberg.apache.org/spec/#delete-files" }
      ]
    }
  },
  {
    title: "Format V3 Support",
    chip: "GA",
    description: "Full support for Iceberg V3 features including deletion vectors and new data types",
    icon: <BoltIcon className="w-6 h-6" />,
    color: "purple",
    details: {
      title: "Iceberg Format V3 in Spark",
      description: "Spark 3.5+ with Iceberg 1.8+ provides complete support for the V3 specification, unlocking advanced features.",
      sections: [
        {
          heading: "Binary Deletion Vectors",
          content: "Efficiently track deleted rows using bitmap structures instead of traditional delete files.",
          subsections: [
            {
              title: "Performance Benefits",
              content: "Deletion vectors reduce metadata overhead and improve read performance by up to 10x for tables with frequent deletes."
            },
            {
              title: "Automatic Optimization",
              content: "Spark automatically uses deletion vectors when appropriate based on delete patterns."
            }
          ]
        },
        {
          heading: "New Data Types",
          content: "Support for variant types, nanosecond timestamps, and geospatial data types.",
          subsections: [
            {
              title: "Variant Type",
              content: "Semi-structured data type for storing JSON-like data efficiently.",
              codeExample: `CREATE TABLE events (
  id BIGINT,
  payload VARIANT
) USING iceberg;`
            }
          ]
        }
      ],
      externalLinks: [
        { label: "V3 Specification", url: "https://iceberg.apache.org/spec/#version-3-extended-types-and-capabilities" },
        { label: "Deletion Vectors", url: "https://www.dremio.com/blog/apache-iceberg-v3/" }
      ]
    }
  },
  {
    title: "Time Travel & Versioning",
    chip: "Native",
    description: "Built-in support for querying historical data and managing table versions",
    icon: <ClockIcon className="w-6 h-6" />,
    color: "orange",
    details: {
      title: "Time Travel in Spark",
      description: "Spark provides native SQL syntax for accessing historical versions of Iceberg tables.",
      sections: [
        {
          heading: "Query Syntax",
          content: "Access historical data using VERSION AS OF or TIMESTAMP AS OF clauses.",
          subsections: [
            {
              title: "Snapshot-based Query",
              content: "Query specific snapshot versions of your table.",
              codeExample: `SELECT * FROM prod.db.table 
VERSION AS OF 1234567890`
            },
            {
              title: "Timestamp-based Query",
              content: "Query table state at a specific point in time.",
              codeExample: `SELECT * FROM prod.db.table 
TIMESTAMP AS OF '2024-01-15 10:00:00'`
            }
          ]
        },
        {
          heading: "Version Management",
          content: "Manage table versions through branches and tags for advanced workflows."
        }
      ],
      externalLinks: [
        { label: "Time Travel Guide", url: "https://iceberg.apache.org/docs/latest/spark-queries/#time-travel" }
      ]
    }
  },
  {
    title: "Streaming Support",
    chip: "Preview",
    description: "Incremental processing with Structured Streaming integration",
    icon: <BoltIcon className="w-6 h-6" />,
    color: "blue",
    details: {
      title: "Streaming with Spark",
      description: "Spark Structured Streaming can read and write Iceberg tables incrementally.",
      sections: [
        {
          heading: "Incremental Reads",
          content: "Consume only new data since last processed snapshot.",
          subsections: [
            {
              title: "Stream Configuration",
              content: "Configure streaming reads with start snapshot or timestamp.",
              codeExample: `spark.readStream
  .format("iceberg")
  .option("stream-from-timestamp", "2024-01-15 00:00:00")
  .load("prod.db.table")`
            }
          ]
        },
        {
          heading: "Streaming Writes",
          content: "Append-only and complete mode writes with exactly-once semantics."
        }
      ],
      externalLinks: [
        { label: "Streaming Documentation", url: "https://iceberg.apache.org/docs/latest/spark-structured-streaming/" }
      ]
    }
  },
  {
    title: "Enterprise Security",
    chip: "Full",
    description: "Comprehensive security integration with catalog-level access controls",
    icon: <ShieldCheckIcon className="w-6 h-6" />,
    color: "green",
    details: {
      title: "Security in Spark",
      description: "Spark delegates security to underlying catalogs, supporting enterprise-grade access controls.",
      sections: [
        {
          heading: "Access Control Integration",
          content: "Seamless integration with existing security frameworks.",
          subsections: [
            {
              title: "Apache Ranger",
              content: "Fine-grained access control through Hive Metastore integration."
            },
            {
              title: "AWS IAM",
              content: "Role-based access for S3 data and Glue catalog operations."
            },
            {
              title: "Nessie RBAC",
              content: "Git-like access control for multi-tenant environments."
            }
          ]
        }
      ],
      externalLinks: [
        { label: "Security Best Practices", url: "https://iceberg.apache.org/docs/latest/security/" }
      ]
    }
  }
];

export const sparkTableData = {
  title: "Apache Spark Iceberg Feature Matrix",
  description: "Detailed breakdown of Iceberg capabilities in Apache Spark 3.3+",
  variant: "default",
  columns: [
    {
      key: "feature",
      header: "Feature",
      tooltip: "Iceberg feature or capability",
      width: "w-64"
    },
    {
      key: "support",
      header: "Support Level",
      tooltip: "Level of support in Spark",
      align: "center",
      width: "w-32"
    },
    {
      key: "details",
      header: "Details",
      tooltip: "Implementation details and notes"
    },
    {
      key: "version",
      header: "Min Version",
      tooltip: "Minimum Spark version required",
      align: "center",
      width: "w-24"
    }
  ],
  rows: [
    {
      feature: { 
        value: <span className="font-medium">Hidden Partitioning</span> 
      },
      support: { 
        value: <span className="text-green-600 dark:text-green-400 font-semibold">Full</span>,
        badge: { text: "GA", variant: "success" },
        tooltip: "Complete support for all partition transforms"
      },
      details: { 
        value: "Identity, year, month, day, hour, bucket, truncate transforms",
        tooltip: "Spark automatically handles partition pruning for all transform types"
      },
      version: { value: "3.0+" }
    },
    {
      feature: { 
        value: <span className="font-medium">Schema Evolution</span> 
      },
      support: { 
        value: <span className="text-green-600 dark:text-green-400 font-semibold">Full</span>,
        badge: { text: "GA", variant: "success" }
      },
      details: { 
        value: "Add, drop, rename, reorder columns; type promotion",
        tooltip: "All schema changes are metadata-only operations"
      },
      version: { value: "3.0+" }
    },
    {
      feature: { 
        value: <span className="font-medium">Merge-on-Read</span> 
      },
      support: { 
        value: <span className="text-green-600 dark:text-green-400 font-semibold">Full</span>,
        badge: { text: "GA", variant: "success" }
      },
      details: { 
        value: "Automatic when delete files exist; configurable write mode",
        tooltip: "Spark automatically switches between CoW and MoR based on table state"
      },
      version: { value: "3.2+" }
    },
    {
      feature: { 
        value: <span className="font-medium">Metadata Tables</span> 
      },
      support: { 
        value: <span className="text-green-600 dark:text-green-400 font-semibold">Full</span>,
        badge: { text: "GA", variant: "success" }
      },
      details: { 
        value: "history, snapshots, files, manifests, partitions, all_data_files",
        tooltip: "Query table metadata using table.metadata$ syntax"
      },
      version: { value: "3.0+" }
    },
    {
      feature: { 
        value: <span className="font-medium">Table Maintenance</span> 
      },
      support: { 
        value: <span className="text-green-600 dark:text-green-400 font-semibold">Full</span>,
        badge: { text: "GA", variant: "success" }
      },
      details: { 
        value: "Stored procedures for expire_snapshots, rewrite_data_files, rewrite_manifests",
        tooltip: "Call procedures using CALL catalog.system.procedure_name"
      },
      version: { value: "3.0+" }
    },
    {
      feature: { 
        value: <span className="font-medium">Branching & Tagging</span> 
      },
      support: { 
        value: <span className="text-yellow-600 dark:text-yellow-400 font-semibold">Partial</span>,
        badge: { text: "Preview", variant: "warning" }
      },
      details: { 
        value: "Read from branches/tags; write requires procedures",
        tooltip: "Full SQL support for branch operations coming in future releases"
      },
      version: { value: "3.3+" }
    },
    {
      feature: { 
        value: <span className="font-medium">Multi-Table Transactions</span> 
      },
      support: { 
        value: <span className="text-red-600 dark:text-red-400 font-semibold">None</span>,
        badge: { text: "Not Supported", variant: "error" }
      },
      details: { 
        value: "Not supported; each table operation is independent",
        tooltip: "Requires distributed transaction coordinator"
      },
      version: { value: "N/A" }
    },
    {
      feature: { 
        value: <span className="font-medium">Row-Level Security</span> 
      },
      support: { 
        value: <span className="text-yellow-600 dark:text-yellow-400 font-semibold">Catalog-dependent</span>,
        badge: { text: "Varies", variant: "info" }
      },
      details: { 
        value: "Delegates to catalog implementation (e.g., Ranger for Hive)",
        tooltip: "Security policies are enforced by the underlying catalog"
      },
      version: { value: "3.0+" }
    }
  ]
};

export const sparkCodeExamples = [
  {
    title: "Basic Table Operations",
    description: "Creating and querying Iceberg tables in Spark",
    language: "scala",
    code: `// Configure Spark with Iceberg
spark.conf.set("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions")
spark.conf.set("spark.sql.catalog.prod", "org.apache.iceberg.spark.SparkCatalog")
spark.conf.set("spark.sql.catalog.prod.type", "hive")

// Create an Iceberg table
spark.sql("""
  CREATE TABLE IF NOT EXISTS prod.db.events (
    id BIGINT,
    event_time TIMESTAMP,
    user_id STRING,
    event_type STRING,
    properties MAP<STRING, STRING>
  ) USING iceberg
  PARTITIONED BY (days(event_time), bucket(16, user_id))
""")

// Insert data
spark.sql("""
  INSERT INTO prod.db.events 
  VALUES 
    (1, timestamp'2024-01-15 10:00:00', 'user123', 'click', map('page', 'home')),
    (2, timestamp'2024-01-15 10:05:00', 'user456', 'view', map('page', 'product'))
""")

// Query with partition pruning
spark.sql("""
  SELECT * FROM prod.db.events
  WHERE event_time >= '2024-01-15'
    AND user_id = 'user123'
""").show()`
  },
  {
    title: "Advanced DML Operations",
    description: "Performing MERGE INTO and DELETE operations",
    language: "sql",
    code: `-- Merge new events with deduplication
MERGE INTO prod.db.events AS target
USING (
  SELECT * FROM staging.events
  QUALIFY ROW_NUMBER() OVER (PARTITION BY id ORDER BY event_time DESC) = 1
) AS source
ON target.id = source.id
WHEN MATCHED AND source.event_type = 'delete' THEN
  DELETE
WHEN MATCHED THEN
  UPDATE SET
    event_time = source.event_time,
    event_type = source.event_type,
    properties = source.properties
WHEN NOT MATCHED THEN
  INSERT *

-- Delete old events
DELETE FROM prod.db.events
WHERE event_time < CURRENT_TIMESTAMP - INTERVAL 90 DAYS

-- Update specific records
UPDATE prod.db.events
SET properties = map_concat(properties, map('updated', 'true'))
WHERE event_type = 'click' AND date(event_time) = CURRENT_DATE`
  },
  {
    title: "Table Maintenance",
    description: "Running maintenance procedures for optimal performance",
    language: "sql",
    code: `-- Expire old snapshots
CALL prod.system.expire_snapshots(
  table => 'db.events',
  older_than => TIMESTAMP '2024-01-01 00:00:00',
  retain_last => 10
);

-- Rewrite small files
CALL prod.system.rewrite_data_files(
  table => 'db.events',
  strategy => 'binpack',
  options => map(
    'target-file-size-bytes', '134217728',
    'min-file-size-bytes', '33554432'
  )
);

-- Rewrite manifests for better performance
CALL prod.system.rewrite_manifests(
  table => 'db.events',
  use_caching => true
);

-- Remove orphan files
CALL prod.system.remove_orphan_files(
  table => 'db.events',
  older_than => TIMESTAMP '2024-01-10 00:00:00',
  dry_run => false
);`
  },
  {
    title: "Streaming Integration",
    description: "Using Spark Structured Streaming with Iceberg",
    language: "scala",
    code: `// Read stream from Kafka
val kafkaStream = spark
  .readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "localhost:9092")
  .option("subscribe", "events")
  .load()

// Parse and transform
val events = kafkaStream
  .select(from_json($"value".cast("string"), eventSchema).as("data"))
  .select("data.*")
  .withColumn("processing_time", current_timestamp())

// Write to Iceberg with exactly-once semantics
val query = events
  .writeStream
  .format("iceberg")
  .outputMode("append")
  .trigger(Trigger.ProcessingTime("1 minute"))
  .option("checkpointLocation", "/tmp/checkpoints/events")
  .option("fanout-enabled", "true")
  .toTable("prod.db.events")

// Read incremental changes from Iceberg
val icebergStream = spark
  .readStream
  .format("iceberg")
  .option("stream-from-timestamp", "2024-01-15 00:00:00")
  .load("prod.db.events")
  .filter($"event_type" === "purchase")
  
// Process and write to another system
icebergStream
  .writeStream
  .format("console")
  .outputMode("append")
  .start()`
  }
];

export const sparkUseCases = [
  {
    title: "Data Lake Analytics",
    description: "Spark excels at large-scale analytical workloads on data lakes",
    scenarios: [
      "ETL pipelines processing petabytes of data",
      "Complex analytical queries with multiple joins",
      "Machine learning feature engineering",
      "Data quality validation and cleansing"
    ]
  },
  {
    title: "Batch Processing",
    description: "Ideal for scheduled batch jobs and data transformations",
    scenarios: [
      "Daily/hourly data aggregations",
      "Historical data reprocessing",
      "Large-scale data migrations",
      "Report generation and materialization"
    ]
  },
  {
    title: "Hybrid Workloads",
    description: "Combine batch and streaming processing in a single framework",
    scenarios: [
      "Lambda architecture implementations",
      "Real-time + historical analytics",
      "CDC pipeline processing",
      "Event-driven data processing"
    ]
  },
  {
    title: "Data Engineering",
    description: "Build robust data pipelines with schema evolution",
    scenarios: [
      "Multi-stage transformation pipelines",
      "Data warehouse loading (ELT/ETL)",
      "Cross-system data synchronization",
      "Data archival and compliance"
    ]
  }
];

<QueryEngineLayout
  title="Apache Spark"
  description="The reference implementation for Apache Iceberg with comprehensive read/write support"
  features={sparkFeatures}
  tableData={sparkTableData}
  codeExamples={sparkCodeExamples}
  useCases={sparkUseCases}
  officialDocs="https://spark.apache.org/docs/latest/"
  gettingStarted="https://iceberg.apache.org/docs/latest/spark-getting-started/"
  additionalResources={[
    { label: "Spark Configuration", url: "https://iceberg.apache.org/docs/latest/spark-configuration/" },
    { label: "Spark Procedures", url: "https://iceberg.apache.org/docs/latest/spark-procedures/" },
    { label: "Performance Tuning", url: "https://spark.apache.org/docs/latest/sql-performance-tuning.html" }
  ]}
/> */}




import { QueryEngineLayout } from '@site/src/components/Iceberg/QueryEngineLayout';
import { 
  ServerStackIcon,
  // DatabaseIcon,
  BoltIcon,
  ShieldCheckIcon,
  ClockIcon,
  CodeBracketIcon,
  CubeIcon,
  CloudIcon
} from '@heroicons/react/24/outline';

export const sparkFeatures = [
  {
    title: "Comprehensive Catalog Support",
    chip: "Full Support",
    description: "Hive Metastore, Hadoop warehouse, REST, AWS Glue, JDBC, Nessie, plus custom plug-ins via spark.sql.catalog.* settings",
    icon: <BoltIcon className="w-6 h-6" />,
    color: "blue",
    score: 100,
    details: {
      title: "Catalog Integration in Spark",
      description: "Spark provides the most comprehensive catalog support among all query engines, making it the reference implementation for Iceberg catalog interactions.",
      overviewContent: {
        strengths: [
          "Supports all major catalog implementations",
          "Seamless switching between catalogs in single session",
          "Custom catalog plugin support via SparkCatalog API",
          "Multi-catalog queries with catalog.database.table syntax",
          "Dynamic catalog configuration and loading"
        ],
        limitations: [
          "Catalog-specific configurations required for each type",
          "Some advanced features vary by catalog implementation",
          "Cross-catalog transactions not supported",
          "Catalog failover requires application logic"
        ],
        bestFor: [
          "Multi-cloud deployments with different catalog types",
          "Organizations migrating between catalog systems",
          "Hybrid on-premise/cloud architectures",
          "Teams requiring maximum catalog flexibility"
        ]
      },
      technicalSpecs: [
        {
          category: "Native Catalog Support",
          items: [
            { label: "Hive Metastore", value: "Full Support", status: "available" },
            { label: "AWS Glue", value: "Full Support", status: "available" },
            { label: "REST/Tabular", value: "Full Support", status: "available" },
            { label: "Nessie", value: "Full Support", status: "available" },
            { label: "JDBC Catalog", value: "Full Support", status: "available" },
            { label: "Hadoop Warehouse", value: "Full Support", status: "available" }
          ]
        },
        {
          category: "Advanced Features",
          items: [
            { label: "Multi-catalog Support", value: "Yes", status: "available" },
            { label: "Dynamic Catalog Loading", value: "Yes", status: "available" },
            { label: "Custom Catalog Plugins", value: "Via SparkCatalog API", status: "available" },
            { label: "Namespace Operations", value: "Full CRUD", status: "available" }
          ]
        }
      ],
      architectureNotes: [
        {
          title: "Catalog Architecture",
          content: "Spark's catalog abstraction provides a unified interface for all Iceberg operations while allowing each catalog implementation to optimize for its specific backend storage and metadata management approach."
        }
      ],
      externalLinks: [
        { label: "Spark Configuration Guide", url: "https://iceberg.apache.org/docs/latest/spark-configuration/#catalogs", type: "docs" },
        { label: "Catalog Properties Reference", url: "https://iceberg.apache.org/docs/latest/spark-configuration/", type: "docs" }
      ]
    }
  },
  {
    title: "Complete Read/Write Operations",
    chip: "Full Support", 
    description: "Full table scans, metadata-table reads, INSERT INTO, atomic INSERT OVERWRITE, DataFrame writeTo, and stored procedures",
    icon: <ServerStackIcon className="w-6 h-6" />,
    color: "green",
    score: 100,
    details: {
      title: "Read/Write Capabilities in Spark",
      description: "Spark offers the most comprehensive read and write capabilities for Iceberg tables, serving as the reference implementation.",
      overviewContent: {
        strengths: [
          "Full table scans with automatic optimization",
          "Complete metadata table access for monitoring",
          "Atomic write operations with ACID guarantees",
          "Built-in stored procedures for maintenance",
          "DataFrame API integration for programmatic access"
        ],
        limitations: [
          "Large writes may require careful partitioning strategy",
          "CTAS/RTAS atomic only under SparkCatalog",
          "Streaming writes have some limitations with delete operations"
        ],
        bestFor: [
          "ETL/ELT pipelines with complex transformations",
          "Data lake analytics with large-scale processing",
          "Mixed batch and streaming workloads",
          "Applications requiring programmatic table access"
        ]
      },
      technicalSpecs: [
        {
          category: "Read Operations",
          items: [
            { label: "Full Table Scans", value: "Optimized", status: "available" },
            { label: "Metadata Tables", value: ".history, .snapshots, .files", status: "available" },
            { label: "Partition Pruning", value: "Automatic", status: "available" },
            { label: "Predicate Pushdown", value: "Full Support", status: "available" }
          ]
        },
        {
          category: "Write Operations", 
          items: [
            { label: "INSERT INTO", value: "ACID Compliant", status: "available" },
            { label: "INSERT OVERWRITE", value: "Atomic", status: "available" },
            { label: "DataFrame WriteTo", value: "Full API", status: "available" },
            { label: "Stored Procedures", value: "Built-in", status: "available" }
          ]
        }
      ],
      externalLinks: [
        { label: "Spark Queries", url: "https://iceberg.apache.org/docs/latest/spark-queries/", type: "docs" },
        { label: "Spark Writes", url: "https://iceberg.apache.org/docs/latest/spark-writes/", type: "docs" },
        { label: "Spark Procedures", url: "https://iceberg.apache.org/docs/latest/spark-procedures/", type: "docs" }
      ]
    }
  },
  {
    title: "Advanced DML Operations",
    chip: "Full Support",
    description: "MERGE INTO, UPDATE, DELETE via Spark Session Extensions; Iceberg 0.14+ emits position/equality-delete files",
    icon: <CodeBracketIcon className="w-6 h-6" />,
    color: "purple",
    score: 100,
    details: {
      title: "DML Operations in Spark",
      description: "Spark provides complete DML support through Session Extensions with modern delete file architecture for optimal performance.",
      overviewContent: {
        strengths: [
          "Complete MERGE INTO with complex conditions",
          "Efficient UPDATE and DELETE operations",
          "Position/equality delete files (Iceberg 0.14+)",
          "Reduced I/O through incremental updates",
          "Parallel execution for large-scale operations"
        ],
        limitations: [
          "Requires Spark Session Extensions configuration", 
          "Performance depends on partition strategy",
          "Complex merges may require optimization"
        ],
        bestFor: [
          "CDC pipelines with frequent updates",
          "Data synchronization between systems",
          "Slowly changing dimension processing",
          "Data correction and cleanup operations"
        ]
      },
      technicalSpecs: [
        {
          category: "DML Operations",
          items: [
            { label: "MERGE INTO", value: "Full Support", status: "available" },
            { label: "UPDATE", value: "Row-level", status: "available" },
            { label: "DELETE", value: "Predicate-based", status: "available" },
            { label: "INSERT", value: "Standard + Overwrite", status: "available" }
          ]
        },
        {
          category: "Performance Features",
          items: [
            { label: "Delete Files", value: "Position + Equality", status: "available" },
            { label: "Parallel Execution", value: "Yes", status: "available" },
            { label: "Incremental Updates", value: "Yes", status: "available" },
            { label: "ACID Guarantees", value: "Full", status: "available" }
          ]
        }
      ],
      externalLinks: [
        { label: "Spark Writes - INSERT INTO", url: "https://iceberg.apache.org/docs/latest/spark-writes/#insert-into", type: "docs" },
        { label: "Iceberg Delete Formats", url: "https://iceberg.apache.org/spec/#delete-formats", type: "docs" }
      ]
    }
  },
  {
    title: "MoR/CoW Storage Strategies",
    chip: "Full Support",
    description: "Copy-on-Write default for delete commands; Merge-on-Read enabled via write.delete.mode=merge-on-read",
    icon: <CubeIcon className="w-6 h-6" />,
    color: "blue",
    score: 100,
    details: {
      title: "Storage Strategy Support",
      description: "Spark provides flexible storage strategies optimized for different workload patterns.",
      overviewContent: {
        strengths: [
          "Automatic strategy selection based on table state",
          "Runtime configuration changes per table",
          "Workload-optimized performance",
          "Seamless switching between strategies"
        ],
        limitations: [
          "Strategy choice impacts read/write performance differently",
          "Requires understanding of workload patterns",
          "Merge-on-read adds complexity to read operations"
        ],
        bestFor: [
          "Mixed workloads with varying read/write patterns",
          "Tables with different lifecycle stages",
          "Performance optimization based on usage patterns"
        ]
      },
      technicalSpecs: [
        {
          category: "Copy-on-Write (Default)",
          items: [
            { label: "Read Performance", value: "Optimized", status: "available" },
            { label: "Write Performance", value: "Higher Latency", status: "available" },
            { label: "Storage Efficiency", value: "High", status: "available" },
            { label: "Best For", value: "Read-heavy workloads", status: "available" }
          ]
        },
        {
          category: "Merge-on-Read",
          items: [
            { label: "Write Performance", value: "Optimized", status: "available" },
            { label: "Read Performance", value: "Merge Overhead", status: "available" },
            { label: "Update Frequency", value: "High", status: "available" },
            { label: "Best For", value: "Write-heavy workloads", status: "available" }
          ]
        }
      ],
      externalLinks: [
        { label: "Configuration - write.delete.mode", url: "https://iceberg.apache.org/docs/latest/configuration/#write.delete.mode", type: "docs" }
      ]
    }
  },
  {
    title: "Streaming Capabilities", 
    chip: "Partial Support",
    description: "Incremental reads with stream-from-timestamp; Append/Complete output modes; overwrite and delete snapshots skipped by default",
    icon: <BoltIcon className="w-6 h-6" />,
    color: "orange",
    score: 75,
    details: {
      title: "Streaming Integration",
      description: "Spark Structured Streaming provides robust integration with Iceberg tables for incremental processing.",
      overviewContent: {
        strengths: [
          "Incremental reads with configurable starting points",
          "Exactly-once semantics for streaming writes",
          "Integration with Kafka and other sources",
          "Append and complete output modes"
        ],
        limitations: [
          "Delete and overwrite snapshots skipped by default",
          "Streaming readers ignore deletion vector snapshots",
          "Micro-batch overhead for small batches",
          "Limited support for complex streaming operations"
        ],
        bestFor: [
          "Real-time data ingestion pipelines",
          "Incremental ETL processing", 
          "Event stream processing",
          "Near real-time analytics"
        ]
      },
      technicalSpecs: [
        {
          category: "Streaming Reads",
          items: [
            { label: "Incremental Processing", value: "stream-from-timestamp", status: "available" },
            { label: "Append Mode", value: "Yes", status: "available" },
            { label: "Complete Mode", value: "Yes", status: "available" },
            { label: "Delete Snapshot Handling", value: "Configurable Skip", status: "limited" }
          ]
        },
        {
          category: "Streaming Writes", 
          items: [
            { label: "Append Output", value: "Yes", status: "available" },
            { label: "Complete Output", value: "Yes", status: "available" },
            { label: "Exactly-Once", value: "Yes", status: "available" },
            { label: "Watermarking", value: "Yes", status: "available" }
          ]
        }
      ],
      externalLinks: [
        { label: "Structured Streaming", url: "https://iceberg.apache.org/docs/latest/spark-structured-streaming/", type: "docs" }
      ]
    }
  },
  {
    title: "Format V3 Support",
    chip: "GA",
    description: "GA read + write on Spark 3.5 with Iceberg 1.8+; Deletion Vectors, Row Lineage, new types, multi-arg transforms",
    icon: <CloudIcon className="w-6 h-6" />,
    color: "purple",
    score: 100,
    details: {
      title: "Iceberg Format V3 Leadership",
      description: "Spark leads the ecosystem with complete General Availability support for Iceberg Format V3 features.",
      overviewContent: {
        strengths: [
          "First engine with GA V3 read/write support",
          "Deletion vectors for efficient delete operations", 
          "Row lineage tracking for audit and debugging",
          "New data types including variant and geospatial",
          "Multi-argument partition transforms"
        ],
        limitations: [
          "Requires Spark 3.5+ for full V3 write support",
          "Streaming still skips deletion vector snapshots",
          "Geospatial types currently read-only",
          "Some features require specific Iceberg versions"
        ],
        bestFor: [
          "Modern data platforms requiring latest features",
          "High-frequency update/delete workloads",
          "Applications requiring data lineage tracking",
          "Teams adopting cutting-edge table formats"
        ]
      },
      technicalSpecs: [
        {
          category: "Format V3 Features",
          items: [
            { label: "Deletion Vectors", value: "GA", status: "available" },
            { label: "Row Lineage", value: "_row_id, _last_updated_sequence_number", status: "available" },
            { label: "New Data Types", value: "variant, nano-timestamp", status: "available" },
            { label: "Geospatial Types", value: "geometry, geography (read-only)", status: "limited" },
            { label: "Multi-arg Transforms", value: "Yes", status: "available" }
          ]
        },
        {
          category: "Version Requirements",
          items: [
            { label: "Spark 3.5+", value: "Full V3 read/write", status: "available" },
            { label: "Spark 3.4", value: "V3 read-only", status: "limited" },
            { label: "Iceberg 1.8+", value: "Required for GA support", status: "available" },
            { label: "Streaming Support", value: "Limited DV support", status: "limited" }
          ]
        }
      ],
      externalLinks: [
        { label: "Format V3 Specification", url: "https://iceberg.apache.org/spec/#version-3-extended-types-and-capabilities", type: "docs" },
        { label: "Iceberg 1.8.0 Release", url: "https://iceberg.apache.org/releases/#1.8.0-release", type: "docs" },
        { label: "Multi-Engine Support", url: "https://iceberg.apache.org/multi-engine-support/", type: "docs" }
      ]
    }
  },
  {
    title: "Time Travel & Versioning",
    chip: "Native SQL",
    description: "SQL VERSION AS OF / TIMESTAMP AS OF supported since Spark 3.3+; DataFrame as-of-timestamp option",
    icon: <ClockIcon className="w-6 h-6" />,
    color: "orange",
    score: 100,
    details: {
      title: "Time Travel in Spark",
      description: "Spark provides native SQL syntax and programmatic APIs for accessing historical versions of Iceberg tables.",
      overviewContent: {
        strengths: [
          "Native SQL time travel syntax",
          "DataFrame API time travel options",
          "Snapshot and timestamp-based queries",
          "Efficient metadata-only operations",
          "Integration with branching and tagging"
        ],
        limitations: [
          "Performance depends on snapshot retention",
          "Historical data availability limited by retention policies",
          "Large time ranges may require optimization"
        ],
        bestFor: [
          "Data auditing and compliance",
          "Debugging data quality issues",
          "A/B testing with historical data",
          "Reproducible analytics workflows"
        ]
      },
      technicalSpecs: [
        {
          category: "SQL Time Travel",
          items: [
            { label: "VERSION AS OF", value: "Snapshot ID", status: "available" },
            { label: "TIMESTAMP AS OF", value: "Point-in-time", status: "available" },
            { label: "Minimum Version", value: "Spark 3.3+", status: "available" },
            { label: "Performance", value: "Metadata-only", status: "available" }
          ]
        },
        {
          category: "DataFrame API",
          items: [
            { label: "as-of-timestamp", value: "Option-based", status: "available" },
            { label: "Programmatic Access", value: "Yes", status: "available" },
            { label: "Snapshot Selection", value: "ID or timestamp", status: "available" },
            { label: "Integration", value: "Full DataFrame API", status: "available" }
          ]
        }
      ],
      externalLinks: [
        { label: "Time Travel Guide", url: "https://iceberg.apache.org/docs/latest/spark-queries/#time-travel", type: "docs" },
        { label: "AWS Time Travel Guide", url: "https://docs.aws.amazon.com/prescriptive-guidance/latest/apache-iceberg-on-aws/iceberg-spark.html#using-time-travel", type: "docs" }
      ]
    }
  },
  {
    title: "Enterprise Security",
    chip: "Delegated", 
    description: "Delegates ACLs to underlying catalog (Hive Ranger, AWS IAM, Nessie policies); snapshot isolation; audit hooks",
    icon: <ShieldCheckIcon className="w-6 h-6" />,
    color: "green",
    score: 100,
    details: {
      title: "Security Architecture",
      description: "Spark provides enterprise-grade security through delegation to proven catalog security systems.",
      overviewContent: {
        strengths: [
          "Integration with existing security frameworks",
          "Fine-grained access control through catalogs",
          "Snapshot isolation for data consistency",
          "Complete audit trail through metadata",
          "Support for multiple authentication methods"
        ],
        limitations: [
          "Security features depend on catalog implementation",
          "Cross-catalog security policies not unified",
          "Requires expertise in underlying security systems"
        ],
        bestFor: [
          "Enterprise environments with existing security infrastructure",
          "Multi-tenant deployments",
          "Regulated industries requiring audit trails",
          "Organizations with complex access control requirements"
        ]
      },
      technicalSpecs: [
        {
          category: "Access Control Systems",
          items: [
            { label: "Apache Ranger", value: "Hive Metastore integration", status: "available" },
            { label: "AWS IAM", value: "S3 + Glue permissions", status: "available" },
            { label: "Lake Formation", value: "Advanced governance", status: "available" },
            { label: "Nessie RBAC", value: "Git-like permissions", status: "available" }
          ]
        },
        {
          category: "Security Features",
          items: [
            { label: "Snapshot Isolation", value: "ACID guarantees", status: "available" },
            { label: "Audit Metadata", value: "metadata_log_entries", status: "available" },
            { label: "Row-level Security", value: "Catalog-dependent", status: "available" },
            { label: "Column Masking", value: "Catalog-dependent", status: "available" }
          ]
        }
      ],
      externalLinks: [
        { label: "Ranger Integration", url: "https://docs.cloudera.com/runtime/7.3.1/iceberg-how-to/topics/iceberg-ranger-introduction.html", type: "docs" },
        { label: "AWS Lake Formation", url: "https://docs.aws.amazon.com/prescriptive-guidance/latest/apache-iceberg-on-aws/governance.html", type: "docs" },
        { label: "Nessie Authorization", url: "https://projectnessie.org/nessie-latest/authorization/", type: "docs" }
      ]
    }
  }
];

export const sparkTableData = {
  title: "Apache Spark Iceberg Feature Matrix",
  description: "Comprehensive breakdown of Iceberg capabilities in Apache Spark 3.3+",
  variant: "default",
  columns: [
    {
      key: "dimension",
      header: "Dimension",
      tooltip: "Iceberg feature category or capability area",
      width: "w-64"
    },
    {
      key: "support",
      header: "Support Level",
      tooltip: "Level of support in Apache Spark",
      align: "center",
      width: "w-32"
    },
    {
      key: "details",
      header: "Implementation Details",
      tooltip: "Specific capabilities and limitations"
    },
    {
      key: "version",
      header: "Min Version",
      tooltip: "Minimum Spark version required",
      align: "center", 
      width: "w-24"
    }
  ],
  rows: [
    {
      dimension: { 
        value: <span className="font-medium">Catalog Types</span> 
      },
      support: { 
        value: <span className="text-green-600 dark:text-green-400 font-semibold">Full</span>,
        badge: { text: "Complete", variant: "success" },
        tooltip: "Support for all major catalog implementations"
      },
      details: { 
        value: "Hive Metastore, Hadoop warehouse, REST, AWS Glue, JDBC, Nessie, custom plug-ins",
        tooltip: "Most comprehensive catalog support in the ecosystem"
      },
      version: { value: "3.0+" }
    },
    {
      dimension: { 
        value: <span className="font-medium">Read/Write Operations</span> 
      },
      support: { 
        value: <span className="text-green-600 dark:text-green-400 font-semibold">Full</span>,
        badge: { text: "Complete", variant: "success" }
      },
      details: { 
        value: "Table scans, metadata reads, INSERT INTO, INSERT OVERWRITE, DataFrame writeTo, stored procedures",
        tooltip: "Complete read/write capabilities with atomic operations"
      },
      version: { value: "3.0+" }
    },
    {
      dimension: { 
        value: <span className="font-medium">DML Operations</span> 
      },
      support: { 
        value: <span className="text-green-600 dark:text-green-400 font-semibold">Full</span>,
        badge: { text: "Complete", variant: "success" }
      },
      details: { 
        value: "MERGE INTO, UPDATE, DELETE with position/equality delete files (Iceberg 0.14+)",
        tooltip: "Modern delete architecture for efficient row-level operations"
      },
      version: { value: "3.2+" }
    },
    {
      dimension: { 
        value: <span className="font-medium">MoR/CoW Support</span> 
      },
      support: { 
        value: <span className="text-green-600 dark:text-green-400 font-semibold">Full</span>,
        badge: { text: "Configurable", variant: "success" }
      },
      details: { 
        value: "Copy-on-Write default; Merge-on-Read via write.delete.mode configuration",
        tooltip: "Flexible storage strategies optimized for different workload patterns"
      },
      version: { value: "3.2+" }
    },
    {
      dimension: { 
        value: <span className="font-medium">Streaming Capabilities</span> 
      },
      support: { 
        value: <span className="text-yellow-600 dark:text-yellow-400 font-semibold">Partial</span>,
        badge: { text: "Limited", variant: "warning" }
      },
      details: { 
        value: "Incremental reads, append/complete modes; delete snapshots skipped by default",
        tooltip: "Strong streaming support with some limitations on delete operations"
      },
      version: { value: "3.0+" }
    },
    {
      dimension: { 
        value: <span className="font-medium">Format V3 Support</span> 
      },
      support: { 
        value: <span className="text-green-600 dark:text-green-400 font-semibold">Full</span>,
        badge: { text: "GA", variant: "success" }
      },
      details: { 
        value: "Deletion Vectors, Row Lineage, new types, multi-arg transforms (Spark 3.5+, Iceberg 1.8+)",
        tooltip: "Industry-leading V3 support with all major features"
      },
      version: { value: "3.5+" }
    },
    {
      dimension: { 
        value: <span className="font-medium">Time Travel</span> 
      },
      support: { 
        value: <span className="text-green-600 dark:text-green-400 font-semibold">Full</span>,
        badge: { text: "Native SQL", variant: "success" }
      },
      details: { 
        value: "VERSION AS OF / TIMESTAMP AS OF SQL syntax; DataFrame as-of-timestamp option",
        tooltip: "Native SQL time travel with programmatic API support"
      },
      version: { value: "3.3+" }
    },
    {
      dimension: { 
        value: <span className="font-medium">Security & Governance</span> 
      },
      support: { 
        value: <span className="text-green-600 dark:text-green-400 font-semibold">Full</span>,
        badge: { text: "Delegated", variant: "success" }
      },
      details: { 
        value: "Delegates to catalog ACLs (Ranger, IAM, Nessie); snapshot isolation; audit metadata",
        tooltip: "Enterprise security through proven catalog integration"
      },
      version: { value: "3.0+" }
    },
    {
      dimension: { 
        value: <span className="font-medium">Schema Evolution</span> 
      },
      support: { 
        value: <span className="text-green-600 dark:text-green-400 font-semibold">Full</span>,
        badge: { text: "Instant", variant: "success" }
      },
      details: { 
        value: "Add/drop/rename columns, type promotion; metadata-only operations",
        tooltip: "Complete schema evolution with instant, non-blocking operations"
      },
      version: { value: "3.0+" }
    },
    {
      dimension: { 
        value: <span className="font-medium">Table Maintenance</span> 
      },
      support: { 
        value: <span className="text-green-600 dark:text-green-400 font-semibold">Full</span>,
        badge: { text: "Built-in", variant: "success" }
      },
      details: { 
        value: "Stored procedures: expire_snapshots, rewrite_data_files, rewrite_manifests, remove_orphan_files",
        tooltip: "Comprehensive maintenance procedures for optimal performance"
      },
      version: { value: "3.0+" }
    },
    {
      dimension: { 
        value: <span className="font-medium">Metadata Tables</span> 
      },
      support: { 
        value: <span className="text-green-600 dark:text-green-400 font-semibold">Full</span>,
        badge: { text: "Complete", variant: "success" }
      },
      details: { 
        value: "history, snapshots, files, manifests, partitions, all_data_files, metadata_log_entries",
        tooltip: "Complete access to all Iceberg metadata for monitoring and debugging"
      },
      version: { value: "3.0+" }
    },
    {
      dimension: { 
        value: <span className="font-medium">Branching & Tagging</span> 
      },
      support: { 
        value: <span className="text-yellow-600 dark:text-yellow-400 font-semibold">Partial</span>,
        badge: { text: "API Only", variant: "warning" }
      },
      details: { 
        value: "Java API support; SQL DDL and procedures for branch operations",
        tooltip: "Programmatic branch/tag support with expanding SQL capabilities"
      },
      version: { value: "3.3+" }
    }
  ]
};



export const sparkUseCases = [
  {
    title: "Enterprise Data Lake Analytics",
    description: "Large-scale analytical workloads with complex transformations",
    scenarios: [
      "Multi-terabyte ETL pipelines with schema evolution",
      "Complex analytical queries across multiple fact tables",
      "Machine learning feature engineering on historical data",
      "Cross-functional data sharing with fine-grained access control"
    ]
  },
  {
    title: "Real-time and Batch Processing",
    description: "Unified platform for both streaming and batch workloads",
    scenarios: [
      "Lambda architecture with consistent data processing logic",
      "CDC pipelines processing database changes in real-time",
      "Event-driven architectures with exactly-once processing",
      "Near real-time analytics with historical context"
    ]
  },
  {
    title: "Data Engineering and Pipeline Development",
    description: "Robust data transformation and pipeline orchestration",
    scenarios: [
      "Multi-stage transformation pipelines with checkpointing",
      "Data quality validation and cleansing at scale",
      "Cross-system data synchronization and replication",
      "Automated data archival and lifecycle management"
    ]
  },
  {
    title: "Advanced Analytics and ML",
    description: "Machine learning and advanced analytical workloads", 
    scenarios: [
      "Feature store implementations with time travel",
      "A/B testing with historical data comparisons",
      "Reproducible ML experiments with data versioning",
      "Large-scale model training on partitioned datasets"
    ]
  }
];

<QueryEngineLayout
  title="Apache Spark 3.3+"
  description="The reference implementation for Apache Iceberg with comprehensive read/write support, full DML capabilities, and GA Format V3 support"
  features={sparkFeatures}
  tableData={sparkTableData}
  useCases={sparkUseCases}
  officialDocs="https://spark.apache.org/docs/latest/"
  gettingStarted="https://iceberg.apache.org/docs/latest/spark-getting-started/"
  additionalResources={[
    { label: "Spark Configuration", url: "https://iceberg.apache.org/docs/latest/spark-configuration/" },
    { label: "Spark Procedures", url: "https://iceberg.apache.org/docs/latest/spark-procedures/" },
    { label: "Performance Tuning", url: "https://spark.apache.org/docs/latest/sql-performance-tuning.html" },
    { label: "Structured Streaming", url: "https://iceberg.apache.org/docs/latest/spark-structured-streaming/" }
  ]}
/>